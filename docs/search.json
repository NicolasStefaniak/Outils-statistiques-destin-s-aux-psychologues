[{"path":"index.html","id":"section","chapter":"","heading":"","text":"","code":""},{"path":"licence.html","id":"licence","chapter":"Licence","heading":"Licence","text":"Ce document est mis à disposition selon les termes de la licence :Creative Commons Attribution – Pas d’Utilisation Commerciale 4.0 Internationale (CC -NC 4.0).Vous êtes autorisé·e à :partager\n— copier, distribuer et communiquer le matérieladapter\n— remixer, transformer et créer à partir du matérielSous les conditions suivantes :Attribution — vous devez créditer l’auteurAttribution — vous devez créditer l’auteurPas d’utilisation commercialePas d’utilisation commercialeTexte complet de la licence : https://creativecommons.org/licenses/-nc/4.0/","code":""},{"path":"pourquoi-les-statistiques.html","id":"pourquoi-les-statistiques","chapter":"1 Pourquoi les statistiques","heading":"1 Pourquoi les statistiques","text":"“Statistical thinking one day necessary efficient citizenship ability read write. (Un jour, la pensée statistiques sera aussi indispensable pour pouvoir être un citoyen engagé qu’être capable de lire et d’écrire)”","code":""},{"path":"pourquoi-les-statistiques.html","id":"á-quoi-servent-les-statistiques","chapter":"1 Pourquoi les statistiques","heading":"1.1 Á quoi servent les statistiques ?","text":"Vous êtes-vous déjà demandé pourquoi vous aviez des cours de statistiques ? Avez-vous déjà pensé ou entendu des camarades dire « je ne vois pas pourquoi de tels cours, moi je veux être psy (ou biologiste, chimiste, médecin…) » Vous avez sans doute déjà pensé ou entendu « de toute façon, j’ai jamais rien compris aux maths, je veux juste arriver à compenser », voire affirmé avec force « les statistiques sont là pour nous éliminer et pour nous démotiver ». Au mieux, certain·es voulaient faire preuve de lucidité « en aura besoin pour notre TER », mais leur voix furent rapidement étouffée par « de toute façon, n’en aura jamais besoin dans notre vie pro ».Cependant, la véritable raison des cours de statistiques est bien plus inavouable que vous ne l’imaginez. Pour la comprendre, il faut se poser une question : à quoi sevent les statistiques ? Prenez le temps d’y répondre par vous-même avant de continuer votre lecture.Beaucoup de personnes considèrent que les statistiques permettent de rendre objectif, scientifique, indiscutable des faits qui sont présentés. Cependant, cette vision des statistiques est très étriquées et ne correspond, la plupart du temps, pas à la réalité. L’objectif premier des statistiques de faire passer un message, de convaincre et d’influencer.Comparez ces différentes phrases et questionnez-vous sur leur caractère persuasif :Il y de plus en plus de personnes au chômage ;Depuis 2023, le taux de chômage doublé ;Depuis 2023, le taux de chômage est passé de 4% à 8% ;Depuis 2023, le taux de chômage est passé de 4% à 8%, ce qui représente un niveau historiquement haut qui n’plus été atteint depuis 1975 ;En deux ans, le taux de chômage doublé, passant de 4% à 8%, cette augmentation est la plus forte enregistrée dans des délais aussi court depuis les années 1970.constate que les deux dernières propositions ont un pouvoir persuasif bien plus important et finalement, qui ira vérifier ? L’important est d’être convaincant, pas que ce soit vrai.Cet usage des statistiques est, qu’le veuille ou non, extrêmement fréquente. Il est donc nécessaire de pouvoir avoir un regard critique sur ce que les chiffres signifient pour pouvoir se forger se propre opinion. En l’absence de cette capacité, peut se trouver face à deux sortes d’écueils tout aussi graves l’un que l’autre :accepter sans condition le message transmis, sans que notre esprit critique n’ait quoique ce soit à y redire ;raisonner sur la base d’une incompréhension des statistiques et aboutir à des conclusions qui sont fausses.Au travers de quelques exemples, nous allons explorer comment les statistiques peuvent être utilisées pour duper les esprits qui y sont hermétiques.","code":""},{"path":"pourquoi-les-statistiques.html","id":"lefficacité-du-vaccin-contre-la-covid","chapter":"1 Pourquoi les statistiques","heading":"1.1.1 L’efficacité du vaccin contre la covid","text":"Durant la crise covid qui eu lieu entre 2020 et 2022, la recherche d’un traitement efficace s’est avéré être d’une importance capitale pour pouvoir reprendre un mode de fonctionnement identique à celui qui précédait le confinement.Parmi ces traitements, le vaccin à ARN s’est vite imposé comme étant une solution fiable et robuste pour lutter contre les dégâts de la maladie. Cependant, à cette époque, de nombreuses critiques issues des antivax se faisaient entendre. Parmi les raisonnements les plus fréquents, pouvait avoir celui-ci : la preuve que le vaccin ne fonctionne pas est qu’il y autant de personnes qui sont hospitalisées à cause de la covid en étant vaccinée que de personnes qui ne le sont pas.Si analye les chiffres bruts, les antivax ont raison : il y autant de personnes vaccinées que de personnes vaccinées, comme le montre la Figure 1.1.\nFigure 1.1: Nombre de personnes hospitalisées en fonction du statut de vaccination\nCependant, un élève de CM1 sait ce qu’est la notion de proportion. Ainsi, ne peut comparer le nombre de personnes vaccinées de personnes non vaccinées qu’à la condition d’avoir autant de personnes vaccinées et non vaccinées, ce qui n’était pas le cas. En effet, il y avait environ 10 fois plus de personnes vaccinées que de personnes non vaccinées. Ainsi, dans la Figure 1.2, le cadre indique les personnes qui ont été hospitalisées et se rend compte que proportionnellement au nombre de personnes non vaccinées, les personnes vaccinées étaient moins souvent hospitalisées, ou pour le formuler autrement le rapport entre les verts en dehors du cadre sur ceux qui sont dans le cadre est plus important que le rapport des rouges en dehors du cadre par rapport à ceux qui sont à l’intérieur du cadre.\nFigure 1.2: Nombre de personnes hospitalisées vs non hospitalisation en fonction du statut de vaccination\n","code":""},{"path":"pourquoi-les-statistiques.html","id":"la-suède-et-le-paradis-des-violeurs","chapter":"1 Pourquoi les statistiques","heading":"1.1.2 La Suède et le paradis des violeurs","text":"","code":""},{"path":"pourquoi-les-statistiques.html","id":"version-standard","chapter":"1 Pourquoi les statistiques","heading":"1.1.2.1 Version standard","text":"En 2015, pendant la campagne présidentielle aux USA, un site d’information d’extrême droite appelé Breitbart publié un article qui incitait les citoyens américains à regarder ce qui se passait en Suède de la manière suivante. La particularité de cet article était qu’il s’appuyait sur une utilisation trompeuse des statistiques pour nourrir la peur, la haine et l’exclusion. Concrètement, ils effectuaient une comparaison brute de chiffres qui ne renvoyaient pas aux mêmes réalités.Concrètement l’article utilisait les statistiques de viol pour dénoncer des sociétés ouvertes à l’immigration, en particulier la Suède en tenant le raisonnement suivant : ce pays présente l’un des taux de viol les plus élevés du monde occidental ; cette situation serait la conséquence directe de l’accueil d’immigrés issus de cultures supposément plus violentes. Une telle conclusion peut sembler convaincante à première vue, mais elle repose sur une incompréhension profonde de ce que mesurent réellement ces chiffres.Comparer les taux de viol entre pays n’de sens que si l’compare des indicateurs construits de manière identique. ce n’est pas le cas. Comme l’explique von Hofer (Hofer, 2000), les statistiques de viol sont des constructions sociales et juridiques avant d’être des reflets directs de la réalité criminelle. Ainsi, en Suède, toute plainte pour viol est comptabilisée comme telle, y compris lorsque l’affaire est ultérieurement classée sans suite ou requalifiée. Dans de nombreux autres pays, seules les condamnations judiciaires sont prises en compte. Ces deux indicateurs ne mesurent donc pas la même chose et cela ne fait dès lors pas sens de comparer les taux de viols entre des pays qui n’utilisent pas la même définition pour les comptabiliser.En outre, s’ajoutent des différences majeures dans les définitions légales. La notion de viol est plus large en Suède que dans beaucoup d’autres pays : des faits qui seraient qualifiés d’« abus sexuel » ailleurs, notamment en France, entrent dans la catégorie du viol en Suède. De plus, lorsqu’une même victime subit des viols répétés par le même agresseur, comme c’est le cas lorsqu’un mari viole sa femme, chaque acte est comptabilisé séparément dans les statistiques suédoises, alors qu’ils sont souvent regroupés en un seul événement dans d’autres pays.Les facteurs culturels jouent également un rôle déterminant. La propension à déclarer un viol dépend fortement du contexte social, du regard porté sur les victimes et des risques encourus après une plainte. Dans certaines sociétés, dénoncer un viol expose les victimes à la stigmatisation, à des représailles, voire à des violences graves. Dans ces conditions, des taux officiellement faibles ne traduisent pas une moindre prévalence des violences sexuelles, mais un sous-signalement massif. À titre d’illustration, une enquête montre qu’environ un quart des Français considèrent que l’auteur d’un viol est moins responsable si la victime portait une tenue jugée sexy (Le Breton, 2016), ce qui donne la mesure des freins culturels à la dénonciation, y compris dans des pays occidentaux.Les écarts entre statistiques officielles et données issues de la recherche scientifique renforcent ce constat. Koss et al. (1987), dans une enquête menée auprès d’étudiantes américaines, montrent que plus de la moitié rapportent avoir subi des abus sexuels au sens large, et qu’environ 15 % déclarent avoir été violées. Ces chiffres sont sans commune mesure avec ceux issus des statistiques policières ou judiciaires, ce qui illustre à quel point les données administratives sous-estiment la réalité des violences sexuelles.Ainsi, affirmer qu’un pays serait plus dangereux qu’un autre en se fondant uniquement sur des taux de viol publiés sans analyse méthodologique revient à tirer des conclusions infondées. Les chiffres ne sont pas des faits bruts : ils sont le produit de définitions juridiques, de pratiques institutionnelles et de contextes culturels. Sans cette compréhension, les comparaisons internationales deviennent non seulement invalides, mais dangereuses, car elles peuvent servir de support à des discours idéologiques qui instrumentalisent la peur.Comprendre les statistiques, c’est apprendre à se demander ce qui est réellement mesuré, comment cela l’est, et ce que les chiffres ne disent pas. C’est à cette condition seulement que les données peuvent éclairer le débat public au lieu de l’obscurcir.","code":""},{"path":"pourquoi-les-statistiques.html","id":"version-conte-de-fées","chapter":"1 Pourquoi les statistiques","heading":"1.1.2.2 Version conte de fées","text":"Il était une fois une forêt, appelée la forêt de Breitbart. Dans la forêt de Breitbart vivait une femme. dit d’elle qu’elle avait des pouvoirs magiques. l’appelait la sorcière de Breitbart. Cette sorcière n’avait pas plus de baguette magique qu’elle ne préparait de potion à base de griffes de dragons. Son pouvoir était bien plus sournois. Le pouvoir de cette sorcière était l’obscurantisme. Elle se complaisait dans la haine et l’exclusion. Elle avait ce pouvoir d’influencer et de manipuler l’esprit des plus fragiles. Elle se montrait patiente et efficace en attendant de pouvoir semer la graine de la haine dans le coeur des malheureux qui osaient s’aventurer dans cette ténébreuse forêt. Il arrivait que des promeneurs venus du monde entier passaient par la forêt de Breitbart et en ressortaient profondément changés : des agneaux transformés en loups. Quand traversait cette forêt, il y avait comme un souffle, un murmure qui caressait les oreilles des passants. Il était si facile à écouter. C’était tellement apaisant et agréable d’entendre « vous êtes les meilleurs, la race supérieure, vous devez vous défendre » ou encore « montrer la supériorité des hommes sur les femmes » (l’auteur de ces lignes ayant des nausées en écrivant la manière de penser de cette sorcière s’est vu contraint de ne pouvoir donner d’autres exemples au risque d’être atteint de vomissements durables).La perfidie de cette sorcière était sans limite. Elle éprouvait une haine sans fin pour un pays appelé Ouvertuland. Dans ce pays, les hommes considéraient les femmes comme leurs égales, les habitants étaient convaincus que l’endroit de notre naissance n’était pas un choix, mais un état de fait et que chacun avait le droit d’aspirer à un monde meilleur lorsque la guerre et les persécutions frappaient, que la souffrance n’était pas tolérable, qu’importe l’endroit où était né.La sorcière de Breitbart étalait cette haine au grand jour. Elle criait à qui voulait l’entendre : « Ecoutez-les se plaindre, regardez ce qui se passe là-bas, ils accueillent les étrangers, leurs femmes et leurs filles se font violer ; ne laissons pas des immigrés faire, protégeons-nous ; nous sommes supérieurs, ils ne méritent pas notre plus petite considération ». Regardez ce qu’il s’y passe, leur taux de viol est le plus élevé du monde occidental (et si vous faites une petite recherche sur le taux de viol en Suède notre statistique d’intérêt, vous trouverez que, en 2010, les statistiques de viol montrent que la Suède arrive en \\(3^{ème}\\) position derrière l’Afrique du Sud et le Botswana et si vous vous dites tout cela pour cela, vous auriez raison), tout cela parce qu’ils ont laissé des immigrés ayant une culture du viol entrer dans leur pays, voyez ce qui s’y passe et ne surtout ne faisons pas comme eux ».Et les esprits fragiles (dont vous ne faites pas partie… ou dont vous ne ferez bientôt plus partie) se laisseraient évidemment envahir par ces pensées, et le coeur des hommes vaillants deviendrait alors sombre et froid, intolérant et xénophobe. Comment était-il possible que ce pays tant admiré pour ses avancées sociales soit l’objet d’une situation aussi abjecte ? Et les passants qui entraient dans la forêt de Breitbart en sortaient avec la certitude que les immigrés étaient la cause de tous les problèmes de ces peuples trop ouverts, trop tolérants, trop naïfs.Un jour, un homme qui était connu sous le nom de Gandwells traversa la forêt de Breitbart. La sorcière de Breitbart exerça son pouvoir maléfique contre Gandwells. Le coeur de Gandwells vacilla, la haine commençait à le submerger, mais Gandwells avait l’esprit vif et incisif. Il concentra ses pensées pour tenter de combattre les pouvoirs de la sorcière. Il lui dit : « Donalda (c’était le prénom de la sorcière de Breitbart, et à nouveau toute ressemblance avec des personnes existant ou ayant existé est purement … volontaire), peux-tu m’expliquer comment les pays dont sont issus ces immigrés ont des taux de viols bien plus faibles que les pays dont proviennent ces immigrés ? » (Vous pouvez constater que des pays comme le Pakistan, l’Albanie ou l’Azerbaïdjan ont des taux de viols particulièrement faibles, alors que ce sont des pays qui correspondent aux critères dénoncés par Breitbart). La sorcière Breitbart argumenta :« C’est parce que, dans ces pays, le viol est tellement commun, qu’ils ne les comptabilisent même pas. »Gandwells demanda alors :« Et dans les autres pays, comment le taux de viol est-il calculé ? ».La sorcière venait d’être touchée, elle tenta de bredouiller mal à l’aise« Ben, euh, comme partout.-Je ne suis pas sûr de comprendre, reprit Gandwells. Vous avez dit que certains pays ne comptabilisent pas les viols, car c’est dans leur nature d’en perpétrer, et puis vous dites que tout le monde comptabilise les viols de la même manière… »La sorcière se tut un instant, ne sachant que répondre. Elle tenta :« Ça n’strictement rien à avoir. Ce n’est pas pareil.»Gandwells ignora la sorcière et continua sa démonstration :« En réalité, je crois que le peuple d’Ouvertuland est particulièrement attentif au respect des êtres-humains, et du corps des femmes (Je focalise sur les femmes car la très grande majorité des viols concerne les femmes). peut penser qu’ils ne comptabilisent pas du tout les viols de la même manière » [Von Hofer (2000) explique en détail pourquoi les statistiques de viol ne peuvent pas être comparées d’un pays à l’autre. En effet, en Suède, est comptabilisé comme viol, toute plainte pour viol, même si par la suite l’affaire est classée ou que les charges sont réduites. Dans la plupart des autres pays, les statistiques de viols portent sur les condamnations uniquement. De même, en Suède, une personne qui va être violée 30 fois par son mari va compter comme 30 viols dans les statistiques, alors que, dans la plupart des autres pays, ce ne sera comptabilisé qu’une seule fois étant donné que ce sont les mêmes personnes qui sont impliquées dans chacun des viols. Par ailleurs, la définition de viol est plus large en Suède que dans d’autres pays. Ainsi, ce qui serait classé comme un abus sexuel dans un autre pays, serait considéré comme viol en Suède. peut rajouter à cela que la culpabilisation des femmes dans la dénonciation des viols est très dépendante de la culture, avec dans certains pays, un réel risque d’être persécutée, voire tuée. Pour comprendre cet effet de la culture, 1/4 des Français estiment que l’auteur d’un viol est moins responsable si la victime portait une tenue sexy (Le Breton, 2016). Il n’est dès lors pas étonnant de ne pas oser dénoncer un viol dans ces conditions. La Suède étant très en avance sur la considération de l’être humain, les femmes reportent plus facilement les abus sexuels dont elles ont été victimes. Pour illustrer encore plus ces différences, sans entrer dans le débat sur la manière dont doit comptabiliser les taux de viols, peut regarder les différences qui existent entre les statistiques officielles et celles issues des articles scientifiques, comme le montre l’article de Koss et al. (1987) qui montre que 53% des étudiantes américaines ont été victimes d’abus sexuels au sens large, et 15% rapportent avoir été violées, ce qui ne correspond absolument pas aux chiffres très inférieurs publiés par les autorités des pays].C’est ainsi que Gandwells mena son combat contre l’obscurantisme en illustrant avec force et conviction que les nombres ne renvoyaient pas aux mêmes réalités et que vouloir comparer les statistiques criminelles entre les pays et Ouvertuland n’avait pas de sens !La sorcière de Breitbart se réfugia dans une grotte cachée au fin fond de sa forêt, humiliée par Gandwells, mais le combat n’était pas gagné pour autant, le plus dur restait à venir. Il fallait que le monde sache la perfidie de Donalda, il fallait que le monde ne se laisse pas manipuler par cette sorcière, il fallait apprendre aux hommes à avoir l’esprit vif et incisif, à se poser les bonnes questions. C’était la seule issue.","code":""},{"path":"pourquoi-les-statistiques.html","id":"pourquoi-les-statistiques-sont-indispensables-pour-de-futurs-psychologues","chapter":"1 Pourquoi les statistiques","heading":"1.2 Pourquoi les statistiques sont indispensables pour de futurs psychologues ?","text":"En tant que futur3es psychologues, vous serez amené·e à utiliser des statistiques de manière récurrente dans plusieurs contextes :Identifier les propriétés psychométriques des outils que vous utilisez. En l’absence de cette analyse, vous risquez d’utiliser des outils dont les fondements scientifiques sont largement insuffisants (Lilienfeld et al., 2000; Strauss et al., 2006) ;Identifier les propriétés psychométriques des outils que vous utilisez. En l’absence de cette analyse, vous risquez d’utiliser des outils dont les fondements scientifiques sont largement insuffisants (Lilienfeld et al., 2000; Strauss et al., 2006) ;Être capable de réaliser des inférences relatives au niveau de fonctionnement des usagers auxquels vous serez confronté·e, c’est-à-dire déterminer si l’usage présente un déficit, un trouble, un risque pour la société … selon l’environnement de travail dans lequel vous travaillerez ;Être capable de réaliser des inférences relatives au niveau de fonctionnement des usagers auxquels vous serez confronté·e, c’est-à-dire déterminer si l’usage présente un déficit, un trouble, un risque pour la société … selon l’environnement de travail dans lequel vous travaillerez ;Vous former en lisant des articles scientifiques, des livres ou des rapports, ou en suivant des formations. qu’importe le moyen vous devrez pouvoir vous demander “comment est-ce que cela été montré ?” et “est-ce que cette manière de le montrer est convaincante ?”.Vous former en lisant des articles scientifiques, des livres ou des rapports, ou en suivant des formations. qu’importe le moyen vous devrez pouvoir vous demander “comment est-ce que cela été montré ?” et “est-ce que cette manière de le montrer est convaincante ?”.Ainsi, la nature même d’un cours de statistiques n’est pas de vous apprendre une suite interminable d’outils dont vous devrez maîtriser les tenants et les aboutissants mais d’identifier les fondements qui sont communs à l’ensemble des outils statistiques afin de pouvoir donner du sens aux nombres.","code":""},{"path":"pourquoi-les-statistiques.html","id":"exercice","chapter":"1 Pourquoi les statistiques","heading":"1.3 Exercice","text":"En 2020, la covid19 fait rage, la population est confinée et aucun traitement ne semble efficace. Des milliers de personnes meurent tous les jours dans le monde à cause de cette infection. Dans cette course effrénée visant à rechercher un traitement, Raoult et son équipe ont publié une étude qui vantait l’efficacité de l’hydroxychloroquine associée à l’azythromycine pendant au moins trois jours pour lutter efficacement contre la covid-19 (Lagier et al., 2020). Imaginez que vous, ou une personne qui vous est chère, développe une forme grave de la covid19. Allez-vous prendre ce remède miracle ? Afin d’éclairer votre réflexion, vous trouverez dans la Figure 1.3 et dans le Tableau 1.4 et 1.5 les informations nécessaires pour prendre votre décision. Il est à noter que pour analyser ces résultats, nul besoin d’avoir des compétences en médecine. Les notions qui sont enseignées dans un cours d’introduction à la méthodologie, quelques connaissances d’ordre général et un peu de logique suffisent amplement à se faire une opinion.\nFigure 1.3: Diagramme de flux des participants permettant de représenter les allocations entre les différents groupes.\n\nFigure 1.4: Tableau des données démographiques.\n\nFigure 1.5: Tableau des résultats.\nNormalement, vous devriez avoir remarqué des problèmes qui remettent considérablement en cause les conclusions de Raoult et de ses collaborateurs.Une des remarques qui est souvent faite est que l’effectif n’est pas le même dans les différents groupes. En réalité, ce point ne pose pas réellement souci car il existe des outils statistiques en mesure de prendre cet aspect en compte. Par ailleurs, si de bonnes raisons de penser qu’un traitement est efficace, il serait éthiquement douteux de ne pas le proposer à une plus grande proportion de personnes que celles qui ne le reçoit pas.En revanche, ce qu’observe :la proportion de personnes hospitalisées qui reçoivent le traitement (13.8%) est largement inférieure à la proportion de personnes hospitalisées qui ne le reçoivent pas (39.3%) ;l’indicateur NEWS-2 est un indicateur est un indicateur permettant d’évaluer l’état du patient au moment où il est examiné. Plus cette valeur est élevée et plus son état est dégradé. Dès lors que le score est supérieur à 5, les risques d’une réponse urgente deviennent important. Pour cet indicatuer, il apparaît que les risques sont beaucoup plus élevés (19.9%) pour les personnes qui ne reçoivent pas le traitement que pour les personnes qui le reçoivent (6.3%)un troisième problème qui saute d’emblée aux yeux est que, parmi les personnes qui font partie de la condition ‘autre traitement’, certains ont reçu l’association hydoxychloroquine (HCQ) et azymothricine (AZ) pendant moins de 3 jours, d’autres ont reçu uniquement de l’hydoxychloroquine, d’autres ont reçu uniquement de l’azymothricine et enfin, certaines personnes n’ont rien reçu. Cette répartition est étonnante car :\ns’attend normalement à ce que le groupe de contrôle ne reçoivent pas le même traitement que le groupe traité (pendant une durée moindre). En effet, peut raisonnablement penser que les personnes pour lesquelles le traitement n’pas durée 3 jours ou plus sont celles dont l’état s’est dégradé et qu’elles ont basculé du bras “traité” au bras “contrôle”. Cette manière de faire est méthodologique douteuse. ne change pas les membres d’une équipe en cours de match. Cela reviendrait à ce que, à un moment du match, un joueur de l’OM se mette à jouer pour le PSG (ou inversément).\nIl est possible de comparer HCQ+AZ >3 jours avec HCQ > 3 jours, AZ > 3 jours et aucun traitement. Cependant, dans ce cas, n’est pas dans un plan avec deux bras parallèles mais dans un plan factoriel. Il est donc anormal de regrouper ces trois groupes ensemble.\ns’attend normalement à ce que le groupe de contrôle ne reçoivent pas le même traitement que le groupe traité (pendant une durée moindre). En effet, peut raisonnablement penser que les personnes pour lesquelles le traitement n’pas durée 3 jours ou plus sont celles dont l’état s’est dégradé et qu’elles ont basculé du bras “traité” au bras “contrôle”. Cette manière de faire est méthodologique douteuse. ne change pas les membres d’une équipe en cours de match. Cela reviendrait à ce que, à un moment du match, un joueur de l’OM se mette à jouer pour le PSG (ou inversément).Il est possible de comparer HCQ+AZ >3 jours avec HCQ > 3 jours, AZ > 3 jours et aucun traitement. Cependant, dans ce cas, n’est pas dans un plan avec deux bras parallèles mais dans un plan factoriel. Il est donc anormal de regrouper ces trois groupes ensemble.Par ailleurs, le Tableau 1.4 montre que les personnes dans “autres groupes” sont plus âgés que dans le groupê HCQ+AZ. Concrètement, 3.6% de personnes de plus de 74 ans dans le groupe HCQ+AZ, alors que cette proportion est de 16% dans “autres traitements”. , sait que les personnes les plus à risque de complication dus à la covid19 sont les personnes âgées. Remarquez que ce phénomène est valable pour chacun des sous-groupe, et en particulier le sous-groupe (à l’exception de l’hydroxychloroquine seule) qui reçu le traitement pendant moins de trois jours. Ce tableau nous indique également que les scores au NEWS supérieur ou égal à 6 (donc requérant des soins urgents) est de 2.6% dans le groupe HCQ+AZ, alors que cette proportion est de 10.5% dans dans les autres groupes, avec une proportion de 14.2% dans le groupe qui reçu le traitement pendant moins de 3 jours. Cette proportion particulièrement importante est interpellante car cela amène à penser que les personnes dont l’état se serait dégradé ou qui serait décédées en ayant reçu le traitement efficace ont été basculées dans le groupe de contrôle. Enfin, observe également que le scanner pulmonaire est proportionnellement plus dégradé chez les personnes appartenant à “autre traitement” qu’au traitement HCQ+AZ. Ceci amène donc à se questionner sur la manière de choisir les personnes qui allaient recevoir le traitement : il semble que les personnes à risque de dégradation avaient moins de chance de recevoir le traitement, ce qui entrainement artificiellement un meilleur résultat pour les personnes traitées.Examinons à présent les personnes qui ont été hospitalisées en soins intensifs (ICU) et celles qui sont décédées dans le Tableau 1.5. Ils sont de 1.1% dans le groupe HCQ+AZ et de 9.4% dans le groupe “autre traitement”, avec un taux de 17% dans le groupe qui reçoit le traitement pendant moins de 3 jours, tandis que les personnes qui ne reçoivent rien ont une évolution vers les soins intensifs et le décès de 3.1%. Pour le formuler autrement, les personnes qui ne reçoivent rien ont un taux d’évolution négative proche du groupe traité, et le groupe pour lequel l’évolution est la plus négative est le groupe ayant reçu le traitement durant moins de trois jours. Donc, même si laissait le bénéfice du doute concernant le fait d’avoir basculé les personnes recevant le traitement miracle dans le groupe de contrôle pour faire apparaître des résultats flatteurs, aurait du mal à trouver une explication rationnelle pour rendre compte du fait que, si l’HCQ+AZ est efficace, les patients qui le reçoivent durant moins de 3 jours voient leur état se dégrader bien plus souvent que n’importe quel autre groupe. devrait s’attendre à ce que cela les aide tout de même un peu (ou minima que cela n’entraîne pas un risque supplémentaire) plutôt que de présenter un risque d’évolution négative plus important que les autres groupes","code":""},{"path":"quel-logiciel-utiliser-et-pourquoi.html","id":"quel-logiciel-utiliser-et-pourquoi","chapter":"2 Quel logiciel utiliser et pourquoi ?","heading":"2 Quel logiciel utiliser et pourquoi ?","text":"“Science advances questioning results, hiding . (La science avance en questionnant les résultats, non pas en les cachant”","code":""},{"path":"quel-logiciel-utiliser-et-pourquoi.html","id":"introduction","chapter":"2 Quel logiciel utiliser et pourquoi ?","heading":"2.1 Introduction","text":"Aborder les statistiques dans la perspective d’une mise en application requiert non seulement de fournir les fondements théoriques pour choisir et comprendre les analyses statistiques mais doit également fournir les bases nécessaires pour utiliser de manière autonome un logiciel de statistiques. Bien qu’il existe une multitude de logiciels statistiques, nous nous consacrerons au logiciel R (R Core Team, 2025) et au package que nous avons développé, easieR (Stefaniak, 2018)Nous commencerons par expliquer pourquoi l’utilisation de R est bon pour vous. Ensuite, nous expliquerons comment l’installer et la logique sous-jacente à R. Nous continuerons avec les règles de bonnes pratiques qui vous seront utiles tout au long de cet ouvrage mais également au-delà si vous étiez amené.e à devoir réaliser des analyses qui ne sont pas couvertes par le contenu de ce livre. Enfin, nous terminerons en présentant quelques outils vous permettant de préparer vos données.","code":""},{"path":"quel-logiciel-utiliser-et-pourquoi.html","id":"quest-ce-r-et-pourquoi-lutiliser","chapter":"2 Quel logiciel utiliser et pourquoi ?","heading":"2.2 Qu’est-ce R et pourquoi l’utiliser ?","text":"Sorti pour la première fois en 1995, R est un langage de programmation ouvert, issu de langage S, spécialisé dans le traitement des données.Á l’heure actuelle, il s’agit d’un des logiciels les plus utilisés dans le monde académique. D’après Choueiry (Choueiry, 2021), non seulement R arrive en seconde position pour traiter les données dans le domaine de la recherche en santé, mais arrive quasiment à égalité de la première place lorsqu’s’intéresse aux articles publiés dans les meilleures revues (si considère que le facteur d’impact d’une revue informe d’une manière ou d’une autre sur la qualité des articles qui y sont publiés). De plus, R fait partie des langages de programmation les plus demandés dans les offres d’emploi de l’IEEE (https://www.facebook.com/48576411181), Á l’heure actuelle, de nombreux mastodontes de l’informatique, en particulier celles qui utilisent l’intelligence artificielle, comme google et Facebook, l’utilisent. Etant donné que R pour vocation première le traitement des données statistiques, il est légitime de se demander ce qui le rend aussi populaire et pourquoi vous devriez utiliser R.Utiliser R, c’est avant tout s’engager vis-à-vis d’un système de valeurs dans lequel considère que l’accès au savoir et à la science doit être ouvert et transparent, que l’évolution des connaissances doit être un processus collaboratif dont le est de faire progresser l’Humanité dans son ensemble. Peut-être pensez-vous que ce projet est utopique, mais de plus en plus de personnes, et en particuliers de plus en plus de scientifiques ont la conviction qu’une science ouverte et transparente est l’avenir de la science. La place que prend R dans la publication dans les revues les plus prestigieuses, tel que nous venons de l’évoquer, n’est pas un phénomène irrationnel, mais est le fruit d’un double processus qui interagit : d’une part, les lignes éditoriales de ces revues imposent aux auteurs plus de transparence à la fois sur les données, mais également sur la manière dont elles ont été traitées et R est un outil parfaitement adapté pour répondre à ce critère ; d’autre part, les chercheurs qui adoptent les pratiques de la science ouverte vont avoir plus de citations, plus d’attention des médias, plus de collaborateurs potentiels, plus d’opportunités d’emploi et plus de facilités pour obtenir des financements (McKiernan et al., 2016). Pour la seconde raison, la question de la poule et de l’oeuf se pose : est-ce les meilleurs chercheurs qui favorisent la science ouverte ou est-ce le fait de favoriser la science ouverte qui rend un chercheur meilleur? Sans doute un peu les deux.Utiliser R est un pas vers cet objectif de science ouverte car R est un logiciel ouvert. Cela signifie qu’il est gratuit (ce qui est une bonne nouvelle pour vous), que vous pouvez analyser le code utilisé pour programmer les fonctions que vous allez utiliser, et vous pourrez également, si vous le souhaitez, contribuer à son développement. En d’autres termes, R est collaboratif, et ce processus collaboratif n’est pas restreint aux contributions que chacun peut faire. Le côté collaboratif de R se manifeste également par la communauté d’entraide qui existe sur les forums en ligne et par le fait que chaque utilisateur la possibilité d’informer un des contributeurs s’il y une erreur ou un bug dans son code.Pour comprendre en quoi cette manière de penser est avantageuse, prenons le temps de nous créer une représentation mentale de la manière dont vos logiciels commerciaux préférés fonctionnent et comparons ce fonctionnement avec celui de R. Pour le logiciel que vous avez à l’esprit, connaissez-vous les auteurs du logiciel et leurs compétences ? Probablement que non. Dans le cas de R, il y des milliers de contributeurs, certains étant des anonymes qui veulent apporter leur pierre à l’édifice, mais d’autres sont d’éminents spécialistes, de domaines parfois très spécialisés.\nconçoit donc aisément que, si la qualité des contributeurs est plus inégale lorsqu’utilise R, vous pourrez cependant bénéficier des outils les plus performants pour traiter une problématique et ce en étant bien guidé et en cherchant les fonctions les plus recommandées. Cela se fait notamment grâce aux communautés actives sur le web et à la liste des packages que le noyau dur des programmeurs de R met en avant. Rappelons que ce seront des spécialistes du domaine qui auront développé la boîte à outil permettant de répondre spécifiquement à votre question. Utiliser R en ligne de commande offre donc une flexibilité qu’aucun logiciel sous la forme d’interface graphique ne peut proposer.Ainsi, si reprend notre comparaison avec un logiciel commercial, imagine aisément qu’une équipe restreinte d’une entreprise commerciale, aussi bonne que soit cette équipe, ne peut se tenir à la pointe de tous les domaines pour pouvoir rivaliser avec une communauté de scientifiques spécialistes de leur domaine. Imaginons à présent que le logiciel commercial que vous utilisez un bug, que faites-vous ? Probablement, rien. Au mieux, vous enverrez un courriel au SAV de\nla société qui commercialise le logiciel qui, avec un peu de chance, prendra vos remarques en compte pour améliorer la version suivante du logiciel, version suivante que vous serez amenés à acheter et qui sortira plus ou moins tardivement par rapport au moment où le bug été identifié. R fonctionne sur un autre modèle : vous pouvez contacter directement le contributeur de la fonction pour laquelle vous avez identifié un souci. Ce dernier identifiera l’origine du problème et le corrigera en quelques jours. Croyez-en mon expérience, les auteurs des packages fiables sont extrêmement réactifs.Les avantages de R ne se limitent pas à être gratuit. En effet, R vous permet également d’être transparent et reproductible, car vous pouvez aisément partager le code qui servi à analyser les données. Cela deux avantages. Le premier, et pas le moindre, est que cela permet à d’autres personnes d’avoir accès et donc de réutiliser la méthode d’analyse qui été utilisée dans un article donné. Ces personnes pourront alors reproduire cette méthode pour l’appliquer à un autre article. Dans cette section, va vous présenter des lignes de commandes que vous réutiliserez pour vos propres données. Ce chapitre été écrit en utilisant directement R. Ainsi, je suis sûr que les lignes de commandes que je vais vous présenter vont fonctionner en l’utilisant exactement de la manière dont elle est présentée. Les erreurs de copier/coller sont donc évitées.\nLe second avantage est que tout le monde peut vérifier qu’il n’y pas d’erreur dans le traitement des données qui ont été publiées. Ces erreurs peuvent se manifester à la fois dans la manière de mener les analyses, mais également dans le report des indices. En effet, pour les articles publiés entre 1985 et 2013 dans 8 grosses revues de psychologie, Nuijten et ses collaborateurs (Nuijten et al., 2016) ont identifié que la moitié des articles présentaient des inconsistances entre la valeur de la probabilité reportée au regard de la valeur de la statistique et des degrés de libertés 1 qui leur étaient associés.Les erreurs peuvent arriver. Il est tout en fait acceptable d’en commettre mais il est moins acceptable de ne pas permettre aux autres de vérifier qu’il n’y en ait pas.Il faut se rappeler que la recherche en France, mais également dans beaucoup de pays du monde, est financée, au moins en partie, par des fonds publics. Á partir du moment où de l’argent public est dépensé, il y une nécessité de transparence à la fois sur la manière dont cet argent été dépensé, mais également sur le fruit de ces dépenses. Ainsi, la valorisation des recherches, soit les financeurs (les contribuables lorsque la recherche est financée sur les fonds publics), doivent non seulement pouvoir accéder aux résultats de la recherche, mais également à toutes les étapes qui ont permis d’atteindre ces résultats, y compris le traitement des données. R permet cette transparence puisqu’peut partager les données ainsi que le code utilisé et qu’il n’est pas nécessaire d’avoir une licence d’un logiciel payant pour vérifier que tout est correct.Au-delà de ces avantages, en termes de fonctionnalités, R est le champion incontestable en offrant bien plus de fonctionnalités que les logiciels payants, avec plus de 150 fois plus de fonctions que SAS (Muenchen, 2015), en faisant ainsi un outil extrêmement puissant et polyvalent pour l’analyse de données.peut ajouter aux qualités de R qu’il est compatible avec tous les systèmes d’exploitation (Windows, MacOS, Linux) et qu’il existe des serveurs en ligne qui vous permettent d’utiliser R sans devoir l’installer sur votre ordinateur. Cela peut être utile lorsqu’peu de place sur le disque dur ou qu’veut travailler de manière collaborative.Certains aficionados vous diront même que R va vous aider à mieux comprendre les statistiques. Je ne partage pas cet avis : un logiciel en soi n’aide pas à comprendre ou à ne pas comprendre. En revanche, la personne qui vous explique les statistiques peut utiliser R pour faire des simulations ou encore l’utiliser comme outil pédagogique pour l’enseignement des statistiques, ce qui ne peut pas être fait dans d’autres logiciels statistiques où ne peut faire que du click. Par exemple, Snow (2024) développé toute une série d’outils permettant d’expliquer des concepts fondamentaux des statistiques.Par ailleurs, utiliser R requiert de prendre des décisions statistiques et de comprendre ce que l’fait sinon c’est le message d’erreur assuré. Ainsi, si vous ne comprenez pas les statistiques, R n’pas le pouvoir magique de vous faire comprendre, mais si vous comprenez les statistiques, l’utilisation de R vous permettra d’aller encore plus loin. Cependant, que vous compreniez ou non les statistiques, utiliser R de manière efficiente nécessite du temps, car il faudra apprendre à utiliser des lignes de commandes, lignes de commandes qui ont des arguments. Cet apprentissage est assez ténu dans un premier temps mais l’équipe qui développe R fait un effort important pour harmoniser la manière d’utiliser les fonctions. Et si l’apprentissage est lent dans un premier temps, par la suite, vous serez beaucoup plus rapide pour toute une série de tâches, telles que les prétraitements de vos données. Par ailleurs, les outils d’intelligence artificiel ont largement progressé pour vous aider à utiliser les lignes de commande. Il n’est pas possible de ne s’appuyer que ces outils sans comprendre ce qu’fait, mais cela fait gagner un temps considérable quand les bons réflexes et les bonnes bases.Enfin, utiliser R un intérêt indirect : l’utilisation des lignes de commande est un enseignement sans compromis à la rigueur. Comme évoqué juste avant, nous commettons tous des erreurs, quand en commet en utilisant des lignes de commande, le résultat est sans appel : un message d’erreur. Le fait d’être exposé à ce message d’erreur n’est pas anodin, il souligne le nombre de fois que nous sommes susceptibles de nous tromper, indépendamment des statistiques, et de prendre conscience de la nécessité d’être exigeant avec nous-mêmes, avec nos productions lorsque nous rédigeons un document afin d’éviter au maximum ces erreurs car la science, au travers des articles scientifiques, pour vocation d’être exempte autant que peu se faire d’erreurs. Si vous vous dites que vous ne voulez pas utiliser R car vous recevez constamment des messages d’erreur, est-ce que vous êtes disposés à être critiqués, comme c’est le cas lorsque nous sommes relus par des coauteurs ou dans le processus d’expertise, et à élever votre niveau pour éviter de commettre les mêmes erreurs à l’avenir. La vertu pédagogique de cet apprentissage est sans doute inégalable pour quiconque envisage une carrière dans la recherche, et pour les autres, cela apprend l’humilité face à nos certitudes, et notre niveau de compétence.R, dans sa forme brute, n’est pas très conviviale. C’est pourquoi nous utiliserons Rstudio, également gratuit, qui est une interface graphique de R et un éditeur de code.","code":""},{"path":"quel-logiciel-utiliser-et-pourquoi.html","id":"quest-ce-que-easier-et-pourquoi-lutiliser","chapter":"2 Quel logiciel utiliser et pourquoi ?","heading":"2.3 Qu’est-ce que easieR et pourquoi l’utiliser","text":"","code":""},{"path":"quel-logiciel-utiliser-et-pourquoi.html","id":"rationnel-deasier","chapter":"2 Quel logiciel utiliser et pourquoi ?","heading":"2.3.1 Rationnel d’easieR","text":"D’un point de vue pédagogique, utiliser R en ligne de commande requiert de faire le deuil concernant l’autonomie des étudiant·es dans le traitement des données. En effet, il n’est pas pédagogiquement possible, quand le volume horaire est restreint pour enseigner les statistiques, de les aborder de manière suffisamment approfondie pour assurer l’autonomie dans le traitement des données tout en assurant l’accompagnement indispensable aux étudiant·es qui se trouvent dans la situation où ils et elles utilisent pour la première fois des lignes de commande, à savoir faire face aux messages d’erreurs. Ces messages d’erreurs pouvaient également être source de frustration et de découragement pour les étudiant·es, en particulier celles et ceux qui ont une vision de l’intelligence en tant qu’entité (Dweck, 2006)C’est dans ce contexte que easieR (Stefaniak, 2018) commencé être développé, à cette époque où JASP (2025) n’était qu’à ses prémices et Jamovi (2025) n’existait pas encore. Si ces deux derniers logiciels se sont imposés dans de nombreux cours de statistiques des programmes de psychologie en France et à l’étranger, easieR se démarque dans sa conception à plusieurs égards expliquant pourquoi il continue à se justifier par rapport à ces alternatives.","code":""},{"path":"quel-logiciel-utiliser-et-pourquoi.html","id":"philosophie-sous-tendant-easier","chapter":"2 Quel logiciel utiliser et pourquoi ?","heading":"2.3.2 Philosophie sous-tendant easieR","text":"Le package easieR (Stefaniak, 2018) s’articule autour de 3 piliers philosphiques (voir Figure 2.1 :une démarche science ouverte ;un pipeline complet et cohérent dans la présentation des analyses ;des outils permettant de répondre aux besoins les plus fréquents des étudiant·es et des chercheur·euses en psychologie ;une structure pédagogique inhérente à sa conception.\nFigure 2.1: Piliers sur lesquels easieR s’appuie.\n","code":""},{"path":"quel-logiciel-utiliser-et-pourquoi.html","id":"une-démarche-science-ouverte","chapter":"2 Quel logiciel utiliser et pourquoi ?","heading":"2.3.2.1 Une démarche science ouverte","text":"Le package easieR s’inscrit dans une démarche sciences ouvertes de deux manières principales:Permettre d’adopter une approche multiverse de l’analyse des données (Steegen et al., 2016) ;Permettre d’utiliser des scripts reproductibles.En ce qui concerne l’approche multiverse de l’analyse des données, il s’agit d’analyser les données de différentes manières afin de s’assurer de la robustesse des résultats. Si l’interprétation des résultats diffère selon la manière dont l’analyse été réalisée, il semble indispensable de s’interroger sur ce que veulent vraiment dire les données. Cette approche est le contre-pied du p-hacking puisque les auteurs sont invités à présenter les résultats (par exemple en annexe) des différentes méthodes d’analyses qui peuvent être envisagées pour tester l’hypothèse et de montrer la cohérence entre ces différentes manières d’analyser les données et les résultats présentés.Quant au fait d’être reproductible et de faciliter la transition entre les outils statistiques à base de boîtes de dialogues vers ceux à base de les lignes de commande, cela est possible car easieR (Stefaniak, 2018) peut être utilisé tant en boîte de dialogue (ce qui est fait avec les étudiant·es) qu’en ligne de commande. Cette fonctionnalité représente l’originalité de easieR en tout qu’outil statistiques. En effet, si de plus en plus de revues scientifiques demandent de partager les codes des analyses, s’approprier un langage informatique est souvent compliqué. C’est le cas pour R (R Core Team, 2025). Ainsi, confrontés aux erreurs répétées que l’peut commettre en débutant à utiliser R (R Core Team, 2025) en ligne de commande, les apprenants peuvent considérer leur courbe d’apprentissage particulièrement lente, ce qui affectera de manière négative leur motivation. Dans le package easieR (Stefaniak, 2018), les erreurs ne sont pas sanctionnés par un message d’erreur mais par une boîte de dialogue qui prend le relais. Ainsi, même en cas d’erreur, l’analyse peut être réalisée, et surtout en comparant la ligne de commande générée automatiquement avec celle initialement utilisée afin d’identifier les causes qui ont empêché les lignes de commande d’être exécutée correctement.\nPar ailleurs, comme easieR est directement utilisé dans l’interface R et permet dès lors sensibiliser les utilisateur·ices à des notions comme le chargement d’une bibliothèque ou l’utilisation d’une fonction, tout en se familiarisant avec l’environnement.Enfin, il est possible de générer un rapport des résultats en html ou en word, ce qui facilite les copiés collers et éviter les erreurs de reports.","code":""},{"path":"quel-logiciel-utiliser-et-pourquoi.html","id":"une-conception-orientée-vers-la-pédagogie","chapter":"2 Quel logiciel utiliser et pourquoi ?","heading":"2.3.2.2 Une conception orientée vers la pédagogie","text":"Souvent les outils statistiques proposent une multitude d’options, mais les utilisateurs ne porte que très peu leur intérêt sur ces options car ces options ne font pas partie de leurs connaissances statistiques. La conception de easieR est fondamentalement différente à bien des égards :easieR est une suite de boîte de dialogue qui force à prendre une décision. Par défaut, la meilleure option (une des options parmi les meilleurs options possibles) statistiques est choisie. Néanmoins, il est possible d’envisager une alternative. Cette décision requiert alors une décision éclairée ;afin d’accompagner les utilisateurs à prendre leurs décisions statistiques, des informations sont affichées dans la console en parallèle de l’apparition des boîtes de dialogue de sorte à aiguiller la décision des néophytes ;lorsque la meilleure option statistique est peu connue par la plupart des personnes, les résultats plus habituels sont présentés à côté ou au-dessus de la solution optimale, ce qui facilite la comparaison et permet d’être initié à ces options alternatives, meilleures mais moins connues. Par exemple, pour une analyse factorielle, les corrélations polychoriques devraient être préférées pour des données ordinales et les corrélations tétrachoriques pour des données dichotomiques. Ce choix est celui par défaut lorsque le nombre de modalités est inférieur à 8 (pour les corrélations polychoriques) ou sont égales à 2 (pour les corrélations tétrachoriques).","code":""},{"path":"quel-logiciel-utiliser-et-pourquoi.html","id":"un-pipeline-cohérent-dans-la-sortie-des-résultats","chapter":"2 Quel logiciel utiliser et pourquoi ?","heading":"2.3.2.3 Un pipeline cohérent dans la sortie des résultats","text":"La philosophie inhérente à easieR (Stefaniak, 2018) est également quelque peu différente. En effet, contrairement à la plupart des packages de statistiques, la vérification des conditions d’application n’est pas une option et il n’est pas possible de l’éviter, tout comme il n’est pas possible d’ignorer les statistiques descriptives. Ces deux prérequis semblent être la base d’une analyse statistique critique puisqu’il n’est pas possible de donner sens aux résultats si les conditions d’applications des analyses ne sont pas respectées (vu que l’estimation des probabilités de dépassement sont inhérentes au respect de ces conditions d’application). De même, si ne regarde pas les statistiques descriptives, il n’est pas possible de s’assurer que les données correspondent aux données censées être analysées : est-ce que l’effectif est correct ? Est-ce que les valeurs minimales et maximales pour les variables quantitatives sont compatibles avec la mesure ? Ainsi, l’utilisateur d’easieR (Stefaniak, 2018) est guidé dans une démarche systématique d’analyse des données qui ne se limite pas à vérifier si la probabilité de dépassement est inférieure à 0.05. Afin de favoriser l’adopter de cette démarche systématique, toutes les analyses sont structurées selon un même schéma (voir Figure 2.2).\nFigure 2.2: Structure de l’analyse des données avec easieR.\nL’analyse commence systématiquement par la présentation des statistiques descriptives avec pour objectif :\n- d’identifier si travaille sur le bon nombre d’observations ;\n- s’assurer que les minimums et maximums sont compatiables avec les échelles qui ont été utilisées (par ex., ne peut pas avoir un QI de 245) ;\n- identifier les tendances d’un point de vue descriptif ;Ensuite, les conditions d’application sont systématiquement testées de sorte à ce que, en cas de violation de ces conditions d’application, l’utilisateur·trice soit amené·e à utiliser une analyse alternative ou à corriger ces violations quand c’est possible. L’objectif est également de représenter un signal d’alarme sur le fait que les conclusions perdent en robustesse dès lors que ces conditions ne sont pas respectées.La présentation de l’analyse principale avec les tailles d’effet et les intervalles de confiance afin de comprendre que l’essence des statistiques n’est pas d’avoir une valeur p inférieur à 0.05, mais d’avoir une taille d’effet qui fasse sens.La présentation des alternatives non paramétriques et/ou robustes, à savoir des bootsrap, des M-estimator ou encore des anovas sur les médianes, afin d’adopter une démarche multiverse (Steegen et al., 2016), consistant à s’assurer que les conclusions qui sont tirées ne dépendent pas de la manière dont les données ont été analysées mais aboutissent à la même conclusion, qu’importe l’outil statistique utilisé.L’identification de valeurs influentes qui pourraient avoir un impact sur les résultats et, le échéant, une réanalyse sans ces valeurs influentes afin de rester dans cette continuité de démarche multiverse d’analyse des données. Á ma connaissance, easieR est le seul outil utilisant des boîtes de dialogue qui intègre directement cette possibilité.","code":""},{"path":"quel-logiciel-utiliser-et-pourquoi.html","id":"répondre-aux-besoins-les-plus-courants-des-utilisateurs","chapter":"2 Quel logiciel utiliser et pourquoi ?","heading":"2.3.2.4 Répondre aux besoins les plus courants des utilisateurs","text":"Les fonctions principales de easieR sont résumées dans le tableau 2.1Table 2.1: Tableau synthétique reprenant les principales fonctions d'easieRChoix dans le menu principalFonction principaleAnalyse alternativeL'option utileDonnéesImporter (en différents formats)Rxporter (en différents formats)VisualiserGénérer un rapport de résultatsPréparation des donnéesFilrer des observationsSélectionner des variablesTransformer en rangsImputation de valeurs manquantesCentrer réduireTrierRéaliser des opérations mathématiquesPasser d'un format large au format longGraphiquesBoxplotNuage des pointsDensitéViolin plotHistogrammesAnalysesChi carré d'ajustementsimulation de Monte CarloChi carré d'indépendancesimulation de Monte Carlo, correction de continuitéContrastes pour identifier les cases qui s'écartent significativementTest de McNemarCorrection de continuitét de Student comparaison à une normetest de Wilcoxon, M-estimator,bootstrapt de Student pour échantillons appariésTest de Wilcoxon, M estimator, bootstrapGraphique selon les recommandations de Loftust de Student pour échantillons indépendantsMann-Whitney, correction de Satterthwaite, t sur les médianes, t sur les moyennes tronquées (avec ou sans bootstrap)ANOVA simple à groupes indépendantsKruskall-Wallis, correction de Welch, anova sur les médianes ou moyennes tronquéesTous les contrastes sont possiblesANOVA à mesure répétéesANOVA de Friedman, correction de Greenhouse et Geisser, anova sur les médianesTous les contrastes sont possiblesANOVA factoriellesanova sur les médianesTous les contrastes sont possiblesANOVA mixtesanova sur les médianesTous les contrastes sont possiblesAnalyse de covarianceTous les contrastes sont possiblesRégressionsBoostrap,M-estimatorEffets de modérations et  effets non linéairesCoefficient de concordanceAlpha de CronbachOmega de McDonaldCoefficient de corrélation intraclasseCoefficient de corrélations de PearsonRho de Spearman, tau de Kendall, boostrapAnalyse par groupe analyse complète ou matrice de corrélation (rectangulaire ou carrée)Analyse factorielle exploratoireEstimateurs robustesUtilisation des corrélations tétrachoriques/polychoriquesAnalyse factorielle confirmatoireEstimateurs robustesUtilisation des corrélations tétrachoriques/polychoriquesAnalyse en composantes principalesEstimateurs robustesUtilisation des corrélations tétrachoriques/polychoriques","code":""},{"path":"installer-le-logiciel-de-traitement-de-données.html","id":"installer-le-logiciel-de-traitement-de-données","chapter":"3 Installer le logiciel de traitement de données","heading":"3 Installer le logiciel de traitement de données","text":"","code":""},{"path":"installer-le-logiciel-de-traitement-de-données.html","id":"installer-r","chapter":"3 Installer le logiciel de traitement de données","heading":"3.1 Installer R","text":"La première étape consiste en l’installation de R. Le logiciel est en libre\ntéléchargement à l’adresse suivante : https://cran.r-project.org/ où vous trouverez le lien de téléchargement pour les différents systèmes d’exploitation.","code":""},{"path":"installer-le-logiciel-de-traitement-de-données.html","id":"je-suis-un-utilisateur-windows","chapter":"3 Installer le logiciel de traitement de données","heading":"3.2 Je suis un utilisateur windows","text":"Pour les utilisateurs windows, il faut se rendre sur http://cran.r-project.org/bin/windows/base/Une fois sur cette page, il faut cliquer « download R » dont le numéro de version évolue constamment.\nComme le fichier téléchargé est un fichier « exe », il faut double-cliquer sur le fichier.\nIl suffit ensuite de cliquer constamment sur suivant jusqu’à ce que le logiciel soit installé.","code":""},{"path":"installer-le-logiciel-de-traitement-de-données.html","id":"je-suis-un-utilisateur-apple","chapter":"3 Installer le logiciel de traitement de données","heading":"3.3 Je suis un utilisateur apple","text":"Pour les utilisateurs MacOS, il faut cliquer sur ce lienHyper important : vérifiez le type de puce sur votre ordinateur en cliquant sur la pomme en haut à gauche et puis “à propos de ce mac”. Si, à côté de puce, vous avez un nom du type M1, M2 ou M3, alors, vous devez télécharger et installé la version arm de R. Si, à côté de puce, vous avez un nom qui contient intel, vous devez télécharger, la version x86 de R.Important : si votre système d’exploitation (OS) est inférieur à Big Sur, vous ne pouvez pas installer la dernière version de R. Vous devez trouver une version de R compatible avec votre système d’exploitation à l’adresse suivante (voir https://cran.r-project.org/). Néanmoins, vous vous exposez à ce que une ancienne version de R ne soit pas compatible avec des packages que vous utiliserez. L’alternative moins risquée est d’utiliser la même procédure que celle préconisée pour les utilisateurs Linux et ChromebookUne fois téléchargé, il faut double-cliquer sur le fichier « pkg », Il suffit ensuite de cliquer sur suivant jusqu’à ce que le logiciel soit installé.Pour finaliser l’installation, vous devez encore installer XQuartz disponible à l’adresse suivante : https://www.xquartz.org/","code":""},{"path":"installer-le-logiciel-de-traitement-de-données.html","id":"je-suis-un-utilisateur-linux-ou-chromebook","chapter":"3 Installer le logiciel de traitement de données","heading":"3.4 Je suis un utilisateur Linux ou Chromebook","text":"Si, théoriquement, il est possible de faire l’installation (du moins sur les ordinateurs ayant un Linux comme système d’exploitation), n’étant pas expert de ces systèmes d’exploitation, La solution la plus simple est de créer un compte (gratuit) sur Rstudio cloud (https://login.rstudio.cloud). Cela évite toutes les difficultés d’installation.Dans votre cas, aucune installation n’est requise. Notez que ce document est écrit à partir d’une version bureau (donc sur un ordinateur personnel) de RStudio et qu’il peut y avoir des petites divergences entre la version bureau et la version cloud.","code":""},{"path":"installer-le-logiciel-de-traitement-de-données.html","id":"installation-de-rstudio","chapter":"3 Installer le logiciel de traitement de données","heading":"3.5 Installation de RStudio","text":"Pour aucun utilisateur, il n’est indispensable d’installer Rstudio. Néanmoins, pour les utilisateurs de R en ligne de commande, l’installation de Rstudio peut représenter un environnement plus simple d’utiliation.RStudio n’est rien d’autre qu’une interface graphique qui va rendre l’utilisation de R un peu plus agréable. R studio propose également quelques fonctionnalités d’importation, de navigation entre les graphiques et d’accès aux objets stockés dans la mémoire de R. Son utilisation n’est pas obligatoire mais est très vivement recommandée dès lors qu’veut travailler avec des scripts reproductibles.Rstudio est disponible gratuitement en téléchargement à l’adresse suivante : https://rstudio.com/products/rstudio/download/Qu’importe votre système d’exploitation, la procédure est la même : clique sur le lien de téléchargement. Une fois téléchargé, double-clique sur le fichier et sur suivant jusqu’à ce que Rstudio soit installé.","code":""},{"path":"installer-le-logiciel-de-traitement-de-données.html","id":"installation-de-easier","chapter":"3 Installer le logiciel de traitement de données","heading":"3.6 Installation de easieR","text":"eaieR est une interface graphique qui peut être directement utilisée dans R et il n’est pas indispensable d’avoir installé Rstudio.La procédure pour installer easieR est la suivante :vous avez téléchargé et installé R2 en fonction de votre système d’exploitation. Pour les utilisateur·ices macOS, n’oubliez pas d’installer XQuartz.vous avez téléchargé et installé R2 en fonction de votre système d’exploitation. Pour les utilisateur·ices macOS, n’oubliez pas d’installer XQuartz.Télécharger et installer PandocTélécharger et installer Pandocvous ouvrez R et dans la fenêtre, vous copier coller le code ci-dessous :vous ouvrez R et dans la fenêtre, vous copier coller le code ci-dessous :Si vous recevez un message indiquant que certains packages peuvent être mis à jour, choisissez l’option 2 (qu’il faut taper dans la console), CRAN .Il est important de noter que, quand trop d’utilisateurs se connectent en même temps à github, github peut empêcher l’installation car il y trop de requêtes en même temps. Il faut alors utiliser la solution alternative, ci-dessous.","code":"\ninstall.packages(\"devtools\")\ndevtools::install_github(\"nicolasstefaniak/easieR\", type=\"binary\") \n  # 1. Télécharger le zip depuis GitHub\n  url <- \"https://github.com/NicolasStefaniak/easieR/archive/refs/heads/master.zip\"\n  temp_zip <- tempfile(fileext = \".zip\")\n  download.file(url, destfile = temp_zip, mode = \"wb\")\n\n  # 2. Dézipper dans un dossier temporaire\n  temp_dir <- tempdir()\n  unzip(temp_zip, exdir = temp_dir)\n\n  # 3. Identifier le dossier du package\n  package_dir <- file.path(temp_dir, \"easieR-master\")\n\n  # 4. Installer le package\n  devtools::install(package_dir, subdir = subdir)\n\n  # 5. Nettoyer (optionnel)\n  file.remove(temp_zip)"},{"path":"installer-le-logiciel-de-traitement-de-données.html","id":"easier-ne-sinstalle-pas-correctement","chapter":"3 Installer le logiciel de traitement de données","heading":"3.6.1 easieR ne s’installe pas correctement","text":"Si vous êtes sur windows, le problème le plus fréquent est que votre compte windows est associé à des caractères spéciaux, tels que “à”,“ç”,“é”,“è”,“ê”… Par exemple, votre compte s’appelle élève. La solution la plus simple est de créer une nouvelle session sur votre ordinateur en passant par :\n- panneau de configuration\n- comptes d’utilisateurs\n- créer un nouveau compte\nPensez à accorder les droits administrateurs à ce compte pour pouvoir installer tout correctement.Un autre problème que vous pourriez avoir est de manquer d’espace sur votre disque dur. La solution à ce problème est plus technique et nécessite d’avoir un support externe pour stocker les packages. Voici la procédure :créer un dossier sur le support externe où les packages seront stockés (imaginons que le chemin d’accès est D:/packages);créer un script R que vous enregistrerez dans document avec le nom .Rprofile (exactement tel que je l’ai écrit)dans demandez à R où sont stockés les packages avec la fonction .libPaths()Dans le document .Rprofile créé, ajouter les deux chemins d’accès suivants (le premier vous est fourni par .libPaths() et le second est celui que vous avez créé) ;Reprendre l’installation de easieR à install_githubEnfin, pour tous les utilisateur·ices, un des problèmes les plus fréquents est que tous les logiciels n’ont pas été installé. Vous pouvez vérifier que pandoc été installé correctement par :Vous avez que pandoc est installé si vous avez une autre valeur que ‘0’.Remarque : il est possible que vous ayez installé pandoc mais que R ne le prenne pas en compte parce que vous avez installé pandoc après avoir ouvert R. Il suffit de fermer R, de le rouvrir et de revérifier.Pour les utiliteurs macOS, il faut aussi s’assurer que XQuartz est correctement installé. Pour cela, allez dans votre répertoire des applications et dans le dossier Utilitaires (le nom peut potentiellement changer en fonction de la version de l’OS). Si vous ne vous XQuartz ni dans vos applications, ni dans aucun des sous-dossiers, c’est que ce n’est pas installé.","code":"\n.libPaths()\n.libPaths(c(\"C:/Program Files/R/R-4.5.2/library\", #chemin existant préalablement \n            \"D:/packages\"))# nouveau chemin pour les packages\nrmarkdown::pandoc_version()## [1] '3.6.3'"},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"débuter-avec-son-logiciel-de-statistiques","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4 Débuter avec son logiciel de statistiques","text":"","code":""},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"débuter-avec-r","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4.1 Débuter avec R","text":"","code":""},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"lenvironnement-r-dans-rstudio","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4.1.1 L’environnement R dans RStudio","text":"La Figure 4.2 représente la fenêtre qui s’ouvre lorsque vous lancez RStudio,\nFigure 4.1: Fenêtre d’accueil dans RStudio.\nTrois parties apparaissent :la console (grosse flèche noire) ;l’environnement (l’encadré jaune) ;et les graphiques (l’encadré vert).Il est possible de taper des lignes de commandes directement dans la console. Ainsi, vous pourriez par exemple utiliser la console comme calculatrice. Il va de soi que R offre bien plus de possibilités que de l’utiliser comme calculatrice. Se servir de R en l’utilisant comme une calculatrice reviendrait approximativement à acheter le dernier smartphone haut de gamme pour s’en servir comme réveil. Néanmoins, quand débute, il est pédagogiquement nécessaire de comprendre comment R fonctionne et ces exemples simplistes permettent d’y contribuer.Ainsi, si vous tapez dans la console 5+3 et que vous appuyez sur la touche ‘entrée’ de votre clavier, voici ce que vous allez obtenir :","code":"\n5+3## [1] 8"},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"le-script","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4.1.2 Le script","text":"Remarquez que nous avons utilisé R de manière triviale en demandant de réaliser une opération arithmétique simple. Cependant, quand le traitement des données va commencer à se complexifier et qu’il faudra utiliser plusieurs fonctions (la notion de fonction sera explicitée un peu plus loin) pour obtenir les résultats souhaités, vous conviendrez que cette pratique sera loin d’être optimale. Tout d’abord, ce ne sera pas pratique de devoir retrouver, chaque fois que vous en aurez besoin, les fonctions qui vous seront utiles et les réécrire. En d’autres termes, la console ne permet pas de garder aisément une trace des opérations qui ont été faites. , il semble évident qu’il est utile d’avoir une trace de ce que vous faites, avec éventuellement la possibilité d’y faire vos propres annotations pour vous aider à vous rappeler la manière dont vous devez utiliser la fonction ou à quoi elle sert. Par ailleurs,pour aller retrouver un résultat que vous avez obtenu précédemment, naviguer dans la console sera fastidieux. Enfin, si vous vous trompez et qu’un message d’erreur apparaît, il n’est pas commode d’utiliser la console pour la corriger.C’est pour cette raison qu’utiliser R directement dans la console est à proscrire. Votre courbe d’apprentissage serait bien plus lente et la démotivation vous gagnerait rapidement.Une manière bien plus adaptée d’utiliser R dans RStudio est d’utiliser un script. Pour cela, il faut cliquer sur la feuille blanche entourée d’un cercle rouge sur la Figure 4.2, et choisir dans le menu déroulant ‘R script’. Vous obtenez alors une nouvelle fenêtre (voir Figure 4.3) :\nFigure 4.2: Procédure de création d’un script dans R.\n\nFigure 4.3: Procédure de création d’un script dans R.\nNous allons donc commencer par donner un titre à notre script. Toute ligne ou tout information précédée par un dièse (#) est interprété comme du commentaire. Nous pourrions donc donner comme nom au script “Introduction à R”. En l’entourant de dièse, il ressortira et permettra d’identifier rapidement de quoi traite le script.Alternativement, vous pouvez créer des chapitres dans Rstudio en cliquant sur Code et ensuite sur Rstudio. En donnant un nom à la section, vous pourrez par la suite naviguer aisément entre les sections de votre script en cliquant sur code et ensuite sur jump .Vous l’aurez compris, vous devriez faire un script pour chaque chapitre ou faire un seul script. Sachant que les scripts peuvent être aussi long que vous le souhaitez, il est théoriquement possible de ne faire qu’un seul et unique script avec toutes les thématiques. Néanmoins, il vous sera plus facile de faire et d’utiliser des scripts plus courts qui ne traitent que d’une thématique.Á présent que nous avons une section, nous allons pouvoir découvrir les opérations de base qu’fait habituellement avec une calculatrice.Dans le script, contrairement à ce qui se passe avec la console, rien ne se passe lorsque vous écrivez une ligne de commande. La raison est que la console interprète directement la ligne de commande quand appuie sur entrée alors que le script ne le fait pas. La ligne de commande sera exécutée uniquement quand vous en ferez la demande explicite. Pour cela, il y deux possibilités : soit vous sélectionnez une ligne (ou un ensemble de lignes à devoir exécuter), et vous cliquez sur ‘Run’ en haut à droite de la fenêtre du script ; soit vous sélectionnez une ligne (ou un ensemble de lignes à devoir exécuter), et vous appuyez sur les touches ctrl et entrée (pomme et entrée pour les mac users) de votre clavier. Dans les deux cas, les résultats vont s’afficher dans la console et vous obtiendrez ceci :Avant de continuer, pensez à enregisgtrer votre script. Quand vous cliquez sur la petite disquette, RStudio vous propose de choisir un type d’encodage. Vous pouvez laisser la valeur par défaut, ou éventuellement choisir “UTF-8” (voir Encadré 1 pour plus de détails).\nLe type d’encodage des scripts \nSur MacOS et sur Linux, “UTF-8” est la valeur par défaut. Pour les utilisateurs windows, la valeur par défaut est ‘ISO-8859-1’. Concrètement, cela n’pas de réel impact tant que vous n’utilisez pas de caractères spéciaux, c’est-à-dire et sans être exhaustif l’un des caractères suivants : é,è, ê, à, ç. En revanche, dès qu’il y des caractères spéciaux, les choses vont se compliquer. Donc si vous commentez en anglais, pas de souci d’utiliser l’un ou l’autre ; en français, cela peut poser souci. Tant que vous êtes le seul utilisateur du fichier, il sera lisible. En revanche, si vous souhaitez partager votre script, les caractères spéciaux ne seront pas lisibles si l’encodage n’est pas bon. Bref, si un utilisateur MacOS travaille avec un utilisateur Windows, il est préférable d’utiliser “UTF-8” qui est considéré comme un format universel. Si vous ouvrez un script où certains caractères ne sont pas lisibles, vous pouvez le rendre lisible en demandant à Rstudio de rouvrir le script avec le bon encodage. Pour cela, clique sur ‘file’, ensuite sur ‘reopen encoding’ et choisit l’encodage.Á présent que vous avez choisi l’encodage, vous pouvez donner un nom à votre script. Pour pouvoir le réutiliser par la suite, il faut que l’extension du fichier, c’est-à-dire ce qui vient après le dernier point soit un R majuscule. Ainsi, j’ai donné le nom suivant à mon fichier : Introduction_R.R.","code":"\n###################################\n###      Introduction à R      ####\n###################################\n5+3 # Addition \n5-3 # Soustraction \n5*3 # multiplication \n5/3 # division \n5^3 # exposant. Donc, ici, 5 exposant 3## [1] 8## [1] 2## [1] 15## [1] 1.666667## [1] 125"},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"créer-des-sections-et-annoter-votre-script","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4.1.3 Créer des sections et annoter votre script","text":"Peut-être avez-vous remarqué que j’ai mis à côté de chaque opération un dièse avec une information textuelle. Cela m’permis d’expliquer à quoi correspondait chacune des lignes de commande. Cette opération peut se faire en annotant à côté de la ligne de commande ou au-dessus de celle-ci (il est théoriquement possible d’annoter en-dessous mais ce n’est pas une procédure conventionnelle).Même si l’utilité d’une fonction, ou son utilisation vous semble triviale, prenez le réflexe d’annoter votre script. Les annotations servent à vous rappeler à quoi sert une fonction, à expliquer à d’autres personnes (y compris à vous-même plus tard) ce que vous avez fait et à indiquer les difficultés que vous avez rencontrées avec la fonction et la manière d’y faire face.Á présent que nous avons succinctement abordé l’utilisation de R comme s’il s’agissait d’une calculatrice, nous pourrions vouloir commencer à utiliser R de manière un peu plus élaborée. va donc aborder une nouvelle thématique.Vous pourrez (et c’est recommandé) créer des délimitations dans votre script afin d’indiquer ces changements de thématique. Ces délimitations s’appellent des sections. Vous pourriez ainsi utiliser des sections pour identifier chacun des sous-chapitres. Pour créer une section, le plus simple est de cliquer sur ‘Code’ en haut de la fenêtre, et ensuite sur ‘insert section’. L’alternative est de créer la section en appuyant simultanément sur les touches ‘Ctrl’, majuscule et ‘R’. Ainsi, en étant au-dessus des lignes de commandes relatives aux opérations mathématiques, vous pourriez créer une première section intitulée ’utiliser R comme une calculatrice et vous verriez cette ligne apparaître dans votre script (voir Figure 4.4). Il faut noter que vous pouvez directement recopier cette ligne et adapter le titre pour créer de nouvelles sections.\nFigure 4.4: Créer une section intitulée : utiliser R comme calculatrice\n\nNaviguer entre les sections de votre script. \nAstuce : si vous cliquez à présent sur ‘Code’ en haut de votre écran et ensuite sur ‘Jump ’, vous allez voir les différentes sections de votre script et vous pourrez naviguer aisément entre ces différentes parties.","code":"\n# Addition\n5+3  \n# Soustraction \n5-3 \n# multiplication \n5*3 \n# division \n5/3 \n# exposant. Donc, ici, 5 exposant 3\n5^3 \n# Utiliser R comme une calculatrice ---------------------------------------"},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"les-fonctions","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4.1.4 Les fonctions","text":"Les fonctions représentent des lignes de commande qui permettent de réaliser des tâches particulières. Par exemple, il y une fonction pour calculer une moyenne, il y une fonction aussi pour compter le nombre d’observations.\nEn réalité, il y des milliers de fonctions dans R. Dans cette section, nous allons décrire quelques fonctions de manière succinctes, dont certaines feront l’objet d’un approfondissement dans les chapitres suivants.D’un point de vue formel, une fonction un nom. Par exemple, la fonction pour calculer une moyenne est mean. Le nom de la fonction est suivi de parenthèses. Á l’intérieur de ces parenthèses, vous allez spécifier ce qu’appelle les arguments. Si je reviens à la fonction qui permet de calculer la moyenne, un des arguments de la fonction consiste à fournir les valeurs numériques sur lesquelles la moyenne doit être calculée.Commençons avec deux fonctions assez simples car elles n’ont pas d’argument, la fonction getwd permet de savoir dans quel répertoire est en train de travailler, et la fonction dir qui permet d’obtenir la liste des fichiers disponibles dans ce répertoire.Pour aborder ces fonctions dans le script, nous allons commencer par créer une nouvelle section que nous allons intituler Les fonctions de base.Nous pouvons à présent utiliser ces deux fonctions sans argument :Par défaut, R va utiliser le dossier ‘documents’ de votre ordinateur comme répertoire de travail, dont le chemin d’accès prendra approximativement la forme de C:/Users/votre_nom/Documents. Ce répertoire de travail n’est sans doute pas le répertoire dans lequel sont localisées vos données, ni celui où vous souhaiteriez travailler. Avoir comme répertoire de travail le répertoire dans lequel se situe l’ensemble des documents est une règle de bonne pratique. Vous pourriez, par exemple, créer dans vos documents un dossier intitulé Statistiques3 et à l’intérieur de ce dossier, un sous-dossier appelé “Introduction” qui contiendra le script pour ce chapitre et les données qui seront utilisées ultérieurement.Il faut à présent préciser à R que votre répertoire de travail ce situe par exemple dans C:/Users/votre_nom/Documents/Statistiques/Introduction. Pour cela, nous allons utiliser la fonction setwd. Cette fonction un argument. Cet argument (code>dir pour directory en anglais) est le répertoire dans lequel vous souhaitez travaillerVous venez d’essayer de changer le répertoire de travail et vous avez reçu un message d’erreur qui prend la forme suivante :Vous avez probablement commis une ou plusieurs des erreurs suivantes :Le dossier n’existe pas. Vous avez oublié de créer le dossier dans lequel vous voulez travailler. Le dossier doit déjà exister sur votre ordinateur ;Le chemin n’est pas correct. Vous avez fait une faute de frappe, ou oublié un sous-répertoire dans le chemin d’accès ;Vous n’avez pas respecté la casse. Comme pour les mots de passe, R est sensible à la casse, c’est-à-dire au respect des majuscules et des minuscules ;Vous êtes sur Windows et vous avez copier/coller le chemin d’accès depuis la fenêtre de navigation. Cependant, R souhaite le symbole slash pour accéder à un dossier enfant, c’est-à-dire un dossier imbriqué dans un autre dossier alors que windows utilise par défaut le backslash. Il faut donc inverser tous les backslash en slash.\nTravailler directement dans le bon environnement de travail. \nIl est possible de se trouver directement dans le bon environnement de travail sans à avoir à le préciser. Pour cela, il faut créer un projet. Dans Rstudio, il suffit de clquer sur File et ensuite New Project. Après l’avoir sauvegardé à l’endroit pertinent, il suffira d’ouvrir sur le fichier créer pour que le répertoire de travail soit automatiquement celui dans lequel le projet été enregitré.Une partie de ces fonctions sont disponibles directement lorsqu’installe R, mais la grande majorité de ces fonctions ont été programmées par les contributeurs qui ont été évoqués à la section expliquant pourquoi il est pertinent d’utiliser R. Ces contributeurs n’ajoutent pas directement les nouvelles fonctionnalités au logiciel mais vont empaqueter les fonctionnalités qu’ils ont programmées dans ce qu’appelle un package. Ces packages devront être installés en plus pour pouvoir être utilisés.Si vous avez du mal à comprendre ce qu’est un package, pensez à votre smartphone : quand vous l’allumez pour la première fois, il n’y que les fonctionnalités de base et vous allez installer les applications selon vos besoins. R fonctionne sur un principe un peu similaire, à la différence qu’ajoute ces nouvelles fonctionnalités à l’intérieur d’un logiciel. Dans un premier temps, nous allons utiliser le package readxl (Wickham & Bryan, 2022) pour importer des fichiers excel, et le package dplyr (Wickham et al., 2022) pour avoir des fonctionnalités de manipulation des données. Nous allons donc installer et charger ces packages. Pour installer un package, utilise la fonction install.packages. Cette fonction pas mal d’arguments mais il ne semble pas utile de les expliciter tous en détail. Nous nous limiterons à installer un package de manière classique en invoquant simplement le nom du package à installer.Vous savez que les packages sont installés correctement par le fait que R vous indique que le package été décompressé avec succès et que la somme des MD5 été vérifiée (package ‘readxl’ successfully unpacked MD5 sums checked). Il arrive néanmoins que certains packages ne s’installent pas aussi aisément. Nous renvoyons le lecteur vers la section “installer les packages” pour une description plus détaillée de cette fonction et les difficultés qui peuvent être associées à l’installation de certains packages.Remarquez qu’il est possible d’utiliser une fonction avec des arguments sans invoquer l’argument de manière explicite. Dans la fonction install.packages, le premier argument s’appelle ‘pkgs’. Il s’agit du nom du package à installer. Si vous respectez l’ordre des arguments, il n’est pas indispensable de les appeler explicitement (comme je l’ai fait pour l’installation du package dplyr) mais si vous ne respectez pas l’ordre ou si vous omettez des arguments intermédiaires pour lesquels vous souhaitez garder les valeurs par défaut, alors il faut les appeler explicitement. En l’occurrence, l’argument pkgs est le premier argument et je peux donc permettre d’omettre de le spécifier quand j’utilise la fonction. En revanche, c’est une pratique qu’il est préférable d’utiliser uniquement avec les fonctions qu’connait parfaitement car cela pourrait entraîner des messages d’erreur ou des résultats qui ne correspondent pas à ce qui était souhaité.Á présent que nos deux packages sont bien installés, nous pouvons les charger pour pouvoir utiliser leurs fonctionnalités ultérieurement. les charge avec la fonction library en indiquant le nom du package sans les guillemets.Charger les packages dont va avoir besoin en début de script représente également une règle de bonne pratique.\nTrouver de l’aide sur une fonction. \nComme évoqué précédemment, il existe une large communauté d’utilisateurs de R qui s’entraident en ligne. Néanmoins, si vous connaissez la fonction que vous devez utiliser, la stratégie de première intention que vous devriez utiliser pour comprendre comment utiliser une fonction est de consulter l’aide qui lui est associée. En effet, cela fait partie du cahier de charge de R de fournir une documentation détaillée quant à l’utilisation des fonctions, avec une explication de chacun des arguments et des exemples sur la manière d’utiliser la fonction. Pour accéder à cette aide, il existe deux manières : utiliser le point d’interrogation suivi du nom de la fonction ou utiliser la fonction help avec le nom de la fonction entre parenthèse.Cette manière de trouver de l’aide sur l’utilisation d’une fonction est une stratégie indispensable car, pour l’utilisateur débutant, l’utilisation des fonctions n’est pas suffisamment automatique et l’aide permet de fournir les indices indispensables pour savoir comment utiliser une fonction et, pour l’utilisateur expert, il se rappellera du nom d’une fonction qu’il n’utilise pas souvent mais pas forcément du nom des arguments ou de la manière de les utiliser.En revanche, cette stratégie ne fonctionne que si la fonction est accessible dans R. Vous ne pourriez pas trouver de l’aide de cette manière pour une fonction qui est dans un package qui n’est pas installé ou qui n’est pas chargé. Pour trouver de l’aide sur une fonction d’un package installé mais pas chargé, peut utiliser deux points d’interrogation successifs avant le nom de la fonction. R va alors chercher dans tous les packages qui sont installés s’il trouve une fonction qui contient le nom que vous rechercher. Á des fins d’illustrations, installez le package psych (Revelle, 2025) sans le charger et vous pourrez trouver de l’aide sur la fonction describe en utilisant les deux points d’interrogation de la manière suivante :","code":"\n# Les fonctions  de base ---------------------------------------\ngetwd() # permet d'obtenir le répertoire de travail\ndir() # permet d'obtenir la liste des fichiers disponibles dans le répertoire \n# Ligne de commande permettant de préciser le répertoire de travail\nsetwd(dir=\"C:/Users/XXX/Documents/livre de statistiques/Introduction\") # Attention que \"XXX\" renvoie à votre nom et que vous devez adapter ce chemin d'accès à votre ordinateurError in setwd(\"C:/Users/XXX/Documents/Statistiques/Introduction\") : cannot change working directory\ninstall.packages(pkgs = \"readxl\") # installation du package readxl - en utilisant l'argument pkgs\ninstall.packages(\"dplyr\") # installation du package dplyr - en omettant l'argument pkgs\nlibrary(readxl) # chargement du package readxl \nlibrary(dplyr) # chargement du package dplyr \n?setwd # recherche de l'aide pour la fonction setwd en utilisant le point d'interrogation \nhelp(install.packages) # recherche de l'aide pour la fonction setwd en utilisant la fonction help\n??describe # cherche l'aide d'une fonction appelée describe dans n'importe quel package installé sur votre ordinateur, même s'il n'est pas chargé. "},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"les-objets-dans-r","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4.1.5 Les objets dans R","text":"Lorsque vous utilisez une des lignes de commande décrite ci-dessus, une fois le résultat obtenu, ce résultat est devenu inutilisable car les informations n’ont pas été stockées dans la mémoire de R. , il pourrait être utile de pouvoir réutiliser le résultat produit. Par exemple, lorsque vous utilisez la fonction dir, vous avez la liste des fichiers disponibles dans votre dossier. Imaginons que ces fichiers soient les résultats individuels de chacun de vos participants et que vous avez 200 participants. Cela pourrait être plus que fastidieux de devoir importer les données de chaque participant les uns après les autres. voudrait pouvoir les importer en une seule fois. Cependant, si la liste des fichiers n’est plus utilisable, ne peut pas le faire. Il est donc nécessaire de stocker ces informations dans la mémoire de R. fait cela en créant des objets.Il existe trois manières de stocker un objet dans la mémoire de R : le symbole ‘<’ et ‘-’ pour créer une flèche orientée vers la gauche ; le symbole ‘>’ et ‘-’ pour créer une flèche orientée vers la droite ; le symbole ‘=’. Nous allons illustrer la création d’objets avec ces trois méthodes. Dans un premier temps, nous allons attribuer la valeur 5 à un objet appelé ‘’. Je vais attribuer la valeur 3 à un objet que je vais appeler ‘b’ et enfin, l’objet ‘c’ sera l’addition entre et b.De ces 3 méthodes que nous avons évoquées pour créer des objets, l’utilisation du symbole ‘=’ est à déconseiller car il est préférable d’utiliser exclusivement cette procédure pour l’attribution des valeurs à un argument d’une fonction. Par exemple, lorsque nous avons utilisé la fonction setwd, il fallait préciser le répertoire de travail, qui est l’argument ‘dir’, et nous l’avons fait avec le symbole ‘=’.L’utilisation des deux autres méthodes se valent mais préférez faire vos flèches toujours dans le même sens pour vous éviter des écueils. Je vous encourage à les faire toujours vers la gauche (comme pour le ‘’ dans le code ci-dessus) afin d’avoir le nom des nouveaux objets en début de ligne à chaque fois. Quand vous ferez plusieurs opérations à la suite, cela vous évitera de passer du temps à chercher le nom que vous avez attribué à un objet 10 lignes de commandes plus haut.Si exécute les trois lignes de commandes ci-dessus, apparemment rien ne se passe. Il n’y rien qui s’affiche dans la console. Pour voir ce que contiennent les objets, il faut les appeler. Ainsi, en tapant leur nom et en cliquant sur ‘run’, vous allez voir ;Le nom de ces objets est pratiquement complètement arbitraire et vous pourriez l’appeler ‘ficus’, ‘Xmen’, ‘HarryPotter’ si vous le souhaitiez. En revanche, si vous l’appelez ‘HarryPotter’, pour pouvoir l’utiliser, il faudra écrire ‘HarryPotter’. Il est donc préférable d’utiliser un nom court pour éviter de devoir retaper des noms interminables.Et si vous faites une faute de frappe (y compris sur la casse4), alors R va renvoyer un message d’erreur indiquant qu’il ne trouve pas l’objet appelé (Error: object ‘Harrypotter’ found).Même si peut utiliser pratiquement n’importe quel nom pour les objets, certaines règles doivent impérativement être respectées. D’abord, vous ne pouvez pas utiliser un nom qui contient des espaces. Dans ce cas, R va renvoyer un message d’erreur indiquant qu’il y un symbole inattendu. Ce symbole est l’espace. En revanche, vous pouvez utiliser un underscore ou un point (mais il est en revanche prohibé d’utiliser une virgule, un point-virgule ou un deux points) pour faire vos séparations.Ensuite, il faut éviter tous les caractères spéciaux. va commencer par la situation la plus pernicieuse : les accents, les ‘ç’, et autres trémats. Je vais attribuer la valeur “Jean” à un objet appelé “élève”Là, tout va bien. Cependant, certaines fonctions ne vont pas reconnaître les accents et vous ne comprendrez pas pourquoi la fonction renvoie un message d’erreur alors que votre objet ne présente aucun problème en apparence (ici le souci est lié au type d’encodage des caractères spéciaux, dont nous avons parlé précédemment). Vous pourriez passer des heures à vous demander quel est le problème et vous finiriez par vous décourager pour un problème aussi trivial. La règle à appliquer est simple : n’utilisez jamais d’accent ou de caractères spéciaux.La dernière règle à appliquer est qu’il ne faut aucun symbole :pas de parenthèse, R considérerait ce qui précède comme une fonction ;pas d’apostrophe car ce serait interprété comme une chaîne de caractères ;pas de symbole mathématique car il voudrait réaliser une opération mathématique.Voici quelques exemples de ce qu’il ne faut pas faire :Remarquez également que, dans votre environnement global (l’encadré jaune de la Figure 4.2), vous avez à présent plusieurs objets que vous pouvez réutiliser pour des traitements ultérieurs. Si pour des opérations aussi triviales que celle qu’vient de voir, cela peu d’intérêt, n’oubliez pas que l’objectif sera fine de pouvoir utiliser des modèles statistiques sur des jeux de données réels et qu’il sera important de pouvoir passer par des étapes intermédiaires.En l’occurrence, il y peu d’objets dans la mémoire de R. Cependant, il peut y en avoir beaucoup plus et vous pourriez vouloir en obtenir la liste. Pour cela, utilise la fonction ls sans devoir préciser d’argument.","code":"\na<-5 # créer un objet appelé a et ayant comme valeur 5\n3 ->b # créer un objet appelé b et ayant comme valeur 3\nc=a+b # créer un objet appelé c et ayant comme valeur la somme de a et b\n# Identifier ici l'argument 'dir' à l'intérieur de la fonction et que le symbole utilisé pour lui attribuer une valeur est '='\nsetwd(dir=\"C:/Users/XXX/Documents/Statistiques/Introduction\")\na # appeler l'objet a, renvoie 5## [1] 5\nb # appeler l'objet b, renvoie 3## [1] 3\nc # appeler l'objet c, renvoie 8## [1] 8\n# Illustration du côté arbitraire des noms \nficus<-5\nXmen<-5\nHarryPotter<-5\nficus## [1] 5\nXmen## [1] 5\nHarryPotter## [1] 5\nHarrypotter # Harrypotter avec un \"p\" minuscule n'existe pas et renvoie un message d'erreurMa valeur cinq<-5 # renvoie un message d'erreur en raison des espaces \nMavaleurcinq<-5 # ne renvoie pas de message d'erreur \nMa_valeur_cinq<-5 # ne renvoie pas de message d'erreur quand on utilise un underscore\nMa.valeur.cinq<-5 # ne renvoie pas de message d'erreur quand on utilise un point\nélève<-\"Jean\"\nélève## [1] \"Jean\"# Quelques exemples de ce qu'il ne faut pas faire \ninterprete(ma.fonction)<- 5 # ne fonctionne pas et renvoie le message d'erreur indiquant que cette fonction ne peut être trouvée\nc'est_pas_terrible<-5 # l'apostrophe pose souci \nceci+cela+non+plus<-5 # le '+' est interprété comme une opération mathématique, de même s'il y avait '-','/','*','^'. \npas_d_egal=erreur<-5 # mettre un \"=\" pose souci car on crée deux objets différents : pas_d_egal et erreur  \nls() # connaître la liste des objets dans la mémoire de R.##  [1] \"a\"               \"arrows\"          \"b\"               \"boxes\"          \n##  [5] \"c\"               \"cards\"           \"condition_label\" \"easier\"         \n##  [9] \"élève\"           \"ficus\"           \"ft\"              \"HarryPotter\"    \n## [13] \"lines\"           \"tot\"             \"tot2\"            \"Xmen\""},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"les-erreurs-les-plus-courantes-et-les-règles-de-bonnes-pratiques","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4.1.6 Les erreurs les plus courantes et les règles de bonnes pratiques","text":"Le plus difficile quand débute avec R est d’être confronté à un message d’erreur et de ne pouvoir lui attribuer du sens.Il va de soi qu’il n’est pas possible de faire une liste exhaustive des erreurs possibles mais voici les plus fréquentes, et leurs solutions :Table 4.1: Liste des erreurs les plus fréquentes et leur cause/solutionErreurSignification et solutionunexpected symbolLe nom de l'objet ou de la variable est incorrect. Vérifiez les règles énoncées précédemmentobject XXX foundL'objet n'pas été créé ou est mal orthographie. Attention à la casse - càd. aux majuscules et minusculescould find function XXXLa fonction est mal orthographiée ou est dans un package non chargé, voire non installé. Cette erreur est courante quand utilise une fonction qu'trouve sur le internet sans vérifier le package auquel elle appartient. Chargez le package contenant la fonction si elle existe.Error cor.test.default(c(\"\", \"b\")) : x doit être un vecteur numériqueLes arguments doivent avoir certaines caractéristiques. Ici, ils doivent être numériques mais ce n'est pas le cas. Vérifiez avec la fonction class ou str si les objets sur lesquels vous souhaitez faire l'analyse ont le format correctargument length zeroUn argument indispensable pour valeur NULL alors qu'il doit être spécifié. Vérifiez l'aide de la fonctionno default argumentUn argument nécessaire n'pas été spécifié. Vérifiez l'aide de la fonctionaucun package nommé XXX n'est trouvéLe package n'est pas installé ou est mal installé. Si vous avez tenté d'installer le package mais que le souci persiste, voir la section installer les packages récalcitrants ens section 6.Néanmoins, en appliquant les règles de bonnes pratiques suivantes, vous diminuerez considérablement les erreurs :Utiliser un script pour toutes les fonctions que vous utilisez.Avant de commencer les analyses, pensez à charger les packages et vérifier qu’il n’y pas de problème concernant leur chargement5.Annotez votre script pour savoir ce que vous faites, mais également pour expliquer les erreurs auxquelles vous avez été confrontées et comment cela été résolu.Faites attention à la casse6.Utilisez des noms simples et pour les noms complexes, favorisez les copier coller en utilisant la fonction ls pour avoir le nom des objets en mémoire et la fonction names pour avoir le nom des variables. Par exemple :Enfin, les outils d’intelligence artificiel représente des aides précieuses pour vous aider à identifier pourquoi une erreur est survenue et pour vous aider à corriger une ligne de commande. Le danger est de trop se reposer sur l’IA sans comprendre ce qu’fait. Dans ce genre de situation, il peut arriver que l’IA ne vous fournisse pas une solution adaptée, même avec un bon prompt, et vous ne pourrez pas régler la difficulté à laquelle vous êtes confronté·e. Il est donc indispensable de comprendre en premier et de profiter de l’aide de l’IA dans un second temps.","code":"\ndata(mtcars)\nnames(mtcars)##  [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n## [11] \"carb\""},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"installer-des-packages-récalcitrants","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4.1.7 Installer des packages récalcitrants","text":"Précédemment, nous avons décrit comment installer des packages. Néanmoins, certains peuvent être plus difficiles à installer que d’autres pour différentes raisons. Voici les solutions à envisager pour installer ces packages. Pour l’illustration, nous utiliserons le package ‘devtools’ (Wickham et al., 2025).Ainsi, quand un package ne veux pas se charger, c’est qu’il n’est pas correctement installé. La procédure ci-dessus explique comment forcer cette installation lorsque les choses ne se passent pas correctement. Chaque étape peut se suffire à elle-même. Néanmoins, si l’installation continue à poser problème, alors, il faut passer à l’étape suivante. Pour le formuler autrement : je tente l’étape 1, ça marche, je m’arrête ; ou, je tente l’étape 1, ça ne marche pas, je passe à l’étape 2.Forcer l’installation des dépendances, le site miroir et les packages compilés. Quand nous avons décrit la fonction install.packages, elle été présentée de sorte à pouvoir l’utiliser sans argument. peut néanmoins préciser des arguments supplémentaires. Le premier argument est l’argument dependencies qui permet de forcer l’installation des dépendances en indiquant l’opérateur logique TRUE. Le second argument qu’va utiliser est l’argument type pour forcer à installer les packages sous une forme compilée (binaire) plutôt que sous une forme où le package devra être compilé (source). Ainsi, cela évite de devoir avoir un logiciel de compilation, comme Rtools. Enfin, va préciser le site miroir sur lequel veut télécharger les packages qu’souhaite installer. Dans cet exemple, il s’agit du CRAN de Lyon.Fermer R et refaire l’étape 1. Il arrive que certaines dépendances soient chargées, empêchant dès lors leur mise à jour et l’installation du nouveau package. Le fait de fermer R permet de régler le souci dans la plupart des situations.Fermer R et refaire l’étape 1. Il arrive que certaines dépendances soient chargées, empêchant dès lors leur mise à jour et l’installation du nouveau package. Le fait de fermer R permet de régler le souci dans la plupart des situations.Vérifier l’espace sur le disque dur. Il arrive parfois que les packages ne s’installent pas parce que l’espace sur le disque dur est insuffisant. S’il est suffisant, passez à l’étape 4. S’il est insuffisant, il existe deux solutions, une assez technique qui consiste à choisir comme répertoire d’installation des packages un support externe. Cela implique de forcer R à aller chercher les packages dans ce dossier. L’autre solution est de travailler en ligne en créant un compte Rstudio cloud.Vérifier l’espace sur le disque dur. Il arrive parfois que les packages ne s’installent pas parce que l’espace sur le disque dur est insuffisant. S’il est suffisant, passez à l’étape 4. S’il est insuffisant, il existe deux solutions, une assez technique qui consiste à choisir comme répertoire d’installation des packages un support externe. Cela implique de forcer R à aller chercher les packages dans ce dossier. L’autre solution est de travailler en ligne en créant un compte Rstudio cloud.Le nom d’accès du dossier d’installation des packages contient des caractères spéciaux. Á nouveau, il existe deux solutions, forcer l’installation des packages dans un autre dossier (mais cela implique de préciser le chemin de ce dossier) ou créer un compte en ligne.Le nom d’accès du dossier d’installation des packages contient des caractères spéciaux. Á nouveau, il existe deux solutions, forcer l’installation des packages dans un autre dossier (mais cela implique de préciser le chemin de ce dossier) ou créer un compte en ligne.","code":"\ninstall.packages(\"devtools\", # nom du package\n                 dependencies = TRUE, # force l'installation des dépendances\n                 type= \"binary\", # on installe uniquement les packages sous un format compilé\n                 repos = \"https://mirror.ibcp.fr/pub/CRAN\") # le site miroir CRAN est celui de Lyon "},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"pour-conclure","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4.1.8 Pour conclure","text":"L’objectif de ce chapitre était de fournir les bases indispensables à l’utilisation de R, comprendre ce qu’est un object, pouvoir utiliser une fonction et adopter les bonnes pratiques pour éviter les erreurs.\nÁ présent qu’s’est dégagé des contraintes relatives à l’utilisation du logiciel, il est possible à présent de s’intéresser au raisonnement statistique et de pouvoir réaliser les analyses adaptées.","code":""},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"débuter-avec-easier","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4.2 Débuter avec easieR","text":"Comme évoqué dans la section consacrée au choix des logiciels, le package easieR (Stefaniak, 2018) peut être utilisé de différentes manières. En particulier, peut l’utiliser en boîte de dialogue uniquement, en ligne de commande uniquement, ou un mixte entre les deux. Dans cette section, nous fournissons les informations pour l’utiliser en boîte de dialogue uniquement dans sa forme la plus générale. Ainsi, pour utiliser easieR, il faut charger la package avec la fonction library et peut accéder à toutes les fonctions de easieR avec la fonction easieR().","code":"\nlibrary(easieR) #charge le package\n# ouvre la boîte de dialogue permettant d'accéder à toutes les options\neasieR() "},{"path":"les-données.html","id":"les-données","chapter":"5 Les données","heading":"5 Les données","text":"Les statistiques n’ont d’intérêt qu’à deux conditions principales.Premièrement, il faut que ce qui est mesuré ait une issue incertaine. Cela implique qu’il faut mesurer quelque chose ayant un caractère aléatoire (par exemple, ne connait pas à l’avance la couleur des cheveux de la prochaine personne qu’va croiser dans la rue) afin de tester une hypothèse dont l’issue est incertaine (par exemple, peut faire l’hypothèse qu’il y plus de personnes aux cheveux clairs dans le nord de l’Europe que dans le sud de l’Europe, mais san l’avoir mesuré, ne peut pas en avoir la certitude).\nCette mesure aléatoire est ce qu’appelle une variable.La seconde condition est de réaliser les analyses sur des données de qualité. Dès lors qu’il y des erreurs dans les données, les analyses statistiques perdent toute leur utilité. Il est donc nécessaire de préparer les données avec soin, tant lorsqu’les encodage dans le jeu de données, que quand les manipule. Cependant, même si applique le plus grand soin dans la création du jeu de données, des erreurs peuvent s’immiscer. Il est bon de mettre en place des procédure de vérification pour éviter ces erreurs. Par exemple, si utilise une échelle dont les valeurs vont de 45 à 155, alors, ne peut pas avoir une valeur à 255. peut donc vérifier que les données sont compatibles avec ce qui été mesuré. Cette procédure peut sembler longue et fastidieuse alors qu’voudrait savoir si nos résultats étayent ou non les hypothèses. Cependant, si considère le temps qui été investi dans la lecture de la littérature pour émettre une hypothèse, le temps pour construire le protocole expérimental, le temps pour recueillir les données, il semble flagrant que réaliser des analyses sur des données qui n’ont pas été vérifiées est faire bien mauvais usage de tout ce temps investi.","code":""},{"path":"les-données.html","id":"les-variables","chapter":"5 Les données","heading":"5.1 Les variables","text":"Quand veut réaliser des statistiques, il est nécessaire de manipuler et/ou de mesurer quelque chose de variable. Si nous mesurons à moult reprises la même valeur, n’pas besoin de statistiques car nous sommes face à des certitudes. Par exemple, il n’est pas nécessaire de faire des statistiques sur le fait de savoir si un·e étudiant·e peut réussir sans venir en cours, sans ouvrir ses cours, sans se renseigner sur ses cours, le tout en allant à l’ensemble de ses examens sans avoir de quoi écrire.Ainsi, puisque pour faire des statistiques, il faut manipuler et/ou mesurer quelque chose qui varie, ce qu’appelle une variable, il s’ensuit que, pour comprendre les statistiques, il est indispensable de pouvoir distinguer toute une série d’informations concernant ces variables.\nPlus précisément, il faut pouvoir distinguer les variables dépendantes des variables indépendantes et pouvoir distinguer la nature des variables.Pour rappel, une variable indépendante est tout ce qu’un·e expérimentateur·rice manipule est une variable indépendante, tandis que tout ce qu’un·e expérimentateur·rice enregistre est une variable dépendante. Maîtriser cette distinction est un prérequis nécessaire pas suffisant.peut rajouter à cette distinction toutes les variables dont veut s’assurer qu’elles ne peuvent être une explication plausible aux résultats qui ont été obtenus. Nous appellerons ces variables les variables de contrôle (ou controlées).Enfin, il existe une catégorie de variables qui sont loin d’être négligeables dans le cadre des statistiques mais dont la compréhension est plus subtile : il s’agit des variables aléatoires. Quand un chercheur·se recrute des participant·es pour prendre part à leur étude, iel n’pas volontairement choisi ces participant·es. Les personnes ont été recrutées parce qu’elles étaient là à ce moment, mais cela aurait pu être n’importe qui d’autre si l’expérience avait été réalisée deux semaines plus tard ou 50 km plus loin. De la même manière, si s’intéresse à la vitesse de lecture pour des textes écrits après 1900 et avant 1900, va choisir un ou plusieurs textes avant 1900 et un ou plusieurs textes après 1900, mais les textes choisis en tant que tels ont été choisis au hasard. Á l’inverse, si ce qui nous intéresse est la vitesse de lecture pour le premier chapitre des Misérables de Victor Hugo par rapport au premier chapitre du Seigneur des Anneaux de Tolkien, les textes ne sont plus choisis au hasard et ne sont donc pas des variables aléatoires. Si cette catégorie de variables vous semble difficile à saisir, vous pouvez, pour le moment, l’ignorer car, dans la plupart des cas, ces variables ne sont pas modélisées de manière explicite. Néanmoins, si elles sont abordées ici, c’est parce que, pour certaines analyses, comme les modèles linéaires mixtes, il est nécessaire de les formaliser.Il est également nécessaire de pouvoir distinguer une variable quantitative (appelée aussi numérique ou métrique), d’une variable ordinale, ou qualitative (appelée aussi nominale ou catégorielle).\nNe pas confondre variables et modalités \nIl apparaît que beaucoup d’étudiant·es confondent la variable avec les modalités avec ses modalités. Il est également indispensable de correctement distinguer les deux. La variable un nom qui est un résumé abstrait de l’ensemble des modalités que la variable recouvre. Par exemple, le genre est une variable qui comme modalités : femme, homme, non binaire, autre. Ainsi, le nom de la variable doit englober l’ensemble des modalités.","code":""},{"path":"les-données.html","id":"les-variables-qualitatives","chapter":"5 Les données","heading":"5.1.1 Les variables qualitatives","text":"Les variables qualitatives (ou catégorielles) sont caractérisée par des étiquettes, telle que le genre, la couleur des cheveux ou l’appartenance à un groupe… Elles ne sont pas dotées d’une structure particulière. Les différentes étiquettes permettent simplement de distinguer les individus les uns pdes autres sur une caractéristique précise. Elles sont caractérisées par la propriété d’exclusivité, c’est-à-dire qu’appartenir à la catégorie des hommes implique qu’n’appartient pas à la catégorie des femmes, non binaire ou autre, et par la propriété d’exhaustivité, c’est-à-dire que les modalités doivent pouvoir prendre en compte l’ensemble des cas de figures possibles.Dans la réalité, ces notions d’exhaustivité et d’exclusivité sont parfois compromises. Par exemple, vous voulez ranger des personnes en fonction de leur couleur de cheveux, certains ayant des cheveux bruns, d’autres des cheveux gris. Imaginons à présent une personne dont les cheveux sont bruns mais avec de nombreuses mèches grises (le fameux poivre et sel). Allez-vous le ranger dans la catégorie des cheveux bruns ou dans celle des cheveux gris ?","code":""},{"path":"les-données.html","id":"les-variables-ordinales","chapter":"5 Les données","heading":"5.1.2 Les variables ordinales","text":"Les variables ordinales sont des variables pour lesquelles il est possible d’attribuer un ordre entre les différentes modalités. Classiquement, les échelles ordinales sont les échelles de Likert, qui peuvent prendre 3, 5, 7 ou parfois 9 modalités différentes. La particularité de ces échelles est que la différence observée entre deux modalités n’est pas représentative d’une différence de même ampleur entre deux autres modalités. Par exemple, si vous pose la question « faites-vous du sport ? » et que vous devez répondre sur une échelle du type « moins d’une fois par mois – une fois par semaine – deux à trois fois par semaine – plus de trois fois par semaine ». Une personne qui répond « moins d’une fois par mois » ne fera pas trois fois moins de sport qu’une personne qui répond « deux à trois fois par semaine ». Il est à noter que les statistiques réalisées sur des variables ordinales se réalisent systématiquement sur le rang des différentes observations. Généralement, le nombre de modalités est limité, et dépasse rarement 9 modalités différentes . Il est aussi à noter que, quand les variables ont peu de modalités, en particulier quand il n’y en que 2 ou 3, tendance à utiliser les propriétés qualitatives de la variables plutôt qu’ordinale.","code":""},{"path":"les-données.html","id":"les-échelles-quantitatives","chapter":"5 Les données","heading":"5.1.3 Les échelles quantitatives","text":"Les échelles quantitatives (ou métrique ou continue ou numérique) sont des variables qui présentent, si pas une infinité, un très grand nombre de modalités. Il est théoriquement possible d’entre dans une distinction supplémentaire : les échelles de rapport et les échelles d’intervalle.Sans entrer dans les détails entre les échelles de rapport et les échelles d’intervalle, une des propriétés des échelles de rapport est que la différence et le rapport entre les différentes modalités ont du sens. Par exemple, si je mets deux poids de 5 kilos sur une extrémité d’une balance et un poids de 10 kg sur l’autre extrémité, la balance sera en équilibre7.","code":""},{"path":"les-données.html","id":"type-de-variable-et-leurs-propriétés","chapter":"5 Les données","heading":"5.1.4 Type de variable et leurs propriétés","text":"Il est à noter que les échelles métriques ont toutes les propriétés des échelles ordinales et des échelles qualitatives et que les échelles ordinales possèdent toutes les propriétés des échelles qualitatives. Par contre, les variables qualitatives n’ont pas les propriétés des variables ordinales et les variables ordinales n’ont pas les propriétés des variables qunatitatives. Ces caractéristiques sont particulièrement importantes à partir du moment où, lorsqu’réalise des statistiques, des opérations sur les variables peuvent avoir été réalisée, de sorte à ce la nature de ces variables été modifiée. Dès lors, il est possible de réaliser toutes les analyses faites sur une variable catégorielle quand une variable métrique ; l’inverse n’est en revanche pas possible. Par exemple, si vous avez les poids de vos participants, vous pouvez traiter cette variable comme une variable quantitative ; vous pouvez également la traiter comme une variable ordinale en attribuant une étiquette aux personnes (par exemple : obèse, en surpoids, corpulence normale, mince, maigre ou anorexique) ; enfin, cette variable peut être transformée en une variable nominale (sans ordonnancement) en classant les individus comme présentant un trouble de la masse corporelle (incluant dès lors les obèses et les anorexiques) ou comme ne présentant pas de troubles de la masse corporelle.Lorsque vous devez choisir un test, il vous faut pouvoir identifier correctement la nature des variables. Si vous éprouvez certaines difficultés pour identifier la nature des variables que vous manipulez, deux règles vous permettront de vous aidez :Si vous hésitez entre une variable ordinale et une variable qualitative, considérez que la variable est qualitative puisque il est acceptable d’appliquer les tests destinés à traiter des variables qualitative sur des variables ordinales ;Si vous hésitez entre une variable ordinale et une variable quantitative, considérez le nombre de modalités différentes qui sont à votre disposition. Si votre variable présente au moins 8 modalités différentes, vous pourrez la considérer comme une variable quantitative. En-dessous de 8 modalités différentes, préférez considérer la variable comme étant ordinale8.","code":""},{"path":"les-données.html","id":"les-objets-et-leur-nature-dans-r","chapter":"5 Les données","heading":"5.2 Les objets et leur nature dans R","text":"","code":""},{"path":"les-données.html","id":"la-nature-des-variables-et-des-objets-dans-r","chapter":"5 Les données","heading":"5.2.1 La nature des variables et des objets dans R","text":"Le chiffre 5 et le mot “bonjour” ne sont pas de même nature et vous ne pourriez pas faire les mêmes traitements statistiques sur des valeurs textuelles que sur des valeurs numériques.\nParmi les natures d’objet, celles qui sont particulièrement importantes sont les valeurs numériques, les valeurs entières, les chaînes de caractères, les valeurs logiques et les facteurs. Nous allons illustrer ces différentes natures d’informations et, grâce à la fonction class, vous pourrez identier la manière dont R interprète chacun des objets que vous allez créer.Commençons par la situation la plus simple : les logiques. Les logiques se résument à deux valeurs possible VRAI (TRUE en anglais) ou FAUX (FALSE en anglais). Remarquez que TRUE et FALSE sont en majuscule sans guillemets.Ensuite, nous avons les valeurs numériques. Les valeurs numériques peuvent être des valeurs avec ou sans virgules. Lorsqu’un ensemble de nombres sans virgule, R va leur attribuer comme nature ‘entier’ (integer en anglais). La raison de cette distinction est que R peut communiquer avec d’autres langages de programmation, notamment C et fortran qui requièrent moins d’espace et vont plus vite pour réaliser des calculs sur les entiers que sur les numériques. Pour vous, cette distinction n’pas d’importance, excepté de savoir que R traite les entiers et les numériques comme des nombres.Enfin, la dernière nature que nous aborderons sont les valeurs textuelles, les chaînes de caractères. Dans R, elles sont toujours entre guillemets, simples ou double.Ces chaînes de caractères peuvent être utilisés pour de l’analyse textuelle par exemple. Cependant, quand veut comparer des conditions expérimentales, il faut indiquer à R qu’il s’agit d’un facteur. Comme pour la distinction entre un entier et un numérique, le facteur est plus gourmand en mémoire, raison pour laquelle R privilégie les chaîne de lettres.Une règle d’que vous devriez toujours appliquer est de vérifier si R interprète correctement vos données : est-ce qu’il ne considère des valeurs numériques comme du texte ou un facteur comme une chaîne de caractères. Lorsque ce n’est pas le cas, peut changer la nature d’une information dans le format souhaité en le faisant précéder par “.”. Dans les exemples ci-dessus, j’ai utilisé la fonction .factor pour transformer une chaîne de caractères en facteur, et j’ai utilisé .integer pour transformer une valeur numérique en entier. Il y deux situations où il faut être particulièrement prudent : la transformation d’un facteur en valeur numérique et la transformation d’une valeur en logique.Commençons par la transformation d’une valeur considérée comme un facteur en valeur numérique. Si je reprends ma valeur numérique et que je la transforme en facteur, je n’ai pas de souci.présent, si j’essaie de transformer la valeur en numérique, la valeur n’est plus 2.3 mais 1 :La raison de ce phénomène est qu’un facteur peut être ordonné. Par exemple, les modalités “petit”, “moyen” et “grand” ont un ordre, et vous pourriez vouloir réaliser des analyses en fonction du rang en attribuant les valeurs 1, 2, et 3 à ces différentes modalités. En l’occurrence, ‘numerique.f’ est la première modalité et la transformation en numérique renvoie dans ce cas le chiffre 1.Pour pouvoir transformer une valeur considérée comme facteur en numérique, il est nécessaire de passer par une étape intermédiaire, qui consiste à transformer l’information en caractère avant de la transformer en numérique.La seconde situation qui mérite notre attention est la situation de la transformation en logique. Toute valeur numérique différente de 0 est considérée comme vraie, et le 0 est considéré comme faux. Pour le comprendre, il faut simplement comprendre que, par convention, faux vaut 0 en langage de programmation, et théoriquement vrai vaut 1. R étend cette logique à tous les nombres différents de 0.\nPour les chaînes de texte, l’information n’est pas interprétable en termes de logique et R renvoie NA. ne peut donc pas l’interpréter en tant que tel mais il est possible de transformer des valeurs textuelles en valeur logique en utilisant d’autres stratégies.\nLes valeurs manquantes \nLes valeurs manquantes sont désignées dans R par NA, pour available. Cette notion est essentielle car il faudra prendre régulièrement des décisions sur les valeurs manquantes. Il est donc important d’être capable de les identifier.","code":"\nlogique<-TRUE # l'alternative est FALSE\nlogique2<-F # TRUE et FALSE peuvent être résumés par T et F\nclass(logique) # indique que c'est une valeur logique## [1] \"logical\"\nclass(logique2) # indique que c'est une valeur logique## [1] \"logical\"\nnumerique<-2.3 # valeur numérique avec virgule. Remarquez que le séparateur de décimale est un point\nnumerique.sans.virgule<-5 # valeur numérique sans virgule.\nentier <-as.integer(5) # valeur qui sera considérée comme un entier par R \nclass(numerique) # considérée par R comme une valeur numérique. (numeric) ## [1] \"numeric\"\nclass(numerique.sans.virgule)# considérée par R comme une valeur numérique. (numeric) ## [1] \"numeric\"\nclass(entier)# considérée par R comme une chaîne un entier. (integer) ## [1] \"integer\"\nchaine_textuelle1<-\"hello world\" # remarquez la présence de guillemets doubles\nchaine_textuelle2<-'hello world' # remarquez la présence de guillemets simples\nclass(chaine_textuelle1)  # considérée par R comme une chaîne de lettres. (character) ## [1] \"character\"\nclass(chaine_textuelle2)  # également considérée par R comme une chaîne de lettres. ## [1] \"character\"\nfacteur<-as.factor(\"hello world\" )\nclass(facteur)## [1] \"factor\"\nnumerique## [1] 2.3\nnumerique.f<-as.factor(numerique ) # transformation de la valeur numérique en facteur\nclass(numerique.f) # R l'interprète comme facteur ## [1] \"factor\"\nas.numeric(numerique.f) # transformation d'un facteur en numérique, renvoie ici la valeur de 1. ## [1] 1\nas.numeric(numerique.f) # transformation d'un facteur en numérique, renvoie ici la valeur de 1. ## [1] 1\nnumerique.f<-as.character(numerique.f) # étape intermédiaire consistant à transformer le facteur en une chaîne de caractère\nas.numeric(numerique.f) # transformation de la chaîne de caractère en numérique, ce qui renvoie 2.3## [1] 2.3\nas.logical(1) # une valeur numérique transformée en valeur logique vaut toujours vrai si elle est différente de 0## [1] TRUE\nas.logical(-1)# une valeur numérique transformée en valeur logique vaut toujours vrai si elle est différente de 0## [1] TRUE\nas.logical(0) # une valeur numérique transformée en valeur logique vaut toujours faux si elle est égale 0## [1] FALSE\nas.logical(\"hello world\") # renvoie NA, pour not available car une valeur textuelle n'est pas interprétable en termes logiques. ## [1] NA\nas.logical(facteur)# renvoie NA, pour not available car une valeur textuelle n'est pas interprétable en termes logiques.## [1] NA"},{"path":"les-données.html","id":"les-différents-types-dobjets","chapter":"5 Les données","heading":"5.2.2 Les différents types d’objets","text":"Jusqu’à présent, nous avons traité d’objets pour lesquels il n’y avait qu’une information, un chiffre par exemple. Cependant, les traitements des données vont impliquer de manipuler plusieurs informations en même temps. La difficulté à laquelle vous allez être confronté.e est qu’il existe plusieurs types d’objets, qui ont chacun leurs propriétés et qu’il faut connaître pour pouvoir utiliser correctement R. Par exemple, pour calculer une moyenne, il vous faut un vecteur de valeurs numériques. Si vous ne savez pas ce qu’est un vecteur, vous ne pourrez pas utiliser la fonction. De même, si une fonction requiert une matrice, il faut savoir à quoi cela correspond.\nLes termes ‘matrices’ ou ‘vecteurs’ font généralement assez peur. Cependant,il s’agit de termes compliqués pour des notions qui peuvent être très largement simplifiées.\nBien que ma description soit volontairement imprécise et incomplète, elle suffira largement à la compréhension de la suite.Pour en revenir à notre propos, il existe dans R une multitude de classes d’objets. Si ces différentes classes d’objet ont leur raison d’être, cela dépasse largement les objectifs d’un chapitre d’introduction à R. Nous focaliserons donc sur les manières les plus fréquentes pour stocker, manipuler et traiter un ensemble de données : les vecteurs, les matrices, les dataframes, les tibbles et les listes.Imaginons qu’au lieu de n’avoir qu’une seule valeur numérique, nous en ayant 10. Par exemple, les résultats à un examen de statistiques. Les données pourraient être 12, 13, 9, 8, 15, 18, 4, 11, 13. Nous voudrions pouvoir les regrouper dans un seul objet. Nous pouvons le faire en créant un vecteur. Un vecteur est un ensemble d’informations de même nature, c’est-à-dire seulement des numériques, seulement des logique ou encore seulement des chaînes de caractères, qui n’ont qu’une dimension. Cela signifie que vous pourriez organiser les informations dans une seule colonne OU dans une seule ligne d’un tableau. Dans R, crée ce vecteur avec la fonction c où chaque élément du vecteur est séparé des autres par une virgule.identifie que stocker un ensemble d’éléments dans un seul objet plutôt que dans plusieurs présentent différents avantages. Premièrement, cela évite d’encombrer la mémoire de R avec une multitude d’objets qui risqueraient de vous perdre. Ensuite, et surtout, cela permet de réaliser des traitements sur ces objets. Par exemple, nous avons évoqué la fonction mean qui permet de calculer des moyennes. Cette fonction comme argument un vecteur de valeurs numériques. Nous pouvons donc calculer la moyenne sur notre objet ‘notes’.peut également créer des vecteurs sur des chaînes de caractères. Le point important ici est qu’il ne faut pas oublier de mettre les guillemets pour chaque élement du vecteur.Notez que si vous mélangez des informations de différentes natures, R va harmoniser la nature des informations en fonction de ce qui semble être le plus cohérent. Par exemple, si mélange des lettres et des chiffres, il va considérer l’ensemble des éléments du vecteur comme étant des chaînes de caractères.Les matrices sont des tableaux de données dans lesquels il n’y que des observations de même nature (que des chiffres, que des chaînes de lettres, que des logiques). Une matrice est une table à deux entrées, ayant donc des lignes et des colonnes. La matrice se différencie donc du vecteur par le fait que la matrice plusieurs lignes ET plusieurs colonnes. Les matrices mériteraient un chapitre complet les concernant mais les compétences requises dépasseraient très largement le niveau d’introduction.Table 5.1: Exemple de matrice ayant 3 lignes et 3 colonnes.V1V2V3147258369\nGénérer automatiquement des valeurs. \nIl existe une multitude de fonctions dans R qui permettent de générer automatiquement des valeurs sans avoir à les répéter Parmi ces fonctions, retrouve seq qui permet de générer une séquence (par exemple, répète 3 fois 4 et ensuite 3 fois 5), rep (qui permet de répéter une information), ou encore gl qui permet de générer un facteur. Lorsque les valeurs à générer est une séquence de chiffres, peut utiliser les deux points.Nous allons recréer cette matrice en utilisant la fonction matrix.Dans la plupart des situations, vos données contiendronsdes valeurs de nature différente. Il y aura des valeurs numériques, mais également des facteurs ou des identifiants de participants. Dans ce cas, la classe d’objet dans R n’est pas une matrice mais un dataframe. Pour le formuler autrement, un dataframe toutes les propriétés d’une matrice (s’il ne contient que des valeurs de même nature, le dataframe pourra être considéré comme une matrice), mais également la propriété de pouvoir gérer des informations de nature différente à la condition que le nombre de ligne soit toujours identique. Ainsi, ne peut pas avoir 5 valeurs dans la première colonne et 4 dans la seconde sans préciser qu’il y une valeur qui manque dans la seconde colonne.\nLe Tableau 5.2 un exemple qui pourrait représenter le genre et la taille de différents individus :Table 5.2: Exemple de dataframe avec une colonne contenant un facteur et une colonne contenant des valeurs numériques.sexetailleHomme176Femme173Homme180Femme165Dans R, votre dataframe prendra la forme suivante :Ce format est donc bien plus flexible que la matrice. Il s’agit du format le plus conventionnel avec lequel vous pouvez réaliser le plus d’opérations. Néanmoins, nous devons aborder une autre manière de gérer des jeux de données qui est les tibbles. Ce format été proposé par l’équipe de tidyverse (Wickham et al., 2019)\npour faciliter la manipulation et améliorer l’affichage dans la console. Néanmoins, tout ce que vous pouvez faire avec un tibble, vous pouvez le faire avec un dataframe, l’inverse n’est pas vrai. Par exemple, vous ne pouvez pas donner des noms aux lignes dans un tibble, ni changer la nature d’une variable. Certaines fonctions vont requérir un dataframe, et les tibbles seront incompatibles. Alors, finalement, pourquoi aborder ce type de format ? Pour deux raisons : la première est que les tibbles est le format dans lequel vos données seront importées si vous utilisez excel pour créer vos données et le package readxl et la seconde raison est que ce format est particulièrement utile pour manipuler les données, notamment avec le package dplyr. Concrètement, voici à quoi va ressembler un tibble par rapport au dataframe.Remarquez que le tibble fourni automatiquement les informations sur le nombre d’observations, le nombre de colonnes, ainsi que la nature des variables (character et double, pour numeric et integer). Vous pouvez obtenir ces informations pour un dataframe grâce à la fonction strEvidemment, de même qu’il est assez aisé de passer d’un dataframe à un tibble, il est assez simple de passer d’un tibble à un dataframe avec la fonction .data.frame.Le dernier format pour stocker des informations dans R que nous allons aborder est la liste. Il y deux raisons qui motivent à aborder ce type de format. La première est que les listes sont les types d’objets qui permettent de stocker des informations de la manière la plus flexible qui soit, plusieurs informations de même taille, plusieurs matrices, plusieurs dataframes ou simplement une valeur ou un vecteur. Cela peut être particulièrement utile quand doit réaliser une tâche itérative. La seconde raison est que les sorties de résultats de la plupart des fonctions sont stockées dans des listes. Ces informations ne seront que rarement directement utilisable. En revanche, des fonctions génériques, comme print ou summary, permettront d’avoir une présentation adaptée des résultats. Illustrons la flexbilité d’une liste en créant un liste où nous allons stocker la plupart des informations que nous avons créé dans la mémoire de R.Par la suite, peut accéder à n’importe quel élément d’un dataframe ou d’une liste en utilisant le symbole “$”.","code":"\nnotes<-c(12, 13, 9, 8, 15, 18, \n         4, 11, 13) # Création d'un vecteur avec 10 valeurs \nnotes## [1] 12 13  9  8 15 18  4 11 13\nmean(notes) # calcul de la moyenne des notes## [1] 11.44444\nmot_de_fin<-c(\"Cordialement\", \"respectueusement\", \"amicalement\", \"amitiés\", \"bonne journée\",\"bien à toi\",\"bien à vous\")\nmot_de_fin## [1] \"Cordialement\"     \"respectueusement\" \"amicalement\"      \"amitiés\"         \n## [5] \"bonne journée\"    \"bien à toi\"       \"bien à vous\"\nc(\"a\",1) # remarquez les guillemets dans la sortie de résultats autour du 1, indiquant qu'il est interprété comme une chaîne de caractères. ## [1] \"a\" \"1\"\n1:5 # crée une séquence de chiffres de 1 à 5## [1] 1 2 3 4 5\nrep(x=1, times = 9) # répète 9 fois le chiffre 1 ## [1] 1 1 1 1 1 1 1 1 1\nseq(from = 1, to = 9, by = 2) # crée une séquence allant de 1 à 9 en ayant un écart de 2 à chaque fois## [1] 1 3 5 7 9\ngl(n= 2, k = 5 , labels = c(\"condition 1\", \"condition 2\")) # crée un facteur ayant 2 modalités, chacune étant répétée 5 fois. ##  [1] condition 1 condition 1 condition 1 condition 1 condition 1 condition 2\n##  [7] condition 2 condition 2 condition 2 condition 2\n## Levels: condition 1 condition 2\nmatrice<-matrix(data = 1:9, # les données sont une séquence de chiffres allant de 1 à 9\n                nrow = 3, # elles doivent être réparties dans 3 lignes\n                ncol = 3, # et dans 3 colonnes - un des deux arguments peut être omis puisque si on a le nombre de lignes on a le nombre de colonnes\n                byrow = F) # logique qui permet d'indiquer si on veut que les valeurs soient organisées par ligne ou par colonne. Ici, par colonne\n\n## Notez que j'ai commenté chacun des arguments à l'intérieur de la fonction \n## et que la fonction a été utilisée sur plusieurs lignes pour plus de lisibilité. \n\nmatrice##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\nsexe<-c(\"Homme\", \"Femme\",\"Homme\", \"Femme\")\ntaille<-c(176,173,180,165)\n\ndt<-data.frame(sexe=sexe, taille=taille) \ndt##    sexe taille\n## 1 Homme    176\n## 2 Femme    173\n## 3 Homme    180\n## 4 Femme    165\n# on utilise ici la fonction tbl pour transformer notre dataframe \n# en tibble mais vous n'aurez normalement jamais à utiliser cette fonction. \ndt_tbl<-tibble(dt) \ndt_tbl## # A tibble: 4 × 2\n##   sexe  taille\n##   <chr>  <dbl>\n## 1 Homme    176\n## 2 Femme    173\n## 3 Homme    180\n## 4 Femme    165## 'data.frame':    4 obs. of  2 variables:\n##  $ sexe  : chr  \"Homme\" \"Femme\" \"Homme\" \"Femme\"\n##  $ taille: num  176 173 180 165\nclass(dt_tbl)## [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\ndt_tbl<-as.data.frame(dt_tbl)\nclass(dt_tbl)## [1] \"data.frame\"\nma.liste<-list(# on crée un liste des objets de taille différentes \n  dataframe = dt, # on ajoute un dataframe \n  vecteur = notes, # un vecteur \n  matrice = matrice, # une matrice \n  valeur = logique2) # et une valeur unique. On peut évidemment continuer\nma.liste## $dataframe\n##    sexe taille\n## 1 Homme    176\n## 2 Femme    173\n## 3 Homme    180\n## 4 Femme    165\n## \n## $vecteur\n## [1] 12 13  9  8 15 18  4 11 13\n## \n## $matrice\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## $valeur\n## [1] FALSE\ndt$sexe ## [1] \"Homme\" \"Femme\" \"Homme\" \"Femme\"\nma.liste$vecteur## [1] 12 13  9  8 15 18  4 11 13"},{"path":"les-données.html","id":"la-nature-des-variables-avec-easier","chapter":"5 Les données","heading":"5.3 La nature des variables avec easieR","text":"Quand vous importez les données, easieR, en plus de l’importation, fournit plusieurs informations importantes à considérer :Le nombre d’observations manquantes par variable. Si aucune information de cette nature n’apparaît dans la console, c’est qu’il n’y pas de valeurs manquantes.Le nombre d’observations et de variables.La nature des variables qui prendra essentiellement deux formes :\nfactor pour les variables qualitatives ;\nnumeric (ou integer) pour les variables quantitatives.\nfactor pour les variables qualitatives ;numeric (ou integer) pour les variables quantitatives.C’est à cette étape qu’il faut s’assurer que les données correspondent à ce qu’est censé avoir : est-ce le bon nombre d’observations ? Est-ce que les variables sont dans le bon format. Par exemple, est-ce qu’une variable numérique n’est pas considérée comme un facteur parce que le caractère de séparation de décimale n’pas été correctement choisi (la virgule à la place du point ou inversément) ?Ici, il est à noter que les variables ordinales, pour pouvoir être utilisées comme tel, doivent avoir un format numérique. Il est possible de faire la transformation directement dans R. Cependant, si l’objectif est d’éviter d’utiliser R en ligne de commande, il est préférable de l’anticiper en préparant correctement les données dans le tableur.","code":""},{"path":"organiser-les-données.html","id":"organiser-les-données","chapter":"6 Organiser les données","heading":"6 Organiser les données","text":"Les ingrédients centraux pour tester une hypothèse sont les données. Aucun manuel, aucun enseignant soulignera suffisamment à quel point il est essentiel d’assurer le plus grand soin pour avoir des données de qualité.\nSi les données que vous avez récoltées n’ont pas été récoltées avec soin, les traitements statistiques réalisés seront vains. Pire si vous identifiez mal la nature de vos données, les traitements que vous réaliserez seront inadaptés et donc erronés.\nEn revanche, ce qui peut être expliqué est la manière d’organiser correctement votre jeu de données. Quelques règles doivent être respectées pour vous assurer que vos données puissent être utilisables pour les traitements ultérieurs.Vos données représentent un tableau à double entrée, c’est-à-dire un dataframe, où les lignes représentent les observations et les colonnes représentent vos variables. Notez que le mot observation doit être pris au sens large. Dans certains cas, notamment quand veut construire deux formes parallèles d’une tâche, il peut être intéressant de vérifier si chaque stimulus d’une version présente des caractéristiques similaires au stimulus qui lui est apparié dans l’autre version de la tâche. Dans ce cas, chaque stimulus pourra être considéré comme une observation.\nIl y une exception à cette organisation : les données avec des mesures répétées. Si un individu passe par différentes conditions expérimentales, le nom de la colonne va représenter la modalité de la variable indépendante en mesure répétée et le contenu en-dessous de ce nom de colonne va représenter la variable dépendante mesurée dans chacune des conditions expérimentales. Si nous reprenons l’exemple du dataframe que nous avons présenté plus haut (avec la taille et le sexe), pourrait avoir une mesure de la taille à 4 ans, à 10 ans et à 15 ans. Le jeu de données prendrait alors la forme du Tableau 6.1.Table 6.1: Exemple de données pour lesquelles il y aurait plusieurs mesures chez les mêmes individussexeTaille 4 ansTaille 10 ansTaille 15 ansHomme102138176Femme108135173Homme105140180Femme99142165On se rend compte ici que la structure des données nous permet d’identifier si les mesures sont à mesures répétées ou à groupes indépendants. Puisque chaque ligne représente un individu et qu’est sur la même ligne, il s’agit ici de données en mesures répétées. En revanche, ne peut pas être totalement certain qu’il s’agit de groupes indépendants si les données sont organisées de la manière utilisée dans le Tableau 6.2 :Table 6.2: Exemple de données pour lesquelles il peut y avoir ambiguité sur la nature répétée ou à groupes indépendants de l'âge des personnessexeAgeTailleHomme4 ans102Femme4 ans108Homme4 ans105Femme4 ans99Homme10 ans138Femme10 ans135Homme10 ans140Femme10 ans142Homme15 ans176Femme15 ans173Homme15 ans180Femme15 ans165Pour pouvoir interprété avec certitude le jeu de données, il faut absolument avoir un identifiant pour les observations. Dans le Tableau 6.3, les données représentent des données à groupes indépendants alors que dans le Tableau 6.4, elles représentent des données à mesure répétées. le sait car les identifiants des observations ne sont pas les mêmes entre les différents âges dans le Tableau ?? alors qu’ils sont identiques aux différents âges dans le Tableau 6.4.Table 6.3: Données à groupes indépendantsIdentifiantsexeAgeTailledlhHomme4 ans102hnzFemme4 ans108gbpHomme4 ans105jiuFemme4 ans99umiHomme10 ans138hayFemme10 ans135fsrHomme10 ans140mivFemme10 ans142uyrHomme15 ans176xewFemme15 ans173bplHomme15 ans180kioFemme15 ans165Table 6.4: Données en mesures répétéesIdentifiantsexeAgeTailleuhrHomme4 ans102qteFemme4 ans108rlcHomme4 ans105xtnFemme4 ans99uhrHomme10 ans138qteFemme10 ans135rlcHomme10 ans140xtnFemme10 ans142uhrHomme15 ans176qteFemme15 ans173rlcHomme15 ans180xtnFemme15 ans165La différence de présentation entre les données présentées dans le Tableau 6.1 et celles présentées dans le Tableau 6 est ce qu’appelle un format large (Tableau 6.1) et un format long (Tableau 6.4).Pour terminer sur l’organisation des données, il est nécessaire de respecter quelques règles pour que l’importation se passe le mieux possible, respectez les règles suivantes :Votre feuille de données ne contient que les données (pas de graphique, pas commentaires, pas de mise en forme …)Votre feuille de données ne contient que les données (pas de graphique, pas commentaires, pas de mise en forme …)Les noms des variables doivent être courts (pas plus de 8 caractères ) rendant ainsi leur utilisation plus simple, ne doivent contenir ni accents, ni espaces. Pour enlever les espaces, utilisez la fonction rechercher/remplacer. Dans la fenêtre « rechercher », tapez un espace, dans la fenêtre « remplacer », tapez un underscore (le tiret en-dessous du 8 « _ ») ou un point.Les noms des variables doivent être courts (pas plus de 8 caractères ) rendant ainsi leur utilisation plus simple, ne doivent contenir ni accents, ni espaces. Pour enlever les espaces, utilisez la fonction rechercher/remplacer. Dans la fenêtre « rechercher », tapez un espace, dans la fenêtre « remplacer », tapez un underscore (le tiret en-dessous du 8 « _ ») ou un point.Il ne doit pas y avoir de ligne ou de colonne vide avant et dans la matrice de données. Les données commencent à la cellule A1 et s’étendent jusqu’à la dernière variable et la dernière observation sans discontinuer.Il ne doit pas y avoir de ligne ou de colonne vide avant et dans la matrice de données. Les données commencent à la cellule A1 et s’étendent jusqu’à la dernière variable et la dernière observation sans discontinuer.Bien que ce ne soient pas indispensable lorsqu’utilise excel mais est indispensable pour les fichiers csv, une bonne pratique consiste à indiquer dans les cellules vides la valeur NA (available) que R reconnait comme étant une valeur non disponible. nouveau, il est possible d’utilisez la fonction rechercher/remplacer dans le logiciel que vous avez utilisé pour stocker vos données.Bien que ce ne soient pas indispensable lorsqu’utilise excel mais est indispensable pour les fichiers csv, une bonne pratique consiste à indiquer dans les cellules vides la valeur NA (available) que R reconnait comme étant une valeur non disponible. nouveau, il est possible d’utilisez la fonction rechercher/remplacer dans le logiciel que vous avez utilisé pour stocker vos données.Si des modifications doivent être faites avant l’importation des données, ne travaillez jamais sur le fichier de données original. Enregistrez d’abord le fichier sous un autre nom.Si des modifications doivent être faites avant l’importation des données, ne travaillez jamais sur le fichier de données original. Enregistrez d’abord le fichier sous un autre nom.L’importation des nombres avec décimales ne pose pas de souci si vos données sont dans un fichier excel. En revanche, s’il s’agit d’un fichier csv, les nombres avec décimales sont considérés comme tels dans R à partir du moment où l’unité est séparée de la décimale par un point (« . »). Si vos décimales sont séparées des unités par des virgules, il faudra le préciser explicitement lors de l’importation des données.L’importation des nombres avec décimales ne pose pas de souci si vos données sont dans un fichier excel. En revanche, s’il s’agit d’un fichier csv, les nombres avec décimales sont considérés comme tels dans R à partir du moment où l’unité est séparée de la décimale par un point (« . »). Si vos décimales sont séparées des unités par des virgules, il faudra le préciser explicitement lors de l’importation des données.Ayez systématiquement un identifant pour chaque observation (càd. pour chaque participant).Ayez systématiquement un identifant pour chaque observation (càd. pour chaque participant).Si avec le temps, R s’est montré de plus en plus flexible pour la gestion des données, ne peut que vous encourager à être très rigide sur la structure de la base de données. La première raison est que votre base de données représentent les ingrédients de vos analyses. Si les ingrédients sont de mauvaises qualités, les analyses seront de mauvaise qualité. La seconde raison est que faire preuve de laxisme sur les données est le meilleur moyen d’être confronté à des messages d’erreurs dans R que vous ne comprendrez pas.","code":""},{"path":"préparer-les-données.html","id":"préparer-les-données","chapter":"7 Préparer les données","heading":"7 Préparer les données","text":"Jusqu’à présent, les données que nous avons utilisées ne correspondent pas à un vrai jeu de données sur lequel voudrait réaliser effectivement des traitements. R permet de travailler avec des données issues de différents formats. Les deux formats de données les plus fréquemment utilisés sont les fichiers excel et les fichiers csv. Nous focaliserons sur ces deux formats, bien que R soit en mesure d’importer des fichiers d’autres natures grâce au package foreign (R Core Team, 2022).Les données sont rarement utilisable directement et il est, dans la grande majorité des cas, nécessaire de réaliser des opérations pour que les données puissent être utilisées pour tester les hypothèses. Cela peut passer par la suppression des observations d’entraînement, la transformation d’une variable, ou l’addition des items d’un questionnaire pour obtenir un score global.L’objectif de ce chapitre est d’illustrer comment importe des données et comment peut les préparer.","code":""},{"path":"préparer-les-données.html","id":"dans-r","chapter":"7 Préparer les données","heading":"7.1 Dans R","text":"","code":""},{"path":"préparer-les-données.html","id":"limportation","chapter":"7 Préparer les données","heading":"7.1.1 L’importation","text":"présent que vous avez précisé le répertoire de travail où se trouve vos données, que vous avez identifié que R trouve effectivement votre fichier dans le dossier de votre répertoire de travail avec la fonction dir, et que vous vous êtes assuré que les règles de construction des jeux de données ont été scrupuleusement respectées, l’étape suivante consiste à importer les données.\nCette étape peut être réalisé avec la fonction  du package readxl si vos données sont stockées dans un fichier excel, avec la fonction read.csv2, inclue de base dans R, pour un fichier csv.Pour la suite du chapitre, nous allons utiliser des données d’amorçage sémantique (données non publiées mais pour des données publiées avec cette tâche et une description en détail de la tâche, voir Stefaniak et al., 2010). Plus précisément, la tâche que les participants devaient réaliser est une tâche de double décision lexicale. Dans cette tâche, les participants doivent déterminer le plus rapidement et le plus précisémant possible si deux chaînes de lettres présentées simultanément à l’écran sont des mots (par exemple, MOIS - CADEAU) ou non (par exemple, AJÛT - MARRER). leur insu, certaines paires de mots sont reliés sémantiquement (par exemple, MAIN - BAGUE) alors que d’autres ne le sont pas. Dans cette tâche, s’attend à ce que les items reliés sémantiquement soient traités plus rapidement que ceux qui ne le sont pas. La tâche est composée de 185 essais répartis entre 5 essais d’entraînement (les 5 premiers) et 180 essais à inclure dans les analyses.Dans le jeu de données mis à disposition, nous avons les données de 40 participants. Les variables d’intérêt sont :Subject : l’identifiant du participant ;Subject : l’identifiant du participant ;Attribute1 : le mot présenté en première position ;Attribute1 : le mot présenté en première position ;Attribute2 : le mot présenté en deuxième position ;Attribute2 : le mot présenté en deuxième position ;CATEGORIE : la condition expérimentale de la paire de mot (SEMANTIQUE = paire sémantiquement reliés ; contre = paire contrebalancée entre les versions de la tâche ; rempl = paire de mots de remplissage ; pseudo = paire contenant au moins un pseudomot)CATEGORIE : la condition expérimentale de la paire de mot (SEMANTIQUE = paire sémantiquement reliés ; contre = paire contrebalancée entre les versions de la tâche ; rempl = paire de mots de remplissage ; pseudo = paire contenant au moins un pseudomot)Running : la variable qui permet de distinguer les essais d’entraînement des esssais cibles, qui doivent être analysés ;Running : la variable qui permet de distinguer les essais d’entraînement des esssais cibles, qui doivent être analysés ;TextDisplay6.RT : le temps de réactionTextDisplay6.RT : le temps de réactionRC : la réponse correcteRC : la réponse correcteIl est important de noter ici que les données ont été préparées pour qu’il n’y ait aucun piège. Autrement dit, les règles présentées précédemment sont scrupuleusement appliquées ici. Pour vos données, il faut être attentif aux règles décrites précédemment.Il faut également noter que certaines variables d’intérêt dans le cadre de mon étude ne seront pas utilisées car cela n’apporteraient pas une réelle plus-value à l’exercice.","code":""},{"path":"préparer-les-données.html","id":"importer-un-fichier-excel","chapter":"7 Préparer les données","heading":"7.1.1.1 Importer un fichier excel","text":"Pour importer, des données en formation excel, va s’appuyer sur le package readxl (Wickham & Bryan, 2022) et va utiliser les fonctions read_excel et excel_sheetsLa premère étape consiste à vérifier si le fichier est effectivement dans mon répertoire de travail avec la fonction dirCette fonction m’indique que mon fichier “semantic_priming.xlsx” est présent dans le répertoire.partir d’ici,je pourrais ouvrir le fichier excel pour voir quel est le nom de la feuille de calcul que je dois importer mais une manière plus rapide de le faire est d’utiliser la fonction excel_sheets. Deux points sont importants ici. Il faut avoir chargé le package readxl (Wickham & Bryan, 2022), ce qui normalement devrait être fait (voir la ligne 34 du script) et il faut que le fichier excel soit fermé. Dans le cas contraire, vous aurez un message vous indiquant que le fichier excel ne peut pas être ouvert alors que le chemin d’accès est correct.En l’occurrence, il n’y qu’une seule feuille de calcul intitulée “tout_direct”. peut donc importer cette feuille avec la fonction read_excel. Nous devons préciser deux arguments : le chemin d’accès au fichier, c’est-à-dire le nom du fichier et la feuille de calcul que nous souhaitons importer. Ici, il est important de noter que, pour pouvoir utiliser ultérieurement les données, va les stocker dans un objet appelé “data.xls”.Ainsi, en tapant ‘data.xls’, vous identifiez qu’il s’agit d’un tibble composé de 7400 lignes et 59 variables.Pour les personnes souhaitant voir leurs données dans l’ensemble, la fonction View vous permettra d’ouvrir un nouvel onglet dans Rstudio dans lequel vous aurez l’intégralité des données.","code":"\ndir()\nexcel_sheets(\"./introR/semantic_priming.xlsx\")## [1] \"tout_direct\"\ndata.xls<-read_excel(path = \"./introR/semantic_priming.xlsx\", sheet = \"tout_direct\")\ndata.xls## # A tibble: 7,400 × 59\n##    ExperimentName Subject Session Display.RefreshRate Group RandomSeed\n##    <chr>            <dbl>   <dbl>               <dbl> <dbl>      <dbl>\n##  1 direct1              1       1                60.1     1 2136425444\n##  2 direct1              1       1                60.1     1 2136425444\n##  3 direct1              1       1                60.1     1 2136425444\n##  4 direct1              1       1                60.1     1 2136425444\n##  5 direct1              1       1                60.1     1 2136425444\n##  6 direct1              1       1                60.1     1 2136425444\n##  7 direct1              1       1                60.1     1 2136425444\n##  8 direct1              1       1                60.1     1 2136425444\n##  9 direct1              1       1                60.1     1 2136425444\n## 10 direct1              1       1                60.1     1 2136425444\n## # ℹ 7,390 more rows\n## # ℹ 53 more variables: SessionDate <chr>, SessionTime <dttm>, Block <dbl>,\n## #   Attribute1 <chr>, Attribute2 <chr>, CATEGORIE <chr>, essais <chr>,\n## #   essais.Cycle <chr>, essais.Sample <chr>, expe <chr>, expe.Cycle <chr>,\n## #   expe.Sample <chr>, List1 <chr>, List1.Cycle <chr>, List1.Sample <chr>,\n## #   Procedure <chr>, REPCORRECTE <chr>, Running <chr>, TextDisplay1.ACC <chr>,\n## #   TextDisplay1.CRESP <chr>, TextDisplay1.DurationError <chr>, …\nView(head(data.xls))"},{"path":"préparer-les-données.html","id":"importer-un-fichier-csv","chapter":"7 Préparer les données","heading":"7.1.1.2 Importer un fichier csv","text":"Pour importer un fichier csv, la fonction utilisée sera read.csv2. Si la procédure est sensiblement la même, quelques étapes et quelques arguments supplémentaires doivent être réalisées/précisés. Tout d’abord, ne peut pas faire l’économie d’ouvrir le fichier car il faut connaître le caractère qui sépare chacune des colonnes. Il faut ouvrir le fichier avec le bloc note car si l’ouvre avec un logiciel tel qu’excel, il ne sera pas possible d’identifier le symbole qui sert de séparateur de colonnes. En l’occurrence, le séparateur est le point-virgule (“;”). Ouvrir le fichier permet également de s’assurer que la première ligne du fichier correspond au nom des colonnes.Lors de l’importation, va donc indiquer si la première ligne correspond au nom des variables avec l’argument header = T, va indiquer le caractère qui sépare chaque colonne avec l’argument sep. Les valeurs possibles sont l’espace, la tabulation, le point-virgule et la virgule. L’espace est à déconseiller car les valeurs textuelles pourraient être traitées de manière inadaptée. Dans ce cas, il faut réengistrer le fichier avec un autre séparateur. Ceci est possible en utilisant par exemple excel, enregistrer sous et en choisissant le format.Il est également nécessaire de préciser le caractère qui va indiquer la séparation des décimales (le point ou la virgule), ainsi que la manière dont les valeurs manquantes ont été gérées. Fondamentalement peut utiliser n’importe quelle valeur pour signaler les valeurs manquantes. Etant habitué à R, le code que j’utilise est NA. Notez qu’il n’est pas absolument nécessaire de le faire dans excel mais qu’il est indispensable de le faire avec un fichier csv. Enfin, l’argument check.names va permettre de vérifier si les noms utilisés comme nom de variables sont valides et les corriger le cas échéant.nouveau, il est possible de voir les données importées en utilisant la fonction View.","code":"\ndata.csv<-read.csv2(file = \"semantic_priming.csv\", \n                    header = T,\n                    sep = \";\", # valeurs possibles \"\", \"\\tab\" ou \",\"\n                    dec =\".\", # valeurs possibles \".\" ou \",\"\n                    na.string = \"NA\",\n                    check.names=T)"},{"path":"préparer-les-données.html","id":"préparer-les-données-1","chapter":"7 Préparer les données","heading":"7.1.2 Préparer les données","text":"Il n’est pas possible d’explorer toutes les potentialités de prétraitements des données qu’offre R mais l’exemple de travail utilisé permet d’illustrer quelques-unes de ces potentialités.Concrètement, pour que les données soient utilisables (nous allons faire l’hypothèse que l’analyse que nous voulons réaliser est une analyse de variance à mesure répétée d’un côté sur les temps de réaction et de l’autre sur les réponses correctes), nous allons devoir suivre les étapes suivantes :sélectionner les variables d’intérêt pour faciliter la manipulation des données ;sélectionner les variables d’intérêt pour faciliter la manipulation des données ;supprimer les observations relatifs aux essais d’entraînement ;supprimer les observations relatifs aux essais d’entraînement ;calculer le pourcentage de réponses correctes par participant et par conditions (la somme totale ne conviendrait pas ici étant donné qu’il n’y pas le même nombre d’items dans toutes les conditions, càd. 30 items dans chacune des trois conditions mots et 90 dans la condition pseudomot) ;calculer le pourcentage de réponses correctes par participant et par conditions (la somme totale ne conviendrait pas ici étant donné qu’il n’y pas le même nombre d’items dans toutes les conditions, càd. 30 items dans chacune des trois conditions mots et 90 dans la condition pseudomot) ;filtrer les temps de réaction pour ne conserver que ceux pour lesquels la réponse est correcte ;filtrer les temps de réaction pour ne conserver que ceux pour lesquels la réponse est correcte ;vérifier qu’il n’y pas de temps de réaction correspondant à des réponses anticipées ou à des réponses inattentives. Habituellement, gère ce cas de figure en utilisant la médiane mais en l’occurrence, par souci pédagogique, va considérer que les réponses anticipées sont les temps inférieurs à 200 ms et les réponses inattentives ceux supérieurs à 2000 ms.vérifier qu’il n’y pas de temps de réaction correspondant à des réponses anticipées ou à des réponses inattentives. Habituellement, gère ce cas de figure en utilisant la médiane mais en l’occurrence, par souci pédagogique, va considérer que les réponses anticipées sont les temps inférieurs à 200 ms et les réponses inattentives ceux supérieurs à 2000 ms.calculer la moyenne des temps de réaction restant par participant et par condition.calculer la moyenne des temps de réaction restant par participant et par condition.Combiner le jeu de données sur les réponses correctes et sur les temps de réaction.Combiner le jeu de données sur les réponses correctes et sur les temps de réaction.","code":""},{"path":"préparer-les-données.html","id":"sélectionner-des-variables","chapter":"7 Préparer les données","heading":"7.1.2.1 Sélectionner des variables","text":"Pour sélectionner des variables, va utiliser la fonction select du package dplyr. Il suffit de donner le nom du jeu de données et les variables qu’il faut sélectionner en les séparant par une virgule.Il existe néanmoins une difficulté ici. La fonction select existe dans plusieurs packages et peut créer un conflit car elle ne s’utilise pas de la même manière en fonction du package. Pour éviter le conflit, peut préciser le package dans lequel il faut aller chercher la fonction en indiquant le nom du package suivi de deux-points, c’est-à-dire dplyr::.Nous conserverons uniquement les variables qui ont été décrites plus haut, à savoir : Subject, Attribute1, Attribute2, CATEGORIE, Running, TextDisplay6.RT, RC.","code":"\ndata.et1<-data.xls%>%dplyr::select( Subject, Attribute1,Attribute2,CATEGORIE,\n                Running, TextDisplay6.RT,RC)"},{"path":"préparer-les-données.html","id":"sélectionner-des-observations-sur-la-base-dune-valeur-textuelle","chapter":"7 Préparer les données","heading":"7.1.2.2 Sélectionner des observations sur la base d’une valeur textuelle","text":"présent que le jeu de données est plus lisible, nous pouvons supprimer les observations correspondant aux essais. trouve cette information dans la variable ‘Running’. Grâce à la fonction unique, peut identifier les différentes valeurs individuelles dans la variable ‘Running’.Sans surprise, les essais s’appellent les essais. Pour sélectionner des valeurs textuelles, deux possibilités : 1) selectionner les chaînes que nous souhaitons conserver ; 2) supprimer celles que nous ne voulons pas conserver. Etant donné qu’il y 3 possibilités (“essais”, “expe”, “List1”) et qu’veut garder tout ce qui n’est pas “essais”, va donc décider d’utiliser la seconde stratégie. La logique consiste à sélectionner tout ce qui ne correspond pas à un critère. En l’occurrence, notre critère consiste à ne pas être un essai.Pour atteindre cet objectif, en plus d’utiliser la fonction filter, il faut connaître la liste des opérateurs logiques dans R. Le Tableau 7.1 une liste non exhaustive.Table 7.1: Liste des opérateurs logiques les plus fréquentsOpérateurSignification==égal à !=Différent de>plus grand>=plus grand ou égal (le égal se met toujours à droite)<plus petit<=plus petit ou égal (le égal se met toujours à droite)&et|ouDans notre exemple, nous allons devoir utiliser l’opérateur “!=” pour indiquer que nous voulons conserver tous les valeurs qui sont différentes de “essais”.Pour déterminer si l’opération s’est déroulée avec succès, peut vérifier si le nombre d’observations été réduit de manière cohérente avec ce qu’attend. Ainsi, sait que nous avons 40 participants, et nous savons qu’il y 5 essais par participants. doit donc avoir une réduction de 200 observations entre les données à l’étape 1 (data.et1) et les données à l’étape 2 (data.et2). obtient cette information avec la fonction dim, qui nous permet de constater que le nombre d’observations est passé de 7400 à 7200.","code":"\nunique(data.et1$Running)## [1] \"essais\" \"expe\"   \"List1\"\ndata.et2<-filter(data.et1, Running!=\"essais\")\ndim(data.et1)## [1] 7400    7\ndim(data.et2)## [1] 7200    7"},{"path":"préparer-les-données.html","id":"réaliser-des-opérations-mathématiques-calcul-de-pourcentages-de-réponses-correctes","chapter":"7 Préparer les données","heading":"7.1.2.3 Réaliser des opérations mathématiques : calcul de pourcentages de réponses correctes","text":"L’étape suivante consiste à obtenir le pourcentage de réponse correcte par condition. Grâce à la fonction unique, est en mesure d’identifier qu’il y 4 conditions (“contre”, “pseudo”, “rempl”, “SEMANTIQUE”)Connaissant les données, j’avais indiqué qu’il y avait 30 items par condition sauf pour les pseudomots où il y 90 items. peut néanmoins s’assurer que cette répartition est correcte et que, individuellement, il n’y pas d’observation manquante. peut obtenir cette information avec la fonction table.Nous pouvons à présent calculer le pourcentage de réponses correctes par participant et par condition. Cette étape est un peu plus complexe et illustre comment il est possible d’enchaîner les opérations avec le package dplyr. Ainsi, nous allons réaliser l’opération en 3 fois. Dans un premier temps, va indiquer sur quel jeu de données les opérations doivent être réalisées. Dans un second temps, indique les variables sur lesquelles nous souhaitons faire le regroupement. Enfin, calcule le pourcentage de réponses correctes dans une nouvelle variable (‘perc’). Il existe sans doute différentes approches pour cette dernière étape. Nous allons privilégier une approche assez simple. Etant donné que les réponses correctes sont des 1 et des 0, en calculant la moyenne par participant et par condition, nous aurons la proportion de réponses correctes qu’il suffira de multiplier par 100. Les symboles %>% s’appellent des pipes et permettent de lier les opérations entre elles. En d’autres termes, peut réaliser l’ensemble des opérations en une seule fois.","code":"\nunique(data.et2$CATEGORIE)## [1] \"contre\"     \"pseudo\"     \"rempl\"      \"SEMANTIQUE\"\ntable(data.et2$Subject, data.et2$CATEGORIE)##     \n##      contre pseudo rempl SEMANTIQUE\n##   1      30     90    30         30\n##   2      30     90    30         30\n##   3      30     90    30         30\n##   4      30     90    30         30\n##   5      30     90    30         30\n##   6      30     90    30         30\n##   7      30     90    30         30\n##   8      30     90    30         30\n##   9      30     90    30         30\n##   10     30     90    30         30\n##   11     30     90    30         30\n##   12     30     90    30         30\n##   13     30     90    30         30\n##   14     30     90    30         30\n##   15     30     90    30         30\n##   16     30     90    30         30\n##   17     30     90    30         30\n##   18     30     90    30         30\n##   19     30     90    30         30\n##   20     30     90    30         30\n##   21     30     90    30         30\n##   22     30     90    30         30\n##   23     30     90    30         30\n##   24     30     90    30         30\n##   25     30     90    30         30\n##   26     30     90    30         30\n##   27     30     90    30         30\n##   28     30     90    30         30\n##   29     30     90    30         30\n##   30     30     90    30         30\n##   31     30     90    30         30\n##   32     30     90    30         30\n##   33     30     90    30         30\n##   34     30     90    30         30\n##   35     30     90    30         30\n##   36     30     90    30         30\n##   37     30     90    30         30\n##   38     30     90    30         30\n##   39     30     90    30         30\n##   40     30     90    30         30\nRC_percent <- data.et2 %>% # le résultat sera stocké dans RC percent et on effectue des opérations sur data.et2\n  group_by(Subject, CATEGORIE )%>% # un regroupe les données par les modalités des variables Subject et CATEGORIE\n  summarise(perc_RC = mean(RC)*100 ) # On résume les données en calculant le pourcentage de réponses correctes## `summarise()` has grouped output by 'Subject'. You can override using the\n## `.groups` argument."},{"path":"préparer-les-données.html","id":"sélectionner-des-observations-sur-la-base-dune-valeur-numérique","chapter":"7 Préparer les données","heading":"7.1.2.4 Sélectionner des observations sur la base d’une valeur numérique","text":"Nous voulons à présent sélectionner les réponses correctes. va utiliser une procédure similaire à celle utilisée pour supprimer les essais, avec comme différence le fait que nous allons conserver les observations pour lesquelles la réponses vaut 1 en utilisant l’opérateur “==”. La seconde différence est que les valeurs numériques ne sont pas entre guillemets contrairement aux chaînes de caractères.","code":"\ntable(data.et2$RC)  # il y a 5679 observations correctes ## \n##    0    1 \n## 1521 5679\ndata.et4<-filter(data.et2, RC==1) # on sélectionne les observations pour lesquelles la réponse est correcte \ndim(data.et4) # on obtient effectivement 5679 observations## [1] 5679    7"},{"path":"préparer-les-données.html","id":"combiner-plusieurs-critères-de-sélection","chapter":"7 Préparer les données","heading":"7.1.2.5 Combiner plusieurs critères de sélection","text":"L’avant-dernière étape consiste à vérifier qu’il n’y pas de réponses anticipées ou des réponses inattentives. Nous allons fixer comme critères 200 ms pour les réponses anticipées. Cela signifie que nous considérons qu’il n’est pas possible de répondre en moins de 200 millisecondes en ayant traité de manière volontaire les stimuli.Dans cet exemple, ce raisonnement est un peu fallacieux car les items sont présentés pendant 400 ms sans qu’ne puisse répondre et un écran blanc apparaît ensuite. Ainsi, avec un critère à 200 ms, cela laisse 600 ms pour répondre ce qui peut avoir été suffisant. L’objectif ici étant d’être pédagogique, nous conserverons tout de même ce critère.Les réponses inattentives sont les réponses pour lesquelles les latences sont tellement longues qu’doit considérer que le participant répondu suite à une inattention et ne reflète pas réellement la manière dont il se comporte dans ce type de tâche. Nous fixerons de manière arbitraire ce critère à 2000 ms.Nous allons donc filtrer les données en appliquant deux critères de sélection.Remarquez que nous avons ici aussi utilisé les pipes pour réaliser plusieurs opérations en une fois.","code":"\ndata.et5<-data.et4  %>%\n          filter(TextDisplay6.RT>200) %>% # on conserve les temps supérieurs à 200 ms\n          filter(TextDisplay6.RT<2000) # on conserve les temps inférieurs à 2000 ms"},{"path":"préparer-les-données.html","id":"réaliser-des-opérations-mathématiques-calcul-de-la-moyenne-des-temps-de-réaction","chapter":"7 Préparer les données","heading":"7.1.2.6 Réaliser des opérations mathématiques : calcul de la moyenne des temps de réaction","text":"présent, nous pouvons faire la moyenne des temps de réaction sur les items restant. Nous avons déjà vu comment réaliser cette opération à l’étape 3.","code":"\ndata.et6 <- data.et5 %>% # le résultat sera stocké dans RC percent et on effectue des opérations sur data.et2\n  group_by(Subject, CATEGORIE )%>% # un regroupe les données par les modalités des variables Subject et CATEGORIE\n  summarise(M_TR = mean(TextDisplay6.RT) ) # On fait la moyenne des temps de réaction par participant et par condition## `summarise()` has grouped output by 'Subject'. You can override using the\n## `.groups` argument."},{"path":"préparer-les-données.html","id":"réaliser-une-jointure-entre-des-jeux-de-données","chapter":"7 Préparer les données","heading":"7.1.2.7 Réaliser une jointure entre des jeux de données","text":"Il nous reste plus qu’à regrouper dans une base de données finales les données avec les pourcentages de réponses correctes et la moyenne des temps de réaction.obtient ce résultat en utilisant la fonction left_join, où les deux premiers arguments sont les jeux de données à combiner et l’argument correspond au nom des variables qui permettent de faire l’appariement. Ici, je les ai donné de manière explicite en indiquant qu’il faut prendre en compte la variable « Subject » et la variable « CATEGORIE », qui s’appellent de la même manière dans les différentes base de données.","code":"\ndata_final<-left_join(data.et6, RC_percent, by=c(\"Subject\"=\"Subject\",\"CATEGORIE\"=\"CATEGORIE\" ))"},{"path":"préparer-les-données.html","id":"avec-easier","chapter":"7 Préparer les données","heading":"7.2 Avec easieR","text":"","code":""},{"path":"préparer-les-données.html","id":"importer-les-données","chapter":"7 Préparer les données","heading":"7.2.1 Importer les données","text":"Pour commencer à utiliser easieR, nous allons débuter par l’importation de données. L’exemple de travail que nous allons utiliser est le suivant : dans une publicité, Georges Clooney va chercher un café au même moment qu’une jeune femme. Il s’aperçoit que c’est la dernière capsule et la laisse donc à la jeune femme, qui était venue chercher un café pour Jean Dujardin. La question que se pose les chercheur est de savoir comment interprète cette publicité. Est-ce que Georges Clooney se laisse manipuler par les femmes, est-ce qu’il s’agit d’un gentleman ou est-ce que Jean Dujardin eu raison de manipuler Clooney pour arriver à ses fins.Les données sont disponibles dans le feuille de calcul appelée ‘Clooney’.","code":""},{"path":"préparer-les-données.html","id":"avec-les-boîtes-de-dialogue","chapter":"7 Préparer les données","heading":"7.2.1.1 Avec les boîtes de dialogue","text":"Pour importer des données avec les boîtes de dialogue, il suffit de lancer easieR avec la fonction easieRLa boîte de dialogue de la Figure 7.1 apparaît. Il faut choisir “Donnees - (importation, exportation, sauvegarde) et cliquer sur OK.\nFigure 7.1: Fenêtre d’accueil de easieR. Choisir : Donnees - (importation, exportation, sauvegarde)\nDe manière assez transparente, pour importer les données, il faut choisir dans la boîte de dialogue de la Figure 7.2 “importer des donnees”\nFigure 7.2: Choisir : importer des donnees\nLe package easieR permet d’importer des données de 4 types de format : CSV, txt, excel et les fichiers SPSS. En réalité, R est en mesure d’importer d’autres formats de données, mais les 4 formats proposés dans easieR représentent les outils utilisés le plus fréquemment utilisés par les psychologues. En l’occurrence, nous travaillerons avec des fichiers excel (voir Figure 7.3). Il est possible d’accéder directement à cette boîte de dialogue grâce à la fonction import()\nFigure 7.3: Format du fichier de données à importer.\nUne fois le format du fichier décidé, il ne reste plus qu’à choisir le fichier de données (Figure @ref(fig=import4)).\nFigure 7.4: Choix du fichier de données, ‘illustration.easieR.xlsx’ en l’occurrence.\nLa plupart du temps, les chercheurs utilisent la première ligne du fichier de données pour indiquer le nom des variables. Cependant, ce n’est pas toujours le cas. Il est donc nécessaire d’indiquer si la première ligne correspond effectivement au nom des variables (Figure 7.5).\nFigure 7.5: Est-ce que la première ligne correspond aux variables ?\nDe la même manière, quand une valeur est manquante, il y plusieurs manières d’indiquer les valeurs manquantes. Si la cellule est vide, peut laisser “NA” en revanche, si une valeur par défaut (comme -9999) est utilisée alors, il faut l’indiquer dans la Figure 7.6.\nFigure 7.6: Valeurs servant à représenter les valeurs manquantes. Si la cellule est vide, laissez NA.\nL’étape suivante consistera à choisir la feuille de calcul qui nous intéresse. En l’occurrence, il s’agit de la feuille ‘Clooney’ (Figure 7.7).\nFigure 7.7: Format du fichier de données à importer.\nEt il faut terminer en donnant un nom aux données qui sera utilisé par la suite dans R. Par défaut, c’est le nom de la feuille de calcul (Figure 7.8). Ce jeu de données doit avoir un nom qui ne contient pas de caractères spéciaux (excepté _ ou .), pas de caractères avec des accents ou des cédilles, et préférez des noms cours (max 20 caractères).\nFigure 7.8: Nom attribué aux données pour les utiliser ensuite dans R.\nLe jeu de données apparaît à présent dans la console (Figure 7.9).\nFigure 7.9: Tableau du jeu de données.\nAinsi que des informations sur les données. En l’occurrence, il s’agit d’un jeu de données avec 67 observations, 3 variables qui sont toutes les trois des variables catégorielles (Figure 7.10).\nFigure 7.10: Structure du jeu de données.\nRemarques importantesTrois remarques doivent être faites :Les explications des boîtes de dialogue sont fournies dans la console quand utilise easieR, ce qui permet de savoir à quoi elles correspondent même si ne le sait pas (Figure 7.11) ;\nFigure 7.11: Information des boîtes de dialogue qui sont affichées dans la console.\neasieR va changer automatiquement le répertoire de travail pour travailler dans le même répertoire que celui où se trouve le jeu de données ;easieR va changer automatiquement le répertoire de travail pour travailler dans le même répertoire que celui où se trouve le jeu de données ;dans la console apparaît un ligne appelée “call”. Cette ligne correspond à la ligne de commande (il faut enlever les guillemets au début et à la fin) pour importer le fichier de données (Figure 7.12).dans la console apparaît un ligne appelée “call”. Cette ligne correspond à la ligne de commande (il faut enlever les guillemets au début et à la fin) pour importer le fichier de données (Figure 7.12).\nFigure 7.12: Ligne de commande qui correspond à l’importation du fichier. Attention, pour l’utiliser, il faut supprimer les guillemets des extrémités.\n","code":"\neasieR()"},{"path":"préparer-les-données.html","id":"en-ligne-de-commande","chapter":"7 Préparer les données","heading":"7.2.1.2 En ligne de commande","text":"peut également utiliser easieR en ligne de commande. Cette fonction 8 arguments :file : le nom du fichierdir : le répertoire où se trouve le fichiertype : le type de fichierdec : lorsqu’utilise un fichier csv ou txt, le séparateur de décimale peut être soit un point, soit une virgule. Il faut le préciser. Dans excel, la fonction le détecte automatiquement.sep : dans les fichiers txt et csv, les colonnes peuvent être séparés par différents types de caractères (tabulation, virgule, espace ou point-virgule). Il faut alors indiquer quel est le séparateur de colonnes. Dans excel, il l’identifie par défaut.na.strings : caractères utilisés pour indiquer qu’une valeur est manquante. NA est la valeur par défaut.sheet : pour les fichiers excel, il faut indiquer la feuille de calcul dans laquelle se trouve les donnéesname : correspond au nom qu’veut attribuer au jeu de données dans R. Ce jeu de données doit avoir un nom qui ne contient pas de caractères spéciaux (excepté _ ou .), pas de caractères avec des accents ou des cédilles, et préférez des noms cours (max 20 caractères).De manière concrète pour importer le même fichier que celui importé en boîte de dialogue, utiliser la fonction suivante :Le jeu de données apparaît à présent dans la console (Figure 7.9).Ainsi que des informations sur les données. En l’occurrence, il s’agit d’un jeu de données avec 67 observations, 3 variables qui sont toutes les trois des variables catégorielles 7.10.Vous avez donc à présent importer le jeu de données présenté dans le Tableau 1.","code":"\nimport(file='illustration.easieR.xlsx',\n       dir='C:/Users/mon_repertoire/Cours de statistiques/Livre/introR',\n       type='Fichier Excel',\n       dec='.',\n       sep=';',\n       na.strings='NA',\n       sheet='Clooney',\n       name='Clooney')"},{"path":"préparer-les-données.html","id":"préparer-les-données-2","chapter":"7 Préparer les données","heading":"7.2.2 Préparer les données","text":"","code":""},{"path":"les-statistiques-descriptives.html","id":"les-statistiques-descriptives","chapter":"8 Les statistiques descriptives","heading":"8 Les statistiques descriptives","text":"“Si les statistiques vous ennuient, c’est sans doute parce que vous n’avez pas les bons chiffres.”\nRésumé \nL’objectif de cette page est de découvrir comment obtenir les principales statistiques descriptives pour des variables quantitatives et qualitatives en utilisant R. Nous apprendrons à calculer des mesures essentielles telles que la moyenne, la médiane, l’écart-type, les quartiles, ainsi que des résumés de fréquences pour les variables qualitatives.Pour les utilisateurs de R en ligne de commande\nDans un premier temps, nous nous concentrerons sur les variables quantitatives en utilisant principalement deux fonctions du package psych (Revelle, 2025) à savoir describe et describeby. Ce package offre une méthode simple et rapide pour générer des résumés détaillés de vos données numériques. À travers des exemples concrets et des exercices d’application, vous pourrez vérifier votre compréhension et appliquer les concepts abordés.Ensuite, nous traiterons des variables qualitatives. Bien que le package psych (Revelle, 2025) ne soit pas utilisé pour ces variables, nous verrons comment obtenir des résumés de fréquences et des proportions en utilisant d’autres fonctions intégrées dans R, comme table et prop.table. Des exemples pratiques vous permettront de comprendre comment analyser et interpréter la répartition des catégories pour les variables qualitatives.Enfin, pour compléter notre analyse, nous aborderons la visualisation de la distribution des données quantitatives à l’aide de graphiques en violon. Ces graphiques, créés avec le package ggplot2, combinent densité et quartiles, offrant une vue d’ensemble complète de la distribution des variables quantitatives. Nous apprendrons à générer et interpréter ces graphiques pour mieux comprendre la forme et les caractéristiques des distributions de nos données.Pour les utilisateurs de easieR\nPrérequis \nD’un point de vue théoriqueêtre en mesure d’identifier la nature d’une variable.Pour les utilisateurs de RAvoir installé R et Rstudio et les principaux packages psych (Revelle, 2025) et ggplot2 (Wickham, 2016a).Savoir installer et charger des packages dans R.Savoir importer un fichier excel dans R.Connaître et utiliser les règles de bonnes pratiques dans R.Pour les utilisateurs d’easieRavoir installé Ravoir installé easieRavoir installé pandocêtre en mesure d’importer un fichier de données","code":""},{"path":"les-statistiques-descriptives.html","id":"introduction-1","chapter":"8 Les statistiques descriptives","heading":"8.1 Introduction","text":"Commencer ses analyses par les statistiques descriptives reste une étape indispensable quand il s’agit d’analyser les données. Elles permettent en effet d’avoir une représentation de prime abord des résultats avant d’éprouver des hypothèses d’un point de vue inférentiel.Cette étape permet aussi de s’assurer de la compatibilité des valeurs de notre jeu de données avec les données possibles (.e., ne peut pas avoir un âge négatif) ainsi que de la distribution des données (imagine aisément que l’âge de décès ne suivra pas une distribution normale).Par exemple, si un chercheur émet l’hypothèse que lever les mains en l’air rend heureux. Il compare un groupe de personnes qui doivent lever leur main 20 minutes par jour et un groupe qui ne le fait pas. Quel sens cela aurait d’aller vérifier la significativité de la différence entre ces deux groupes si cet expérimentateur s’apercevait de prime abord que la moyenne relative à la sensation de bonheur du groupe qui ne lève pas les mains est supérieure à celle de ceux qui lèvent la main ? Si la différence s’avérait significative, la seule conclusion possible serait qu’il ne faut absolument pas lever les mains pour être heureux.\nUn exemple farfelu… mais peut-être pas tant que cela. \nAu-delà du côté farfelu de cette hypothèse, il faut se rappeler que les hypothèses doivent systématiquement être construites sur des éléments théoriques solides. Il n’est dès lors pas acceptable de penser des statistiques sans les mettre en relation avec les résultats obtenus dans d’autres recherches et sans les mettre en relation avec le plan expérimental qui été utilisé\npour acquérir les données. Nous reviendrons à ce titre sur l’importance de réfléchir aux statistiques lors de la construction du plan expérimental.L’exemple proposé ci-dessus, bien que caricatural, est en réalité très souvent observé dans la littérature scientifique, car beaucoup de chercheur·euses oublient que la science ne consiste pas à confirmer leur hypothèse, mais à proposer des modèles qu’tolérera en attendant de l’infirmer. L’objectif n’est donc pas de confirmer des hypothèses, mais d’infirmer des modèles (Popper, 1959). Cette notion est particulièrement importante, car elle est à la base de la réflexion concernant un protocole expérimental : plutôt que de se demander « comment il est possible de confirmer une hypothèse », il faudrait se demander « quelle est la pire condition à laquelle un modèle pourrait être exposé pour s’assurer qu’il du sens ? ». Si cet encart peut paraître être très lié à de la méthodologie expérimentale plutôt qu’à des statistiques, il pour objectif de mettre en exergue que tout est lié : il n’est pas possible de comprendre correctement les statistiques sans comprendre la méthodologie et sans avoir les bases théoriques qui sous-tendent des hypothèses (Popper, 1959).Comme tout logiciel de statistiques qui se respecte, R permet de réaliser un ensemble de statistiques descriptives telles qu’une moyenne. Il est dès lors nécessaire de présenter brièvement les possibilités offertes par le logiciel.","code":""},{"path":"les-statistiques-descriptives.html","id":"un-exemple-de-travail","chapter":"8 Les statistiques descriptives","heading":"8.2 Un exemple de travail","text":"Il faut noter que l’exemple présenté ici l’est à titre pédagogique. Des exemples pratiques seront mis à disposition de la section ad hoc. Néanmoins, afin de permettre de suivre les exercices, les données brutes utilisées pour l’exemple sont présentées ci-dessous.Des chercheurs en informatique ont développé un indice pour mesurer la sensibilité des souris : le Mickey. Il s’agit du plus petit mouvement détectable par un souris (de l’ordre d’un centième de millimètre). Voici les valeurs de Mickey pour un échantillon de 50 souris. Pour suivre le contenu de ce chapitre, il est possible de copier coller le code ci-dessous dans la console R, ou mieux dans un script.","code":"\nMickey<-c(0.0113330511711523, 0.0105185934305092, 0.0122513865231973, \n0.00799584561999877, 0.0132469458473191, 0.0157068027032564, \n0.00566814411520385, 0.00651748542037685, 0.0136296612222217, \n0.0131472071409701, 0.0112584029308029, 0.0073426743605309, 0.00819412905929006, \n0.0136385776747903, 0.00862091807205004, 0.00281600176335617, \n0.0110708097988487, 0.0166980801865014, 0.010854177755818, 0.0149495260876833, \n0.00718270492413349, 0.0115037061826869, 0.00636648780668218, \n0.00882227263188745, 0.0110918586149659, 0.010824072121312, 0.0140521995836573, \n0.007002510588409, 0.00684296297629619, 0.00846894194399499, \n0.00951172046799207, 0.0035962148433201, 0.0141868125694707, \n0.00833252179279338, 0.0130351412904844, 0.0112662065604392, \n0.0132115045987431, 0.00750178351850188, 0.00605748408387878, \n0.00901230294010093, 0.00756045671627793, 0.0149879411056116, \n0.0132127370139097, 0.0145783151332873, 0.0115756506296022, 0.0117585225313167, \n0.0145584346705547, 0.0112858517472226, 0.00884871032853877, \n0.0159595622769445)"},{"path":"les-statistiques-descriptives.html","id":"aspects-théoriques","chapter":"8 Les statistiques descriptives","heading":"8.3 Aspects théoriques","text":"","code":""},{"path":"les-statistiques-descriptives.html","id":"les-indices-de-tendance-centrale","chapter":"8 Les statistiques descriptives","heading":"8.3.1 Les indices de tendance centrale","text":"Trois indices de tendance centrale sont communément utilisés en statistiques : la\nmoyenne, la médiane et le mode. Plus récemment, un quatrième indice commencé à émerger : les moyennes tronquées (Wilcox, 2012). Il existe par ailleurs des indices moins connus tels que le M-estimator de Huber (Huber, 1981) ou la moyenne winsorisée (Wilcox, 2012).","code":""},{"path":"les-statistiques-descriptives.html","id":"le-mode","chapter":"8 Les statistiques descriptives","heading":"8.3.1.1 Le mode","text":"Le mode représente la valeur la plus représentée dans l’effectif. Par\nexemple, si 2 personnes ont la valeur 5 et 3 personnes ont la valeur 10, le mode sera 10, car il y plus de personnes qui ont 10 que de personnes qui ont 5. Cet indice de tendance central peut être utilisé sur tous les types de variables (catégorielle, ordinale, métrique). Elle est particulièrement adaptée pour les variables qualitatives.Calculer le mode sur une variable numérique relativement peu de sens, du moins si elle est continue, car il est très rare qu’il y ait plusieurs observations avec exactement la même valeur. Nous allons recatégoriser ces valeurs en valeurs inférieures à 0.005, entre 0.005 et 0.1 , entre 0.01 et 0.015 et au-dessus :peut à présent comptabiliser les valeurs uniques différentes :Dans la table ci-dessus, constate que ce sont les valeurs comprises entre 0.005 et 0.01 qui sont les plus fréquentes, ce qui représente donc le mode.","code":"\nMickey.range<-ifelse(Mickey<0.005, \"<0.005\",\n                     ifelse(Mickey>0.005 & Mickey<0.01, \"0.005<x<0.01\", \n                            ifelse(Mickey>0.015, \"<0.015\",\"0.01<x<0.015\" )))\nMickey.range##  [1] \"0.01<x<0.015\" \"0.01<x<0.015\" \"0.01<x<0.015\" \"0.005<x<0.01\" \"0.01<x<0.015\"\n##  [6] \"<0.015\"       \"0.005<x<0.01\" \"0.005<x<0.01\" \"0.01<x<0.015\" \"0.01<x<0.015\"\n## [11] \"0.01<x<0.015\" \"0.005<x<0.01\" \"0.005<x<0.01\" \"0.01<x<0.015\" \"0.005<x<0.01\"\n## [16] \"<0.005\"       \"0.01<x<0.015\" \"<0.015\"       \"0.01<x<0.015\" \"0.01<x<0.015\"\n## [21] \"0.005<x<0.01\" \"0.01<x<0.015\" \"0.005<x<0.01\" \"0.005<x<0.01\" \"0.01<x<0.015\"\n## [26] \"0.01<x<0.015\" \"0.01<x<0.015\" \"0.005<x<0.01\" \"0.005<x<0.01\" \"0.005<x<0.01\"\n## [31] \"0.005<x<0.01\" \"<0.005\"       \"0.01<x<0.015\" \"0.005<x<0.01\" \"0.01<x<0.015\"\n## [36] \"0.01<x<0.015\" \"0.01<x<0.015\" \"0.005<x<0.01\" \"0.005<x<0.01\" \"0.005<x<0.01\"\n## [41] \"0.005<x<0.01\" \"0.01<x<0.015\" \"0.01<x<0.015\" \"0.01<x<0.015\" \"0.01<x<0.015\"\n## [46] \"0.01<x<0.015\" \"0.01<x<0.015\" \"0.01<x<0.015\" \"0.005<x<0.01\" \"<0.015\"\ntab<-table(Mickey.range)\ntab## Mickey.range\n##       <0.005       <0.015 0.005<x<0.01 0.01<x<0.015 \n##            2            3           19           26\ntab[which.max(tab)]## 0.01<x<0.015 \n##           26"},{"path":"les-statistiques-descriptives.html","id":"la-médiane","chapter":"8 Les statistiques descriptives","heading":"8.3.1.2 La médiane","text":"La médiane représente la valeur x dans un échantillon de N individus qui divisera cet échantillon en deux lorsque les valeurs sont ordonnées de manière croissante. Cet indice de tendance central peut être utilisé avec une variable ordinale et une variable métrique. La raison pour laquelle il est admis d’utiliser la médiane sur les variables ordinales est que cet indice ne nécessite aucune présupposition sur les propriétés d’intervalle des échelles. Par ailleurs, la médiane est particulièrement intéressante pour limiter l’impact des valeurs extrêmes. Par exemple, quand enregistre les temps de réaction, ils sont le plus souvent de l’ordre de quelques centaines de millisecondes. Pourtant,certains temps sont supérieurs à plusieurs secondes. Cela arrive quand l’individu été inattentif à cet instant. L’utilisation des médianes permet d’éviter que cette valeur ait un impact trop important sur les résultats. En d’autres termes, la médiane n’est pas sensible aux valeurs extrêmes. Notez néanmoins que cette médiane est souvent utilisée en intra-individuel (lorsque plusieurs mesures de temps de réaction sont réalisées chez le même individu), mais plus rarement sur des comparaisons de groupes (néanmoins, pour outils inférentiels utilisant la médian, voir Wilcox, 2012).Dans notre exemple, peut commencer par trier les données :L’observation qui va diviser notre échantillon en deux échantillons de taille identique est donc comprise entre la \\(25^e\\) et la \\(26^e\\) observations. va donc faire la moyenne entre ces deux valeurs.peut obtenir cette valeur plus rapidement avec la fonction median :","code":"\nMickey<-sort(Mickey)\nMickey##  [1] 0.002816002 0.003596215 0.005668144 0.006057484 0.006366488 0.006517485\n##  [7] 0.006842963 0.007002511 0.007182705 0.007342674 0.007501784 0.007560457\n## [13] 0.007995846 0.008194129 0.008332522 0.008468942 0.008620918 0.008822273\n## [19] 0.008848710 0.009012303 0.009511720 0.010518593 0.010824072 0.010854178\n## [25] 0.011070810 0.011091859 0.011258403 0.011266207 0.011285852 0.011333051\n## [31] 0.011503706 0.011575651 0.011758523 0.012251387 0.013035141 0.013147207\n## [37] 0.013211505 0.013212737 0.013246946 0.013629661 0.013638578 0.014052200\n## [43] 0.014186813 0.014558435 0.014578315 0.014949526 0.014987941 0.015706803\n## [49] 0.015959562 0.016698080\n(Mickey[25]+Mickey[26])/2## [1] 0.01108133\nmedian(Mickey)## [1] 0.01108133"},{"path":"les-statistiques-descriptives.html","id":"la-moyenne","chapter":"8 Les statistiques descriptives","heading":"8.3.1.3 La moyenne","text":"Le troisième indice de tendance centrale est la moyenne, qui est la somme des valeurs\nindividuelles \\(x_i\\) divisé par le nombre d’observations N :\\[\\mu=\\frac{\\sum_{=1}^{N}X_i}{N}\\]Notez que la moyenne est désignée par la lettre grecque \\(\\mu\\) lorsqu’il s’agit d’un paramètre et par la lettre « m » lorsqu’il s’agit d’une estimation du paramètre à partir d’un échantillon. Cette distinction entre lettres romaines et lettres grecques pour distinguer l’évaluation d’un paramètre ou son estimation à\npartir d’un échantillon s’applique à l’ensemble des indices. Ainsi, pour l’estimation d’une moyenne à partir d’un échantillon, la formule est :\\[m=\\frac{\\sum_{=1}^{N}X_i}{N}\\]Concrètement, si prend notre exemple, la moyenne vaut la somme des Mickey :qu’va diviser par le nombre d’observation :Ce qui donne :peut évidemment obtenir cette valeur directement à l’aide de la fonction mean","code":"\nsum(Mickey)## [1] 0.527654\nlength(Mickey) # 50 observations ## [1] 50\nsum(Mickey)/length(Mickey)## [1] 0.01055308\nmean(Mickey)## [1] 0.01055308"},{"path":"les-statistiques-descriptives.html","id":"la-moyenne-tronquée","chapter":"8 Les statistiques descriptives","heading":"8.3.1.4 La moyenne tronquée","text":"La moyenne tronquée est un mélange entre la moyenne et la médiane : la moyenne tronquée représente la moyenne sur les valeurs qui restent après avoir supprimé un certain pourcentage des valeurs les plus extrêmes. fait donc la moyenne sur $k=1 - 2$ où \\(\\alpha\\) représente la probabilité d’observations tronquées. multiplie cette probabilité par 2 puisque la troncature s’applique sur les deux extrémités de la courbe. Notez que, en fonction du contexte, la troncature peut s’appliquer sur une des extrémités de la courbe uniquement.Ainsi, pour une moyenne tronquée à 10%, il faut d’abord identifier le nombre d’observations sur lesquelles il faut faire la moyenne. En l’occurrence, nous avons 50 observations, 10% de 50 vaut 5. Comme fait la troncature sur les deux extrémités, va donc retirer \\(k= 50\\times 2\\times 0.10 = 10\\). Donc, la moyenne sera réalisée sur \\(R=50 - 10 =40\\) observations restantes.\\[\nT=\\frac{1}{n\\times(1-2\\alpha)}\n\\left(\n(1-r)\\left(X_{g+1}+X_{n-g}\\right)\n+\\sum_{=g+2}^{n-g-1} X_i\n\\right)\n\\]où X représente les valeurs ordonnées, n est le nombre d’observation, \\(\\alpha\\) est la troncature à chaque extrémité, g est la part entière de \\(n \\times \\alpha\\) et r est la partie restante après la virgule de \\(n \\times \\alpha\\).Ainsi, commence par trier les observationsOn identifie la \\(6^e\\) observationEt la \\(45^e\\) observationPuisque \\(0.1 \\times 50 = 5.00\\), la part entière de g vaut 5 et la partie restante après la virgule vaut 0.peut à présent compléter la formule avec les valeurs suivantes :\\[\nT=\\frac{1}{n\\times(1-(2\\alpha))}\n\\left(\n(1-r)\\left(X_{g+1}+X_{n-g}\\right)\n+\\sum_{=g+2}^{n-g-1} X_i\n\\right)\n\\]\\[\nT=\\frac{1}{50 \\times (1-(2 \\times 0.10))}\n\\left(\n(1-0)\\left(0.0065 + 0.0146\\right)\n+\\sum_{=7}^{44} X_i\n\\right) = 0.01062119\n\\]Dans R, obtient la moyenne tronquée en précisant l’argument tr (pour trimmed) de la fonction mean.","code":"\nMickey.tri<- sort(Mickey)\nMickey.tri[6]## [1] 0.006517485\nMickey.tri[45]## [1] 0.01457832\nTr<-(1/40 *((1-0)*(Mickey.tri[6]+Mickey.tri[45])+sum(Mickey.tri[7:44])))\nTr## [1] 0.01062119\nmean(Mickey, tr=0.10)## [1] 0.01062119"},{"path":"les-statistiques-descriptives.html","id":"la-moyenne-winsorisée","chapter":"8 Les statistiques descriptives","heading":"8.3.1.5 La moyenne winsorisée","text":"La moyenne winsorisée fonctionne sur un principe similaire à la moyenne tronquée. Elle s’en distingue par le fait que, au lieu de ne pas inclure les valeurs extrêmes dans le calcul, ramène les valeurs tronquées aux dernières valeurs à l’intérieur des limites. D’après Wilcox (2012), il s’agit d’un indicateur plus robuste que la moyenne tronquée.Dans notre exemple, la \\(6^e\\) valeur des Mickey lorsqu’les trie par ordre croissant estOn va donc attribuer cette valeur aux 5 valeurs inférieures à cette valeur :fait la même chose pour les observations au-delà de la 45ème position :Ainsi, les 6 premières d’observations ont la même valeur et les 6 dernières ont la même valeur :peut à présent faire la moyenne sur ces valeursLa fonction winsor.mean du package ‘psych’ permet d’obtenir la moyenne winsorisée :note : peut également calculer une variance ou un écart-type winsorisé.","code":"\nMickey.tri[6]## [1] 0.006517485\nMickey.win<-Mickey.tri\nMickey.win[1:5] <-Mickey.win[6]\nMickey.win[46:50]<-Mickey.win[45]\nMickey.win##  [1] 0.006517485 0.006517485 0.006517485 0.006517485 0.006517485 0.006517485\n##  [7] 0.006842963 0.007002511 0.007182705 0.007342674 0.007501784 0.007560457\n## [13] 0.007995846 0.008194129 0.008332522 0.008468942 0.008620918 0.008822273\n## [19] 0.008848710 0.009012303 0.009511720 0.010518593 0.010824072 0.010854178\n## [25] 0.011070810 0.011091859 0.011258403 0.011266207 0.011285852 0.011333051\n## [31] 0.011503706 0.011575651 0.011758523 0.012251387 0.013035141 0.013147207\n## [37] 0.013211505 0.013212737 0.013246946 0.013629661 0.013638578 0.014052200\n## [43] 0.014186813 0.014558435 0.014578315 0.014578315 0.014578315 0.014578315\n## [49] 0.014578315 0.014578315\nmean(Mickey.win)## [1] 0.01060654\nlibrary(psych)\nwinsor.mean(Mickey.win, tr=0.10)## [1] 0.01060654"},{"path":"les-statistiques-descriptives.html","id":"le-m-estimateur-de-huber","chapter":"8 Les statistiques descriptives","heading":"8.3.1.6 Le M-estimateur de Huber9","text":"Il s’agit d’une fonction de minimisation. Il fonctionne sur la base d’un processus itératif (comme le maximum de vraisemblance si cette notion vous parle). Huber (1981) proposé d’utiliser cette fonction de minimisation pour estimer le paramètre d’une moyenne.\\[\\sum_{=1}^n \\psi \\left(\\frac{X_i-t}{\\sigma} \\right)= 0 \\]où\n\\[ \\psi_k(x) =  \\left \\{\n   \\begin{array}{r c l}\n      k, & x \\geq k \\\\\n      x   &  -k \\geq x \\leq k \\\\\n      -k  & x \\leq -k\n   \\end{array}\n   \\right .\\]Si, d’un point de vue mathématique, cette fonction peut paraître un peu compliquée, se rend compte en termes de calcul qu’il s’agit d’une opération assez simple.commence par définir la moyenne par la médiane.Pour cette fonction, l’idée est de déterminer si, dans un ensemble de données, il existe des valeurs extrêmes et, si c’est le cas, de les remplacer par des valeurs moins extrêmes. Un des moyens pour identifier s’il y des valeurs extrêmes est de déterminer si certaines valeurs s’éloignent de plus de 1.5 écart moyen à la médiane (mad - voir les indices de dispersion). Pour bien comprendre le fonctionnement de cette fonction, nous prendrons des données qui présentent des valeurs extrêmes.La première étape est donc de déterminer un indice de tendance centrale, et cet indice est la médiane. Dans notre exemple, la médiane de vaut :et le madOn se rend compte que les valeurs 100 et 1000 sont bien supérieures à 1.5 mad de la médiane. va donc les remplacer par $médiane + 1.5 mad $ :obtient donc les valeurs suivantes :peut à présent calculer la moyenne sur ces valeurs :va vérifier si l’estimation de notre moyenne s’écarte d’une valeur inférieure à une valeur déterminée de tolérance. Généralement, considère qu’une tolérance acceptable est \\(10^{-6}\\). Ainsi, il ne faut pas que la moyenne s’écarte de plus de \\(10^{-6} \\times mad\\) en valeur absolue.Si l’écart est supérieur, comme c’est le cas ici, répète l’opération, mais va comparer la prochaine moyenne estimée à la première moyenne estimée. Concrètement,calcule une nouvelle moyenne :Et vérifie si l’écart entre la première et la seconde estimation de la moyenne est inférieure à la tolérance :Ce n’est pas le cas, recommence l’opérationCe n’est toujours pas le cas, va donc encore répéter l’opération jusqu’à ce notre test devienne vrai :la 8e itération, le critère de tolérance est accepté, nous avons donc obtenu notre M-estimateur qui vaut :obtient le M-estimateur de Huber à l’aide de la fonction huber du package MASS.","code":"\na<-c(1:10, 100, 1000)\na##  [1]    1    2    3    4    5    6    7    8    9   10  100 1000\nmedian(a) ## [1] 6.5\nmad(a) ## [1] 4.4478\n  median(a) + 1.5*mad(a)## [1] 13.1717\n  yy <- pmin(pmax(median(a) - 1.5 * mad(a), a), median(a) + 1.5 * mad(a))\n  yy##  [1]  1.0000  2.0000  3.0000  4.0000  5.0000  6.0000  7.0000  8.0000  9.0000\n## [10] 10.0000 13.1717 13.1717\nm<-mean(yy)\nm## [1] 6.778617\nabs(m-median(a)) < 10^-6 *mad(a)## [1] FALSE\n  yy <- pmin(pmax(m - 1.5 * mad(a), a), m + 1.5 * mad(a))\n  yy##  [1]  1.00000  2.00000  3.00000  4.00000  5.00000  6.00000  7.00000  8.00000\n##  [9]  9.00000 10.00000 13.45032 13.45032\n  m2 <- mean(yy)\n  m2## [1] 6.825053\nabs(m-m2) < 10^-6 *mad(a)## [1] FALSE\n  yy <- pmin(pmax(m2 - 1.5 * mad(a), a), m2 + 1.5 * mad(a))\n  yy##  [1]  1.00000  2.00000  3.00000  4.00000  5.00000  6.00000  7.00000  8.00000\n##  [9]  9.00000 10.00000 13.49675 13.49675\n  m3 <- mean(yy)\n  m3## [1] 6.832792\nabs(m2-m3) < 10^-6 *mad(a)## [1] FALSE\n# 4e itération   \n  yy <- pmin(pmax(m3 - 1.5 * mad(a), a), m3 + 1.5 * mad(a))\n  m4<-mean(yy)\n  abs(m4-m3) < 10^-6 *mad(a)## [1] FALSE\n# 5e itération   \n  yy <- pmin(pmax(m4 - 1.5 * mad(a), a), m4 + 1.5 * mad(a))\n  m5<-mean(yy)\n  abs(m4-m5) < 10^-6 *mad(a)## [1] FALSE\n# 6e itération   \n  yy <- pmin(pmax(m5 - 1.5 * mad(a), a), m5 + 1.5 * mad(a))\n  m6<-mean(yy)\n  abs(m6-m5) < 10^-6 *mad(a)## [1] FALSE\n# 7e itération  \n  yy <- pmin(pmax(m6 - 1.5 * mad(a), a), m6 + 1.5 * mad(a))\n  m7<-mean(yy)\n  abs(m6-m7) < 10^-6 *mad(a)## [1] FALSE\n# 8e itération  \n  yy <- pmin(pmax(m7 - 1.5 * mad(a), a), m7+ 1.5 * mad(a))\n  m8<-mean(yy)\n  abs(m8-m7) < 10^-6 *mad(a)## [1] TRUE\nm8## [1] 6.83434\nlibrary(MASS)\nhuber(a)## $mu\n## [1] 6.834339\n## \n## $s\n## [1] 4.4478"},{"path":"les-statistiques-descriptives.html","id":"les-indices-de-dispersion","chapter":"8 Les statistiques descriptives","heading":"8.3.2 Les indices de dispersion","text":"","code":""},{"path":"les-statistiques-descriptives.html","id":"minimum-maximum-et-étendue","chapter":"8 Les statistiques descriptives","heading":"8.3.2.1 Minimum, maximum et étendue","text":"Le minimum est la plus petite valeur à notre disposition, tandis que le maximum est la valeur la plus élevée.L’étendue est la différence entre la valeur maximale et la valeur minimaleOn obtient cette étendue en combinant la fonction diff avec la fonction range :","code":"\nmin(Mickey) # minimum## [1] 0.002816002\nmax(Mickey) # maximum## [1] 0.01669808\nmax(Mickey)- min(Mickey)## [1] 0.01388208\ndiff(range(Mickey))## [1] 0.01388208"},{"path":"les-statistiques-descriptives.html","id":"quartile-et-centiles","chapter":"8 Les statistiques descriptives","heading":"8.3.2.2 Quartile et centiles","text":"Tout comme la médiane permet de séparer","code":""},{"path":"les-statistiques-descriptives.html","id":"variance","chapter":"8 Les statistiques descriptives","heading":"8.3.2.3 Variance","text":"","code":""},{"path":"les-statistiques-descriptives.html","id":"écart-type-et-intervalle-de-confiance","chapter":"8 Les statistiques descriptives","heading":"8.3.2.4 Écart-type et intervalle de confiance","text":"","code":""},{"path":"les-statistiques-descriptives.html","id":"écart-moyen-absolu-à-la-médiane-mad","chapter":"8 Les statistiques descriptives","heading":"8.3.2.5 Écart moyen absolu à la médiane (mad)","text":"","code":""},{"path":"les-statistiques-descriptives.html","id":"les-statistiques-descriptives-avec-r","chapter":"8 Les statistiques descriptives","heading":"8.4 Les statistiques descriptives avec R","text":"Ainsi, pour rappel, il est toujours bon d’avoir installé et chargé les packages au début de votre script. Nous aurons besoin du package psych (Revelle, 2025), readxl (Wickham & Bryan, 2022), ggplot2 (Wickham, 2016b) et éventuellement Hmisc (Harrell Jr, 2025) et summarytools (Comtois, 2025)Les packages que vous n’avez pas encore installés peuvent être installés avec les lignes de commande suivantes :Pour charger ces packages, il faut utiliser les lignes de commande suivantes :","code":"\ninstall.packages(\"ggplot2\")\ninstall.packages(\"psych\")\ninstall.packages(\"readxl\")\ninstall.packages(\"Hmisc\")\ninstall.packages(\"summarytools\")\ninstall.packages(\"grid\")\ninstall.packages(\"titanic\")\nlibrary(\"ggplot2\")\nlibrary(\"psych\")\nlibrary(\"readxl\")\nlibrary(\"Hmisc\")\nlibrary(\"summarytools\")\nlibrary(\"grid\")\nlibrary(\"titanic\")"},{"path":"les-statistiques-descriptives.html","id":"traiter-des-données-quantitatives","chapter":"8 Les statistiques descriptives","heading":"8.5 Traiter des données quantitatives","text":"Lorsque l’cherche à analyser des données quantitatives, il est important de fournir des statistiques descriptives telles que la moyenne, la médiane, l’écart-type, le minimum, le maximum ou encore les quartiles. Ces statistiques permettent d’obtenir les caractéristiques principales d’une variable quantitative. La façon la plus simple et la plus rapide d’y parvenir est d’utiliser la fonction summary.Afin d’illustrer cette fonction (et les suivantes), nous utiliserons le jeu de données mtcars qui est intégré dans R.\nLa base de données mtcars contient des informations sur 32 voitures et 11 variables, telles que la consommation de carburant par miles (mpg), le nombre de cylindres (cyl), la puissance en chevaux (hp), et le poids (wt). Pour ceux qui sont intéressés, vous pouvez obtenir le détail complet du jeu de données en utilisant la fonction ?mtcars. Les premières lignes du jeu de données sont présentées ci-dessous :mpgcyldisphpdratwtqsecvsamgearcarb21.061601103.902.62016.46014421.061601103.902.87517.02014422.84108933.852.32018.61114121.462581103.083.21519.44103118.783601753.153.44017.02003218.162251052.763.46020.221031On obtient donc les statistiques descriptives en utilisant la fonction summary de ce jeu de données de la manière suivante :Cependant, il existe plusieurs packages R permettant d’obtenir des statistiques descriptives plus précises ou plus directement, notamment la fonction describe du package Hmisc (Harrell Jr, 2025) ou l’intégralité du package summarytools (Comtois, 2025) qui est spécifiquement dédié aux statistiques descriptives. Dans ce document, nous utiliserons principalement le package psych (Revelle, 2025) et plus particulièrement les fonctions describe et describeby qui permettent de fournir rapidement les statistiques descriptives souhaitées. Nous laissons le soin au lecteur intéressé d’explorer les autres fonctions et packages si les informations fournies par le package psych ne sont pas suffisantes.Attention : Si vous avez chargés à la fois les packages psych et hmisc, vous pourriez rencontrer un conflit car ces deux packages possèdent une fonction appelée describe. Cela peut entraîner des erreurs lors de l’exécution de votre code. Pour éviter ce problème, veillez à préciser le package dont vous souhaitez utiliser la fonction en écrivant par exemple : psych::describe ou Hmisc::describe. Cette syntaxe permet de spécifier clairement le package à utiliser dans votre code en faisant préceder la fonction describe par le nom du package suivant de deux fois deux points.Table 8.1: Statistiques descriptives des caractéristiques des voitures (mtcars)varsnmeansdmediantrimmedmadminmaxrangeskewkurtosisse132.0020.096.0319.2019.705.4110.4033.9023.500.61-0.371.07232.006.191.796.006.232.974.008.004.00-0.17-1.760.32332.00230.72123.94196.30222.52140.4871.10472.00400.900.38-1.2121.91432.00146.6968.56123.00141.1977.1052.00335.00283.000.73-0.1412.12532.003.600.533.703.580.702.764.932.170.27-0.710.09632.003.220.983.333.150.771.515.423.910.42-0.020.17732.0017.851.7917.7117.831.4214.5022.908.400.370.340.32832.000.440.500.000.420.000.001.001.000.24-2.000.09932.000.410.500.000.380.000.001.001.000.36-1.920.091032.003.690.744.003.621.483.005.002.000.53-1.070.131132.002.811.622.002.651.481.008.007.001.051.260.29Lorsque l’execute le code dans R, peut constater que la fonction describe du package psych fournit des statistiques descriptives pour chaque variable de la base de données mtcars.En regardant les résultats nous savons par exemple qu’il y 32 individus et que le nombre moyen de cylindre par voiture est de 6.19.peut également faire apparaître simplement les quantiles de nos variables dans le tableau des résultats si veut avoir une idée de la distribution de nos données.L’argument quant permet de spécifier les qunatiles à calculer pour chaque variable.Table 8.2: Statistiques descriptives de mtcars avec les quantilesvarsnmeansdmediantrimmedmadminmaxrangeskewkurtosisseQ0.1Q0.25Q0.5Q0.75Q0.9132.0020.096.0319.2019.705.4110.4033.9023.500.61-0.371.0714.3415.4319.2022.8030.09232.006.191.796.006.232.974.008.004.00-0.17-1.760.324.004.006.008.008.00332.00230.72123.94196.30222.52140.4871.10472.00400.900.38-1.2121.9180.61120.83196.30326.00396.00432.00146.6968.56123.00141.1977.1052.00335.00283.000.73-0.1412.1266.0096.50123.00180.00243.50532.003.600.533.703.580.702.764.932.170.27-0.710.093.013.083.703.924.21632.003.220.983.333.150.771.515.423.910.42-0.020.171.962.583.333.614.05732.0017.851.7917.7117.831.4214.5022.908.400.370.340.3215.5316.8917.7118.9019.99832.000.440.500.000.420.000.001.001.000.24-2.000.090.000.000.001.001.00932.000.410.500.000.380.000.001.001.000.36-1.920.090.000.000.001.001.001032.003.690.744.003.621.483.005.002.000.53-1.070.133.003.004.004.005.001132.002.811.622.002.651.481.008.007.001.051.260.291.002.002.004.004.00La variance de nos variables n’apparait pas dans le tableau et doit donc ajouter une colonne que l’va inclure dans le tableau si désire accéder à cette information. Il suffit d’élever l’écart-type (correspondant à la colonne sd) au carré pour obtenir la variance.Table 8.3: Statistiques descriptives des caractéristiques des voitures (mtcars) - avec variancevarsnmeansdmediantrimmedmadminmaxrangeskewkurtosisseQ0.1Q0.25Q0.5Q0.75Q0.9variance132.0020.096.0319.2019.705.4110.4033.9023.500.61-0.371.0714.3415.4319.2022.8030.0936.32232.006.191.796.006.232.974.008.004.00-0.17-1.760.324.004.006.008.008.003.19332.00230.72123.94196.30222.52140.4871.10472.00400.900.38-1.2121.9180.61120.83196.30326.00396.0015,360.80432.00146.6968.56123.00141.1977.1052.00335.00283.000.73-0.1412.1266.0096.50123.00180.00243.504,700.87532.003.600.533.703.580.702.764.932.170.27-0.710.093.013.083.703.924.210.29632.003.220.983.333.150.771.515.423.910.42-0.020.171.962.583.333.614.050.96732.0017.851.7917.7117.831.4214.5022.908.400.370.340.3215.5316.8917.7118.9019.993.19832.000.440.500.000.420.000.001.001.000.24-2.000.090.000.000.001.001.000.25932.000.410.500.000.380.000.001.001.000.36-1.920.090.000.000.001.001.000.251032.003.690.744.003.621.483.005.002.000.53-1.070.133.003.004.004.005.000.541132.002.811.622.002.651.481.008.007.001.051.260.291.002.002.004.004.002.61On obtient donc une colonne supplémentaire dans la table avec la variance de chacune de nos variables.","code":"\ndata(mtcars)\nsummary(mtcars) ##       mpg             cyl             disp             hp       \n##  Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n##  1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n##  Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n##  Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n##  3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n##  Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n##       drat             wt             qsec             vs        \n##  Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n##  1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n##  Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n##  Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n##  3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n##  Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n##        am              gear            carb      \n##  Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n##  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n##  Median :0.0000   Median :4.000   Median :2.000  \n##  Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n##  3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n##  Max.   :1.0000   Max.   :5.000   Max.   :8.000\ndesc<-psych::describe(mtcars)\ndesc.data<-psych::describe(mtcars,quant=c(.1,.25,.5,.75,.90)) \n#Ajout de la colonne variance \n\ndesc.data$variance<-desc.data$sd^2"},{"path":"les-statistiques-descriptives.html","id":"analyse-en-sous-groupe","chapter":"8 Les statistiques descriptives","heading":"8.5.1 Analyse en sous-groupe","text":"L’analyse en sous-groupes permet de mieux comprendre les variations au sein d’un jeu de données notamment en fonction de critères spécifiques. Par exemple, des scientifiques font souvent des hypothèses concernant des différences de moyennes entre des conditions. Analyser les données en sous-groupes permet de savoir si les résultats vont dans le sens des hypothèses, et quand ils sont significatifs, ces statistiques descriptives permettent d’interpréter le sens de l’inférence à devoir réaliser.\nEn résumé, diviser les données en groupes permet d’identifier des tendances, des différences ou des comportements particuliers qui seraient invisibles dans une analyse globale. Cette approche permet ainsi d’obtenir des informations plus détaillées et pertinentes pour chaque groupe.La fonction describeby de psych permet à ce titre d’analyser un jeu de données en créant des sous-groupes basés sur une variable catégorielle.Cela nous donne des statistiques descriptives détaillées pour les voitures à 4, 6 et 8 cylindres. Cependant, l’analyse des résultats peut être complexe comme le voit avec la sortie du code, car ces sous-groupes génèrent beaucoup de valeurs à interpréter. Il peut donc être difficile de tirer des conclusions rapidement si l’regarde toutes les statistiques en même temps et les tableaux peuvent être difficile à lire simultanément.À ce titre, nous allons nous intéresser spécifiquement aux voitures à 4 cylindres, ce qui nous permettra de simplifier l’interprétation des données en nous concentrant sur un sous-groupe particulier.Les résultats sont présentés dans le Tableau @(tab:cylinde)Ce code extrait uniquement les statistiques descriptives pour les voitures à 4 cylindres à partir des résultats complets obtenus précédemment avec la fonction describeBy.Table 8.4: Statistiques descriptives des voitures à 4 cylindres.varsnmeansdmediantrimmedmadminmaxrangeskewkurtosisse111.0026.664.5126.0026.446.5221.4033.9012.500.26-1.651.36211.004.000.004.004.000.004.004.000.000.00311.00105.1426.87108.00104.3043.0071.10146.7075.600.12-1.648.10411.0082.6420.9391.0082.6732.6252.00113.0061.000.01-1.716.31511.004.070.374.084.020.343.694.931.241.000.120.11611.002.290.572.202.270.541.513.191.680.30-1.360.17711.0019.141.6818.9018.991.4816.7022.906.200.55-0.020.51811.000.910.301.001.000.000.001.001.00-2.474.520.09911.000.730.471.000.780.000.001.001.00-0.88-1.310.141011.004.090.544.004.110.003.005.002.000.11-0.010.161111.001.550.522.001.560.001.002.001.00-0.16-2.150.16","code":"\n# La fonction describeBy permet de calculer des statistiques descriptives pour chaque sous-groupe\n# basé sur une variable catégorielle, ici le nombre de cylindres des voitures.\n\ndesc_by_cyl <- psych::describeBy(mtcars, group = mtcars$cyl)\n\ndesc_by_cyl## \n##  Descriptive statistics by group \n## group: 4\n##      vars  n   mean    sd median trimmed   mad   min    max range  skew\n## mpg     1 11  26.66  4.51  26.00   26.44  6.52 21.40  33.90 12.50  0.26\n## cyl     2 11   4.00  0.00   4.00    4.00  0.00  4.00   4.00  0.00   NaN\n## disp    3 11 105.14 26.87 108.00  104.30 43.00 71.10 146.70 75.60  0.12\n## hp      4 11  82.64 20.93  91.00   82.67 32.62 52.00 113.00 61.00  0.01\n## drat    5 11   4.07  0.37   4.08    4.02  0.34  3.69   4.93  1.24  1.00\n## wt      6 11   2.29  0.57   2.20    2.27  0.54  1.51   3.19  1.68  0.30\n## qsec    7 11  19.14  1.68  18.90   18.99  1.48 16.70  22.90  6.20  0.55\n## vs      8 11   0.91  0.30   1.00    1.00  0.00  0.00   1.00  1.00 -2.47\n## am      9 11   0.73  0.47   1.00    0.78  0.00  0.00   1.00  1.00 -0.88\n## gear   10 11   4.09  0.54   4.00    4.11  0.00  3.00   5.00  2.00  0.11\n## carb   11 11   1.55  0.52   2.00    1.56  0.00  1.00   2.00  1.00 -0.16\n##      kurtosis   se\n## mpg     -1.65 1.36\n## cyl       NaN 0.00\n## disp    -1.64 8.10\n## hp      -1.71 6.31\n## drat     0.12 0.11\n## wt      -1.36 0.17\n## qsec    -0.02 0.51\n## vs       4.52 0.09\n## am      -1.31 0.14\n## gear    -0.01 0.16\n## carb    -2.15 0.16\n## ------------------------------------------------------------ \n## group: 6\n##      vars n   mean    sd median trimmed   mad    min    max  range  skew\n## mpg     1 7  19.74  1.45  19.70   19.74  1.93  17.80  21.40   3.60 -0.16\n## cyl     2 7   6.00  0.00   6.00    6.00  0.00   6.00   6.00   0.00   NaN\n## disp    3 7 183.31 41.56 167.60  183.31 11.27 145.00 258.00 113.00  0.80\n## hp      4 7 122.29 24.26 110.00  122.29  7.41 105.00 175.00  70.00  1.36\n## drat    5 7   3.59  0.48   3.90    3.59  0.03   2.76   3.92   1.16 -0.74\n## wt      6 7   3.12  0.36   3.21    3.12  0.36   2.62   3.46   0.84 -0.22\n## qsec    7 7  17.98  1.71  18.30   17.98  1.90  15.50  20.22   4.72 -0.12\n## vs      8 7   0.57  0.53   1.00    0.57  0.00   0.00   1.00   1.00 -0.23\n## am      9 7   0.43  0.53   0.00    0.43  0.00   0.00   1.00   1.00  0.23\n## gear   10 7   3.86  0.69   4.00    3.86  0.00   3.00   5.00   2.00  0.11\n## carb   11 7   3.43  1.81   4.00    3.43  0.00   1.00   6.00   5.00 -0.26\n##      kurtosis    se\n## mpg     -1.91  0.55\n## cyl       NaN  0.00\n## disp    -1.23 15.71\n## hp       0.25  9.17\n## drat    -1.40  0.18\n## wt      -1.98  0.13\n## qsec    -1.75  0.65\n## vs      -2.20  0.20\n## am      -2.20  0.20\n## gear    -1.24  0.26\n## carb    -1.50  0.69\n## ------------------------------------------------------------ \n## group: 8\n##      vars  n   mean    sd median trimmed   mad    min    max  range  skew\n## mpg     1 14  15.10  2.56  15.20   15.15  1.56  10.40  19.20   8.80 -0.36\n## cyl     2 14   8.00  0.00   8.00    8.00  0.00   8.00   8.00   0.00   NaN\n## disp    3 14 353.10 67.77 350.50  349.63 73.39 275.80 472.00 196.20  0.45\n## hp      4 14 209.21 50.98 192.50  203.67 44.48 150.00 335.00 185.00  0.91\n## drat    5 14   3.23  0.37   3.12    3.19  0.16   2.76   4.22   1.46  1.34\n## wt      6 14   4.00  0.76   3.76    3.95  0.41   3.17   5.42   2.25  0.99\n## qsec    7 14  16.77  1.20  17.18   16.86  0.79  14.50  18.00   3.50 -0.80\n## vs      8 14   0.00  0.00   0.00    0.00  0.00   0.00   0.00   0.00   NaN\n## am      9 14   0.14  0.36   0.00    0.08  0.00   0.00   1.00   1.00  1.83\n## gear   10 14   3.29  0.73   3.00    3.17  0.00   3.00   5.00   2.00  1.83\n## carb   11 14   3.50  1.56   3.50    3.25  0.74   2.00   8.00   6.00  1.48\n##      kurtosis    se\n## mpg     -0.57  0.68\n## cyl       NaN  0.00\n## disp    -1.26 18.11\n## hp       0.09 13.62\n## drat     1.08  0.10\n## wt      -0.71  0.20\n## qsec    -0.92  0.32\n## vs        NaN  0.00\n## am       1.45  0.10\n## gear     1.45  0.19\n## carb     2.24  0.42\n# Ici, nous sélectionnons les statistiques spécifiques aux voitures à 4 cylindres \n# parmi les résultats obtenus pour tous les sous-groupes de cylindres.\n\ndesc_4_cyl <- desc_by_cyl[[\"4\"]]"},{"path":"les-statistiques-descriptives.html","id":"faire-une-représentation-graphique-des-données","chapter":"8 Les statistiques descriptives","heading":"8.6 Faire une représentation graphique des données","text":"Lorsque l’mène une analyse descriptive de données sur R, peut s’aider à l’aide de graphiques, ce qui permet de mieux visualiser leur distribution, leur tendance et les relations entre les variables. Pour cela, le package ggplot2 (Wickham, 2016b) est largement utilisé. Ce package permet de créer une variété de graphiques personnalisés, comme des histogrammes, des graphiques en violon ou des diagrammes de dispersion, facilitant ainsi l’analyse et l’interprétation des données.","code":""},{"path":"les-statistiques-descriptives.html","id":"graphique-violon","chapter":"8 Les statistiques descriptives","heading":"8.6.1 Graphique violon","text":"Le graphique en violon (violin plot) est une excellente optioin pour visualiser des données quantitatives. Il permet de représenter la distribution des valeurs d’une variable en montrant à la fois la densité et les principales statistiques descriptives.Le package ggplot2 (Wickham, 2016b), avec sa fonction ggplot permet de crée assez aisément un graphique en violon. Pour faciliter la lisibilité, les lignes de commande seront annotées au sein du code.Dans l’exemple suivant, va représente la consommation de carburant en miles par gallon (mpg). L’axe des ordonnées représente la consommation et l’axe des abscisses est fixé à une valeur constante pour afficher une seule distribution. Le violon montre la densité de la variable, et un point rouge indique la moyenne de la consommation, accompagné d’une barre bleue représentant l’intervalle de confiance autour de cette moyenne.\nFigure 8.1: Distribution de la consommation de carburant par miles (mpg)\npeut également utiliser ce type de graphique pour comparer la distribution d’une variable continue en fonction d’une variable catégorielle.\nPar exemple, dans le code suivant, la variable catégorielle cyl (le nombre de cylindres) est utilisée pour créer des violons séparés pour chaque groupe de cylindres, permettant de comparer les distributions de la consommation pour chaque catégorie de cylindres.\nFigure 8.2: Distribution de la consommation de carburant par miles (mpg) en fonction du nombre de cylindres (cyl)\nobserve que chaque violon correspond à une catégorie de cylindre (4,6 ou 8 cylindres) et montre la densité des valeurs de consommation à l’intérieur de cette catégorie.Avec cet exemple peut dire que les véhicules à 4 cylindres ont une consommation de carburant généralement moins élevée,cela montre qu’elles sont plus économes en carburant tandis que les véhicules à 8 cylindres ont une consommation plus élevée. Les véhicules à 6 cylindres se situent entre ces deux extrêmes.\nDans cet exemple, nous allons expliquer comment lire un graphique en violon en utilisant un exemple de la densité de la loi normale. Le graphique ci-dessous montre la densité de la distribution normale générée avec une moyenne de 50 et un écart-type de 10.Un graphique en violon représente la distribution des données en combinant des éléments d’un diagramme en boîte et la visualisation de la densité des données. Cette visualisation permet de montrer non seulement la tendance centrale (moyenne ou médiane), mais aussi la forme de la distribution et sa dispersion. Cela ressemble à la courbe de densité d’une loi normale, comme celle présentée ici par la courbe rouge.Les graphiques en violon sont souvent utilisés pour visualiser la distribution des données. Cependant, ces graphiques peuvent être difficiles à interpréter sans repères. C’est pourquoi nous utilisons la loi normale (ou distribution gaussienne) comme référence. La loi normale est l’une des distributions les plus courantes en statistiques et possède une forme bien définie et symétrique. En comparant la forme du graphique en violon à celle d’une loi normale, nous pouvons mieux comprendre comment les données se répartissent, identifier les éventuels écarts et repérer les asymétries ou anomalies dans la distribution.En d’autres termes, la courbe de densité de la loi normale nous aide à avoir un repère visuel pour interpréter les graphiques en violon.Dans le graphique en violon, cette densité est simplement représentée dans un axe à 90° par rapport à cette distribution.\nFigure 8.3: Représentation de la densité d’une distribution normale telle qu’peut la rencontrer dans un graphe violon.\nLes graphiques en violon sont souvent présentés avec une orientation horizontale, comme c’est le cas avec le graphique ci-dessus. Cela permet de mieux visualiser la densité des données sur un axe horizontal, ce qui correspond à l’orientation habituelle des graphiques en violon.ExerciceDansla base de données mickey2, trouve les caractéristiques de souris d’ordinateur. Veuillez répondre aux questions proposées.Utilisez la fonction describe ou bien la fonction describeBy du package psych (Revelle, 2025) pour obtenir des statistiques descriptives sur la variable Prix de notre jeu de données, tout en calculant également la variance. De plus, nous voudrions les quantiles .10, .25, .75, .90 de cette variable.Par ailleurs, voudrait également les statistiques descriptives de la variable prix en fonction des dpi des soucis.réaliser une analyse descriptive des variables, réaliser une analyse en sous-groupe avec la variable de votre choix dans la base de données mickey2 et réalisez au moins un graphique avec ggplot2.Nous devons dans un premier temps importer le jeu de données :À présent, nous allons définir une fonction appelée Prix, qui extraira spécifiquement la variable Prix du jeu de données Mickey. Cette fonction nous permettra de simplifier l’analyse en nous concentrant uniquement sur cette variable, afin d’explorer ses statistiques descriptives et d’identifier ses principales caractéristiques (Tableau 8.5).Table 8.5: Presentation du jeu de données mickeysourismickeyPoidsPrixDPIhz1.000.0185.0045.99800.00125.002.000.01120.0089.501,200.00500.003.000.0195.0019.991,600.001,000.004.000.0175.00129.00800.00125.005.000.01100.0059.992,400.00500.006.000.02110.0075.001,600.001,000.007.000.0190.00105.991,000.00125.008.000.01105.0089.901,200.00500.009.000.0180.0029.991,600.001,000.0010.000.01130.0049.002,000.00500.0011.000.01115.0085.00800.001,000.0012.000.0195.00120.002,400.00125.0013.000.01125.0095.503,200.00500.0014.000.0185.0060.00800.00125.0015.000.01110.00119.991,800.001,000.0016.000.00100.0050.001,200.00500.0017.000.01120.00110.001,000.00125.0018.000.02105.0069.992,400.001,000.0019.000.01115.0079.993,200.00500.0020.000.0195.0085.991,600.00125.0021.000.0180.00109.001,000.001,000.0022.000.01110.0059.99800.00125.0023.000.01130.0099.001,600.00500.0024.000.01120.0055.504,000.001,000.0025.000.01100.0089.995,000.00500.0026.000.0190.00120.001,000.001,000.0027.000.01125.0079.001,200.00125.0028.000.01105.00135.002,000.00500.0029.000.01115.0065.99800.00125.0030.000.01110.0079.991,600.001,000.0031.000.0195.0049.992,400.00125.0032.000.0085.0089.003,200.001,000.0033.000.01100.00120.501,800.00500.0034.000.01110.00100.002,400.00500.0035.000.01120.0069.004,000.00125.0036.000.0190.00110.991,000.001,000.0037.000.01105.0045.001,800.00125.0038.000.01115.0080.991,600.00500.0039.000.01100.00125.001,200.001,000.0040.000.01110.0090.002,000.00125.0041.000.01130.0075.993,200.00500.0042.000.01115.00120.001,600.001,000.0043.000.0185.0060.002,400.00125.0044.000.01100.0069.993,200.00500.0045.000.01125.00115.001,600.00125.0046.000.0195.0099.991,800.001,000.0047.000.01120.00130.002,400.00125.0048.000.01105.0060.991,600.001,000.0049.000.01110.0060.00800.00500.0050.000.0290.0045.004,000.00500.00Pour cela, nous allons utiliser le package psych et sa fonction describe, en appliquant ces outils directement à la fonction Prix, qui extrait cette variable du jeu de données Mickey.obtient le résultat suivant :Table 8.6: Statistiques descriptives de la variable prixvarsnmeansdmediantrimmedmadminmaxrangeskewkurtosisseQ0.1Q0.25Q0.75Q0.9variance1.0050.0083.6328.5283.0083.9134.0919.99135.00115.01-0.05-0.934.0348.7060.00108.25120.05813.40On constate que la moyenne des prix est de 83.63, ce qui reflète le prix moyen dans le jeu de données. L’écart-type vaut 28.52, avec des prix allant de 19.99 euros à 135.00 euros.Si veut effectuer une analyse en sous-groupes, il suffit d’utiliser la même logique que celle que nous avons employée ici. Imaginons que l’veuille des statistiques descriptives concernant la variable DPI , alors nous pouvons utiliser la fonction describeby de psych.obtient nos statistiques descriptives pour chaque sous-groupe de DPI.Tableau . Statistiques descriptives des variables Poids,Prix et hzcolumn nameitemgroup1varsnmeansdmediantrimmedmadminmaxrangeskewkurtosissePrix118001.007.0072.2827.5860.0072.288.8845.99129.0083.011.09-0.3010.42Prix2210001.005.00111.205.27110.00111.201.48105.99120.0014.010.72-1.242.35Prix3312001.005.0086.6826.8889.5086.6815.5750.00125.0075.000.07-1.4812.02Prix4416001.0010.0076.6932.6680.4978.3728.1819.99120.00100.01-0.40-1.1510.33Prix5518001.004.0096.3735.55109.9996.3715.2045.00120.5075.50-0.60-1.8117.78Prix6620001.003.0091.3343.0290.0091.3360.7949.00135.0086.000.03-2.3324.84Prix7724001.007.0084.2832.0869.9984.2829.6549.99130.0080.010.32-1.9012.12Prix8832001.005.0082.0910.1979.9982.0913.3669.9995.5025.510.14-1.944.56Prix9940001.003.0056.5012.0355.5056.5015.5745.0069.0024.000.08-2.336.95Prix101050001.001.0089.9989.9989.990.0089.9989.990.00Comme vu précedemment, peut également représenter nos données avec un graphique afin d’obtenir un rendu plus clair et visible.\nFigure 8.4: Distribution des prix en fonction des DPI de la souris souris\nvoit que les prix ne semblent pas vraiment dépendre du DPI de la souris.","code":"\nggplot(data = mtcars, aes(x = 1, y = mpg)) + \n  # data = le jeu de données où on va trouver les variables d'intérêt, mpg en l'occurrence\n  # x = 1 : L'axe des x est constant, ce qui fait que tous les violons seront positionnés sur la même ligne\n  # y = mpg : L'axe des y représente la consommation (miles per gallon)\n  \n  geom_violin(fill = \"lightblue\", trim = FALSE) + \n  # fill = \"lightblue\" : Remplissage uniforme pour le violon avec la couleur 'lightblue'\n  \n  stat_summary(fun = mean, geom = 'point', color = \"red\") + \n  # Affiche la moyenne de la consommation comme un point rouge\n  \n  stat_summary(fun.data = 'mean_sdl', geom = \"errorbar\", color = \"blue\") + \n  # Affiche l'intervalle de confiance autour de la moyenne en bleu\n  \n  xlab(\" \") + \n  # Aucun titre pour l'axe des x, car il n'y a qu'une seule catégorie\n  \n  ylab(\"Consommation (mpg)\") + \n  # Titre de l'axe des y, indiquant la consommation en miles par gallon\n  \n  theme_light() + \n  # Thème léger pour le fond et le style du graphique\n  \n  theme(axis.title.x = element_blank(), \n        axis.text.x = element_blank(), \n        axis.ticks.x = element_blank())\n  # Supprime l'axe des x, car il n'est pas nécessaire ici\n  # la fonction ggplot permet de faire le graphique\n  # data = mtcars : On utilise le jeu de données mtcars\n  # aes(x = factor(cyl), y = mpg, fill = factor(cyl)) : \n  # - x = factor(cyl) : L'axe des x représente le nombre de cylindres (converti en facteur pour chaque groupe)\n  # - y = mpg : L'axe des y représente la consommation (miles per gallon)\n  # - fill = factor(cyl) : On colore les violons en fonction du nombre de cylindres\nggplot(data = mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) + \n  # geom_violin() : Crée un graphique en violon pour montrer la distribution des données\n  # trim = FALSE : On ne découpe pas les bords des violons, laissant toute la distribution visible\n  geom_violin(trim = FALSE) + \n  # stat_summary() : Ajoute une statistique résumée au graphique\n  # fun = mean : On veut afficher la moyenne de chaque groupe (cylindres) comme un point sur les violons.\n  # geom = 'point' : Le résumé est représenté par un point\n  stat_summary(fun = mean, geom = 'point') + \n  # stat_summary(fun.data = 'mean_sdl') : Affiche une statistique avec l'intervalle de confiance autour de la moyenne\n  # fun.data = 'mean_sdl' : Affiche la moyenne et l'écart-type comme une barre d'erreur autour du point moyen\n  stat_summary(fun.data = 'mean_sdl') + \n  # xlab(\"Cylindres\") : Ajoute un titre à l'axe des x, indiquant que l'on mesure les cylindres\n  xlab(\"Cylindres\") + \n  # ylab(\"Consommation\") : Ajoute un titre à l'axe des y, indiquant que l'on mesure la consommation de carburant par miles (mpg)\n  ylab(\"Consommation (mpg)\") + \n  # theme_light() : Applique un thème clair pour le fond et le style du graphique\n  theme_light() \nMickey<-readxl::read_xlsx(\"./statdesc/mickey2.xlsx\")\nMickeymouse <- psych::describe(Mickey$Prix, quant = c(.1, .25,  .75, .90))\n\nMickeymouse$variance <- Mickeymouse$sd^2\n\n\nMickeymouse_df <- as.data.frame(Mickeymouse)\ndesc_by_dpi <- psych::describeBy(Mickey[,c( \"Prix\")], \n                                 group = Mickey$DPI, mat=T)"},{"path":"les-statistiques-descriptives.html","id":"pourquoi-utiliser-la-loi-normale-pour-comprendre-les-graphiques-en-violon","chapter":"8 Les statistiques descriptives","heading":"8.7 Pourquoi utiliser la loi normale pour comprendre les graphiques en violon ?","text":"Les graphiques en violon sont souvent utilisés pour visualiser la distribution des données. Cependant, ces graphiques peuvent être difficiles à interpréter sans repères. C’est pourquoi nous utilisons la loi normale (ou distribution gaussienne) comme référence. La loi normale est l’une des distributions les plus courantes en statistiques et possède une forme bien définie et symétrique. En comparant la forme du graphique en violon à celle d’une loi normale, nous pouvons mieux comprendre comment les données se répartissent, identifier les éventuels écarts et repérer les asymétries ou anomalies dans la distribution.En d’autres termes, la courbe de densité de la loi normale nous aide à avoir un repère visuel pour interpréter les graphiques en violon.Dans le graphique en violon, cette densité est simplement représentée dans un axe à 90° par rapport à cette distribution.\nFigure 8.3: Représentation de la densité d’une distribution normale telle qu’peut la rencontrer dans un graphe violon.\nLes graphiques en violon sont souvent présentés avec une orientation horizontale, comme c’est le cas avec le graphique ci-dessus. Cela permet de mieux visualiser la densité des données sur un axe horizontal, ce qui correspond à l’orientation habituelle des graphiques en violon.","code":""},{"path":"les-statistiques-descriptives.html","id":"traiter-des-données-qualitatives","chapter":"8 Les statistiques descriptives","heading":"8.8 Traiter des données qualitatives","text":"Nous avons vu que la manière la plus efficace pour décrire les données qualitatives est de fournir la table des effectifs. Il s’agit du nombre d’observations pour chacune des modalités.Si la fonction summary permet de le faire, il semble plus pertinent d’utiliser la fonction table ou ftable en fonction du nombre de variables à prendre en considérationPour illustrer la manière dont fonctionnent ces fonctions, nous pouvons aussi utiliser un jeu de données intégré à R : le jeu de données titanic_train dans le package titanic (Hendricks, 2015).Voici les premières lignes du jeu de données :Tableau . Statistiques descriptives du jeu de données titanic PassengerIdSurvivedPclassNameSexAgeSibSpParchTicketFareCabinEmbarked103Braund, Mr. Owen Harrismale22.0010A/5 211717.25S211Cumings, Mrs. John Bradley (Florence Briggs Thayer)female38.0010PC 1759971.28C85C313Heikkinen, Miss. Lainafemale26.0000STON/O2. 31012827.92S411Futrelle, Mrs. Jacques Heath (Lily May Peel)female35.001011380353.10C123S503Allen, Mr. William Henrymale35.00003734508.05S603Moran, Mr. Jamesmale003308778.46QOn peut commencer par se demander combien de personnes étaient dans chacune des classes de passagers :voit qu’il y 216 personnes en première classe, 184 en deuxième et 491 personnes en troisième classe.Si se demande dans quelle mesure, le fait d’être sauvé dépend de la classe, nous pouvons croiser les deux variables et faire un tableau à doubles entrées.Nous nous rendons compte de deux phénomènes :l’absence de nom aux dimensions de la table la rend difficile à lire ;il est difficile d’apprécier la survieNous pouvons corriger ces deux défauts en utilisant l’argument dnn et la fonction addmargins. L’argument dnn permet de donner un nom aux dimensions. La première information est le nom de la variable pour les lignes et la seconde est le nom des variables pour les colonnes.Pour rendre les choses encore plus claires, connaitre la fréquence pour chaque modalité serait un plus. peut calculer les fréquences sur les effectifs totaux en divisant la table par la somme des effectifs :Dans certains cas, il est souhaitable d’obtenir ces informations par ligne ou par colonne. nouveau, R permet d’obtenir ces informations mais l’obtention de ces informations est techniquement un peu plus compliqué. Pour traduire la fonction ci-dessous : va retourner une nouvelle table grâce à la fonction sweep. Cette nouvelle table contiendra de nouvelles marges grâce à la fonction addmargins. va ajouter ces marges à l’objet “tab”, sur la première dimension de la table (donc ajoute des lignes) et dans ses marges, nous souhaitons les totaux de chaque colonne (en ajoutant des lignes), qui sera intitulé “N”. fera également, grâce à la fonction sum, une somme des différentes lignes (aboutissant donc à 100% dans chaque colonne). Enfin, pour les pourcentages, va utiliser la fonction apply sur la seconde dimension (le 2) et en divisant les totaux divisés par 100.Si cette fonction vous semble compliquée, vous pouvez simplement vous contenter de comprendre que vous devez donner le nom de la table, ‘tab’, dans mon exemple et garder le reste sans changement.Avec cette même logique, peut obtenir les pourcentages par ligne.Dans ce tableau, constate beaucoup plus aisément qu’avait plus de chance de survivre si avait un ticket en \\(1^e\\) qu’en \\(3^e\\) classe.Lorsqu’doit faire des tables plus complexes, impliquant plus de deux variables, peut utiliser la fonction ftable. L’utilisation de la fonction est assez simple : il suffit de donner à R un ensemble de variables qu’il peut considérer comme des facteurs. Le plus simple est d’utiliser un data.frame et de préciser le nom des variables à inclure.Ainsi, si veut regarder si le croisement entre le pont d’embarquement et le sexe eu un impact sur la survie en fonction de la classe, peut faire :Notez qu’il est possible de modifier l’apparence de la table en choisissant les variables qui doivent être présentées sur les lignes et celles qu’il faut présenter sur les colonnes en précisant les arguments row.vars et col.vars.ExerciceCet exercice vous permettra de mettre en pratique les outils de statistiques descriptives sur des données qualitatives et d’analyse de tableaux croisés en R. Vous explorerez les relations entre les variables qualitatives du jeu de données Titanic en manipulant des tableaux de fréquences avec la fonction table et en ajoutant des totaux grâce à addmargins. Vous calculerez également des pourcentages à l’aide des fonctions sweep et apply, avant d’utiliser ftable pour analyser les interactions entre plusieurs variables, notamment le sexe, l’âge catégorisé, et la survie , vous êtes libres de chosir les variables à analyser. Enfin, vous serez amené à interpréter les résultats obtenus pour tirer des conclusions sur les tendances observées.Questions :Comptez le nombre de passagers en fonction du sexe.Comptez le nombre de passagers en fonction du sexe.la survie est-elle la même entre les hommes et les femmes ?la survie est-elle la même entre les hommes et les femmes ?Ajoutez des totaux pour chaque catégorie dans le tableau croisé entre sexe et survie.Ajoutez des totaux pour chaque catégorie dans le tableau croisé entre sexe et survie.Transformez les comptages en pourcentages pour visualiser les proportions de survie.Transformez les comptages en pourcentages pour visualiser les proportions de survie.Créez des groupes d’âge (enfants, jeunes adultes, adultes, seniors) et analysez la survie au sein de chaque groupe.Créez des groupes d’âge (enfants, jeunes adultes, adultes, seniors) et analysez la survie au sein de chaque groupe.Pour débuter, il est indispensable d’importer la base de données titanic_train nécessaire à la réalisation de l’exercice. Il est probable que le jeu de données soit déjà installé sur votre session mais si toutefois ça n’est pas le cas , vous pouvez utiliser la fonction data pour l’importer directement dans votre environnement de travail.Si le lecteur désire plus d’informations concernant le jeu de données titanic_train , il peut utiliser la commande ?titanic_train pour avoir le détail des informations concernant cette dernière.Commençons par examiner combien de passagers étaient des hommes et combien étaient des femmes dans le jeu de données. Pour ça utilise la fonction table sur la variable Sex.voit que parmis les passagers, il y 314 femmes et 577 hommes.Ensuite, analysons comment la survie varie selon le sexe. Nous croisons les variables Sex et Survived à l’aide d’un tableau à doubles entrées pour savoir si la survie est similaire entre les sexes.Le tableau croisé montre le nombre de survivants (1) et de non-survivants (0) selon le sexe. Pour les femmes, 233 ont survécu et 81 n’ont pas survécu. Pour les hommes, 109 ont survécu et 468 n’ont pas survécu. Il semble donc, priori, qu’il y ait plus de femmes à avoir survécu que d’hommes.Néanmoins, le tableau précédent n’est pas très lisible. Donnons des noms aux dimensions (colonnes et lignes) et ajoutons des marges pour avoir les totaux. Cela nous permettra d’interpréter plus facilement les résultats.Pour mieux comprendre les proportions, transformons le tableau précédent en fréquences en divisant chaque cellule par le total des effectifs.Maintenant, voyons comment les pourcentages de survie varient par ligne (par sexe). Cette étape est utile pour analyser les chances de survie selon chaque groupe.Nous allons maintenant créer une nouvelle variable, AgeGroup, pour catégoriser l’âge en groupes : enfants, jeunes adultes, adultes et seniors. Une fois la variable créée, examinons si l’âge eu un impact sur la survie des passagers.\nLa fonction cut permet de diviser une variable continue en intervalles définis. Dans cet exemple, elle est utilisée pour créer une nouvelle variable AgeGroup en catégorisant les âges des passagers en quatre groupes : “Enfant” pour les passagers de 0 à 18 ans, “Jeune adulte” de 18 à 35 ans, “Adulte” de 35 à 50 ans, et “Senior” de 50 à 100 ans. Les intervalles sont définis avec breaks et les catégories sont nommées avec labels. Cette fonction transforme ainsi une variable numérique en une variable catégorielle et vous pouvez l’utiliser dans vos lignes de code.Enfin, peut combiner les variables AgeGroup, Sex et Survived pour examiner leur interaction. Pour ça peut utiliser la fonction ftable pour obtenir un tableau lisible et multidimensionnel.Le tableau montre que les femmes, surtout les enfants et les seniors, avaient des chances de survie nettement plus élevées que les hommes. Les enfants, indépendamment du sexe, avaient de meilleures chances, confirmant les priorités “femmes et enfants d’abord”. En revanche, les hommes jeunes et adultes ont eu les taux de survie les plus faibles, reflétant une vulnérabilité accrue dans ces groupes.","code":"\nlibrary(titanic)\ndata(titanic_train)\ntable(titanic_train$Pclass)## \n##   1   2   3 \n## 216 184 491\ntable(titanic_train$Pclass, titanic_train$Survived)##    \n##       0   1\n##   1  80 136\n##   2  97  87\n##   3 372 119\ntab<-table(titanic_train$Pclass, titanic_train$Survived, dnn=list(\"classe\", \"Survie\"))\ntab<-addmargins(tab)\ntab##       Survie\n## classe   0   1 Sum\n##    1    80 136 216\n##    2    97  87 184\n##    3   372 119 491\n##    Sum 549 342 891\ntab<-round(tab/sum(tab), 3) \ntab##       Survie\n## classe     0     1   Sum\n##    1   0.022 0.038 0.061\n##    2   0.027 0.024 0.052\n##    3   0.104 0.033 0.138\n##    Sum 0.154 0.096 0.250\nround(\n  sweep(\n    addmargins(tab, 1, list(list(All = sum, \n                                 N = function(x) sum(x)^2/100))), 2,apply(tab, 2, sum)/100, \"/\"),\n    1)##       Survie\n## classe     0     1   Sum\n##    1     7.2  19.9  12.2\n##    2     8.8  12.6  10.4\n##    3    33.9  17.3  27.5\n##    Sum  50.2  50.3  49.9\n##    All 100.0 100.0 100.0\n##    N     0.3   0.2   0.5\nround(\n   sweep(\n     addmargins(\n       tab, 2, list(list(All = sum, N = \n                           function(x) sum(x)^2/100))), 1,apply(tab, 1, sum)/100, \"/\"),\n       1)##       Survie\n## classe     0     1   Sum   All     N\n##    1    18.2  31.4  50.4 100.0   0.1\n##    2    26.2  23.3  50.5 100.0   0.1\n##    3    37.8  12.0  50.2 100.0   0.3\n##    Sum  30.8  19.2  50.0 100.0   0.5\nftable(titanic_train[,c(\"Survived\", \"Pclass\", \"Sex\",\"Embarked\")])##                        Embarked       C   Q   S\n## Survived Pclass Sex                            \n## 0        1      female            0   1   0   2\n##                 male              0  25   1  51\n##          2      female            0   0   0   6\n##                 male              0   8   1  82\n##          3      female            0   8   9  55\n##                 male              0  33  36 231\n## 1        1      female            2  42   1  46\n##                 male              0  17   0  28\n##          2      female            0   7   2  61\n##                 male              0   2   0  15\n##          3      female            0  15  24  33\n##                 male              0  10   3  34\ntab<-ftable(titanic_train[,c(\"Survived\", \"Pclass\", \"Sex\",\"Embarked\")], \n       row.vars =c(\"Pclass\", \"Embarked\") , \n       col.vars =c(\"Survived\", \"Sex\"))\ntab##                 Survived      0           1     \n##                 Sex      female male female male\n## Pclass Embarked                                 \n## 1                             0    0      2    0\n##        C                      1   25     42   17\n##        Q                      0    1      1    0\n##        S                      2   51     46   28\n## 2                             0    0      0    0\n##        C                      0    8      7    2\n##        Q                      0    1      2    0\n##        S                      6   82     61   15\n## 3                             0    0      0    0\n##        C                      8   33     15   10\n##        Q                      9   36     24    3\n##        S                     55  231     33   34\ndata(\"titanic_train\")\ntable(titanic_train$Sex)## \n## female   male \n##    314    577\ntable(titanic_train$Sex, titanic_train$Survived)##         \n##            0   1\n##   female  81 233\n##   male   468 109\ntab <- table(titanic_train$Sex, titanic_train$Survived, dnn = list(\"Sexe\", \"Survie\"))\ntab <- addmargins(tab)\ntab##         Survie\n## Sexe       0   1 Sum\n##   female  81 233 314\n##   male   468 109 577\n##   Sum    549 342 891\ntab <- round(tab / sum(tab), 3)\ntab##         Survie\n## Sexe         0     1   Sum\n##   female 0.023 0.065 0.088\n##   male   0.131 0.031 0.162\n##   Sum    0.154 0.096 0.250\nround(sweep(addmargins(tab, 2, list(list(All = sum, N = function(x) sum(x)^2 / 100))), \n            1, apply(tab, 1, sum) / 100, \"/\"), 1)##         Survie\n## Sexe         0     1   Sum   All     N\n##   female  13.1  36.9  50.0 100.0   0.2\n##   male    40.4   9.6  50.0 100.0   0.3\n##   Sum     30.8  19.2  50.0 100.0   0.5\ntitanic_train$AgeGroup <- cut(titanic_train$Age, breaks = c(0, 18, 35, 50, 100), \n                              labels = c(\"Enfant\", \"Jeune adulte\", \"Adulte\", \"Senior\"))\ntable(titanic_train$AgeGroup, titanic_train$Survived)##               \n##                  0   1\n##   Enfant        69  70\n##   Jeune adulte 221 137\n##   Adulte        92  61\n##   Senior        42  22\nftable(titanic_train[, c(\"Survived\", \"AgeGroup\", \"Sex\")])##                       Sex female male\n## Survived AgeGroup                    \n## 0        Enfant               22   47\n##          Jeune adulte         26  195\n##          Adulte               15   77\n##          Senior                1   41\n## 1        Enfant               46   24\n##          Jeune adulte         94   43\n##          Adulte               41   20\n##          Senior               16    6"},{"path":"les-statistiques-descriptives.html","id":"statistiques-descriptives-avec-easier","chapter":"8 Les statistiques descriptives","heading":"8.9 Statistiques descriptives avec easieR","text":"Pour réaliser des statistiques descriptives avec easieR (Stefaniak, 2018), il faut avoir au préalable avoir chargé le package easieR et lancé easieR avec la fonction easieR().La seconde étape consiste à importer les données.","code":""},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"les-différentes-méthodes-dinférences-statistiques","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9 Les différentes méthodes d’inférences statistiques","text":"“Le statisticien ne peut pas se soustraire à l’obligation d’être au clair quant aux principes de l’inférence scientifique, mais de même, aucune autre personne censée ne peut se soustraire à une telle obligation.”\nRésumé \nDans ce chapitre nous aborderons les trois principales écoles de pensées pour l’inférence statistique :l’approche de Neyman-Pearsonl’approche de vraisemblancel’approche bayésienneNous aborderons les tenants et les aboutissants de ces différents méthodes, leurs avantages et leurs inconvénients.\nL’objectif est de permettre au lecteur d’avoir les notions indispensables pour lire, comprendre ou réaliser des analsyes statistiques selon n’importe laquelle des écoles d’inférence, et de pouvoir développer un regard critique sur chacune de ces écoles.\nPrérequis \nD’un point de vue théoriqueSavoir ce qu’est une distribution normale.Savoir ce qu’est une moyenne, un écart-type et une erreur-typeSavoir ce qu’est le score z","code":""},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"introduction-2","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9.1 Introduction","text":"Les statistiques permettent de tester des hypothèses, ces hypothèses peuvent être de nature extrêmement différentes :De combien de béton ai-je besoin pour éviter qu’un pont d’autoroute ne s’écroule avec un risque d’erreur de 1/100 000 ?De combien de béton ai-je besoin pour éviter qu’un pont d’autoroute ne s’écroule avec un risque d’erreur de 1/100 000 ?Est-ce que ce traitement est efficace ?Est-ce que ce traitement est efficace ?Est-ce que les oiseaux migratoires prennent toujours la même route ?Est-ce que les oiseaux migratoires prennent toujours la même route ?Est-ce que des enfants apprennent mieux avec la méthode ou avec la méthode B ?Est-ce que des enfants apprennent mieux avec la méthode ou avec la méthode B ?Quelle date de péremption faut-il mettre sur les emballages pour s’assurer que le produit ne sera pas périmé dans 99.9% des situations ?Quelle date de péremption faut-il mettre sur les emballages pour s’assurer que le produit ne sera pas périmé dans 99.9% des situations ?Quelle est la méthode la plus efficace pour faire comprendre les statistiques aux étudiants ?Quelle est la méthode la plus efficace pour faire comprendre les statistiques aux étudiants ?Toutes ces questions amènent des hypothèses. Par exemple, mes calculs permettent de prédire qu’il faut X tonnes de béton pour s’assurer que le pont ne fissure pas dans plus d’un 1 car sur 100 000.peut raisonnablement se demander comment il est possible de répondre de manière fiable à ces questions. Il existe au minimum trois approches différentes pour apporter des éléments de réponses : l’approche de Neyman-Pearson (1933), l’approche bayésienne (Bayes & Price, 1763; LaPlace, 1814) et l’approche par vraisemblance (Royall, 1997).Dans ce chapitre, nous allons considérer que l’approche de Neyman-Pearson (1933) est la plus indispensable pour débuter avec l’inférence statistique car c’est l’approche la plus communément rencontrée dans la littérature scientifique. Il est donc nécessaire de pouvoir apporter les bases théoriques nécessaires à la lecture des statistiques selon cette approche. Ainsi, dans ce chapitre, vous pourrez être dans une logique de découverte de l’inférence statistique et vous limitez à lire et comprendre l’approche de Neyman-Pearson (1933), être déjà au fait avec cette approche et découvrir une autre école d’inférence statistique ou enfin vouloir découvrir naïvement les différentes écoles de pensées et vous faire votre propre opinion sur celle qui vous correspond le mieux. Dans le premier et second cas de figure, vous pourrez concentrer votre lecture sur l’école de pensées qui vous intéresse et dans le dernier cas de figure, vous ne pourrez pas faire l’économie d’une lecture complète de ce chapitre.Si, pour toutes les approches, nous essaierons d’aborder les choses de manière assez intuitive dans un premier temps, ce chapitre est tellement important que ce serait une erreur d’aborder les choses que de manière superficielle. Ainsi, après cette première phase plus intuitive, nous aborderons progressivement les choses de manière plus technique. S’assurer d’une bonne compréhension de la méthode d’inférence que vous voudrez utiliser ou analyser est non seulement indispensable pour utiliser de manière adaptée les outils mais facilitera la compréhension des autres chapitres.","code":""},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"lapproche-de-neyman-pearson","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9.2 L’approche de Neyman-Pearson","text":"","code":""},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"une-approche-intuitive","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9.2.1 Une approche intuitive","text":"Imaginons qu’un chercheur développe un médicament qui rend les gens intelligents. Pour montrer l’efficacité de son traitement, il faut passer un test d’intelligence à un groupe de personnes, il donne son traitement miraculeux à ce groupe et au bout de quelques semaines de traitement, il évalue à nouveau le niveau intellectuel de ces personnes (avec un autre test pour éviter les effets d’apprentissage mais qui est tout aussi bien standardisé). Il observe que la moyenne au temps 1 est de 100 et que la moyenne au temps 2 est 100,1. Après son expérimentation, ce chercheur présente les résultats de son étude en affirmant haut et fort que son traitement rend plus intelligent. Probablement, que face à cette affirmation, vous pourriez être amusé·e, mais probablement pas convaincu·e. La question qui se pose est de savoir à partir de quel moment vous seriez convaincu·e. la réponse à cette question est simple : à partir du moment où la différence observée fait sens. D’un point de vue statistique, c’est ce qu’appelle la significativité10.Pour comprendre intuitivement la notion de significativité, demandez-vous si l’intelligence d’Albert Einstein est une intelligence exceptionnelle, ou une intelligence typique de la population humaine.Si vous aboutissez à la conclusion qu’Albert Einstein une intelligence exceptionnelle, cela signifie que vous considérez que son intelligence est significativement différente de celle de la population générale.La raison pour laquelle vous arrivez à la conclusion qu’Einstein est quelqu’un de particulièrement intelligent est que, de manière générale, voit peu de personnes ayant réalisé autant d’accomplissement scientifiques et que, pour atteindre un tel niveau d’accomplissement, il est nécessaire que cette personne ait une intelligence très développée. L’idée sous-tendant les tests statistiques est d’estimer la probabilité de croiser une personnes aussi intelligente ou plus intelligente qu’Albert Einstein en considérant que ce niveau intellectuel ne se distingue pas de celui de la population générale.Ainsi, en considérant dans un premier temps qu’Albert Einstein une intelligence qu’peut observer de manière générale dans la population, considère qu’Albert Einstein ne se différencie pas de la population générale. D’un point de vue statistique, cette absence de différence est ce qu’appelle une hypothèse nulle, souvent dénoté \\(H_0\\).Pour tester cette hypothèse, peut utiliser un tester évaluant le quotient intellectuel (QI).\nLa moyenne pour une échelle de QI est de 100 et l’écart-type est de 15. considère que le niveau d’intelligence se distribue en suivant une distribution normale. Sachant qu’Einstein avait un QI estimé à 162, peut calculer la probabilité d’obtenir un QI aussi élevé ou plus élevé que le sien. Nous pouvons obtenir cette probabilité à l’aide du score z, qui est obtenu par la formule (9.1) :\\[\\begin{equation}\nz=\\frac{x_i-\\bar{x}}{s}\n\\tag{9.1}\n\\end{equation}\\]Dans R, c’est la fonction pnorm qui permet de réaliser le score Z. Cette fonction 4 arguments :q : le score \\(x_i\\) de l’individuq : le score \\(x_i\\) de l’individumean : la moyenne du groupe de référencemean : la moyenne du groupe de référencesd : l’écart-type du groupe de référencesd : l’écart-type du groupe de référencelower.tail : permet de choisir le côté de la courbe (lower.tail = TRUE signifie qu’calcule la probabilité pour les scores inférieurs ou égal au \\(x_i\\) et lower.tail = FALSE pour les scrores supérieurs ou égal au \\(x_i\\))lower.tail : permet de choisir le côté de la courbe (lower.tail = TRUE signifie qu’calcule la probabilité pour les scores inférieurs ou égal au \\(x_i\\) et lower.tail = FALSE pour les scrores supérieurs ou égal au \\(x_i\\))Avec une probabilité de \\(1.78 \\times 10^{-5}\\), trouve moins de deux personnes sur 100 000 ayant une intelligence aussi élevée ou plus élevée que celle d’Albert Einstein. Ainsi, la probabilité de croiser quelqu’un d’aussi intelligent qu’Albert Einstein est très faible, c’est pour cela que nous avons considéré intuitivement qu’il avait un QI qui se distinguait significativement de la plupart des gens.\nIl faut donc comprendre qu’considère qu’il y une différence significative lorsque la probabilité d’observer un phénomène au regard de la distribution sur laquelle s’appuie est faible.Pour le formuler autrement, la probabilité associée à un test statistique dans le cadre des tests d’hypothèse nulle est la probabilité d’observer une telle différence (avoir un QI de 162 signifie qu’il y une différence de 62 points par rapport à la moyenne) si cette différence est vraie (le QI d’Einstein aurait pu être un QI égal à 100).Ainsi, si la probabilité d’observer cette différence est faible, alors, va considérer cette différence comme étant significative car il y peu de chance que cette différence soit observé uniquement en raison du fruit du hasard.\nHabituellement, considère que la probabilité est faible si la probabilité associée au test statistique est inférieure à 0.05. C’est ce qu’appelle le seuil de significativité.Ainsi, dans le Figure 9.1, la barre verticale représente le seuil de significativité à 5% lorsqu’considère que seul un côté de la courbe est digne d’intérêt. parle dans ce cas d’hypothèse unilatérale.\nEn d’autres termes, dans une hypothèse unilatérale, considère que seul les 5% supérieurs de notre distribution sont intéressants pour tester notre hypothèse. Le seuil de significativité sera placé sur l’extrémité d’intérêt de la distribution. Pour reprendre l’exemple sur le QI. Au-delà d’Albert Einstein, va considérer que quelqu’un un niveau intellectuel qui se distingue significativement si son QI est supérieur à 124.6728, indiqué par la barre verticale l’indique dans la Figure 9.1.\nFigure 9.1: Seuil de significativité pour une hypothèse unilatérale\nUne autre manière de répartir les 5% est d’estimer que le QI d’une personne se distingue significativement dès lors qu’elle appartient au 5% d’une des deux extrémités de la courbe.\nDans ce cas, il faut répartir les 5% sur les deux extrémités, ce qui signifie que le seuil de significativité est atteint lorsqu’une personne un QI inférieur ou égal aux 2.5% à l’extrémité gauche de la courbe (.e., les QI les plus faibles) ou aux 2.5% à l’extrémité droite de la courbe (.e., les QI les plus élevés), voir Figure ??.\nFigure 9.2: Seuil de significativité pour une hypothèse bilatérale\nDans le premier cas de figure, parle d’hypothèse unilatérale et dans le second cas de figure, parle d’hypothèse bilatérale. Il est raisonnable d’utiliser le premier cas de figure uniquement lorsque les éléments théoriques sur lesquels s’appuie pour tester l’hypothèse sont extrêmement robustes, et même dans ce cas, cela est, la plupart du temps, considéré comme insuffisant pour justifier une hypothèse unilatérale.Ainsi, si une personne un QI de 135, va rejeter l’hypothèse nulle et considérer qu’il y une différence significative. Quand conclut à la présence d’une différence significative, c’est que les données permettent d’étayer une autre hypothèse que l’hypothèse nulle : cette autre hypothèse est l’hypothèse d’une différence. l’appelle l’hypothèse alternative. Tous les tests statistiques sont construits sur le même principe : ils opposent une hypothèse nulle à une hypothèse alternative. De manière systématique, l’hypothèse nulle doit être formalisée sous la forme d’une absence de différence et l’hypothèse alternative en termes de la présence d’une différence. Bien que cela semble simple lorsqu’présente les choses ainsi, il n’est pas si simple de pouvoir générer ces deux hypothèses si ne comprend pas à quoi sert le test, ce qui peut entraîner des erreurs d’interprétation. Cependant, si vous entraînez à formuler les hypothèses nulles et alternatives correctement, vous pourrez facilement transposer à de nouvelles situations que vous n’aurez jamais rencontrées en appliquant le même principe.Dans le cas d’Einstein, il n’était pas nécessaire de tester statistiquement l’hypothèse pour connaître la réponse car savait qu’il s’agit d’une personne particulièrement intelligente. Le problème est que, dans la réalité, ne connait pas la réponse à la question que l’se pose avant d’avoir mis à l’épreuve cette question. Ainsi, si estime le QI d’une personne, c’est par exemple, pour essayer de comprendre pourquoi cette personne est en échec scolaire. Ainsi, pourrait faire l’hypothèse que son niveau intellectuel est faible, mais pourrait également faire l’hypothèse que, en raison de son niveau intellectuel élevé, elle s’ennuie à l’école, elle se désintéresse de ses cours, ce qui la fait échouer.Dans la section suivante, nous allons décrire comment la significativité est déterminée et ce qu’elle signifie à partir dun exemple de travail plutôt simple.","code":"\npnorm(q=162, mean = 100, sd = 15, lower.tail = FALSE)## [1] 1.787698e-05\n x<-rnorm(10000, 100,15)\n r <- c(40,160)\nhist(x,freq=FALSE,col=0,ylim=c(0,0.05),20, xlab=NULL, ylab=\"Densité\", main=NULL)\n plot(function(x) dnorm(x,100,15),xlim=r,col=3,add=TRUE)\n cutoff<-qnorm(0.95, 100, 15)\n abline(v=cutoff)\n text(y=0.005, x=135 ,labels =\"5%\")\n cutoff## [1] 124.6728\n cutoff1<-qnorm(0.025, 100, 15)\n cutoff2<-qnorm(0.975, 100, 15)\n cutoff1## [1] 70.60054\n cutoff2## [1] 129.3995\n x<-rnorm(100000, 100,15)\n r <- c(40,160)\nhist(x,freq=FALSE,col=0,ylim=c(0,0.05),20, xlab=NULL, ylab=\"Densité\", main=NULL)\n plot(function(x) dnorm(x,100,15),xlim=r,col=3,add=TRUE)\n abline(v=cutoff1)\n  abline(v=cutoff2)\n text(y=0.005, x=145 ,labels =\"2.5%\")\n text(y=0.005, x=50 ,labels =\"2.5%\")"},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"quid-quand-cest-moins-évident","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9.2.2 Quid quand c’est moins évident?","text":"Une jeune femme fait l’hypothèse qu’il est possible de rencontrer l’homme de sa vie en sortant dans un bar. Elle n’est pas tout à fait naïve est sait très bien que la plupart des hommes ne seront pas son prince charmant. Pour accorder une chance au jeune homme qui viendra la trouver, elle va donc opérationnaliser son hypothèse en considérant que l’homme de sa vie sera en mesure de l’intéresser pendant au moins 90 minutes. Elle lui donnera donc son numéro qu’à cette condition.Pour formaliser les choses, les hommes géniaux doivent intéresser notre jeune femme pendant 5400 secondes. Néanmoins, dans la catégorie des hommes géniaux, il y en qui sont moins bavards, il y en qui sont plus timides. Donc, les hommes géniaux vont en réalité se distribuer autour de la moyenne de 5400 secondes avec un écart-type de 1000 seconde en suivant la distribution présentée en Figure 3.Le problème est que les jeunes hommes qui ne méritent pas l’intérêt de notre jeune femme n’ont pas écrit sur leur front qu’ils ne sont pas dignes d’intérêt.\nBien sûr, certains vont être éliminés très rapidement (“hey mademoiselle, t’un 06 ?”), mais pour certains, les choses sont moins clairs. peut raisonnablement penser qu’une personne qui n’est pas un prince charmant est tout de même en mesure d’intéresser notre jeune personne pendant une cinquantaine de minutes, c’est-à-dire 3000 secondes. La distribution des hommes qui sont moins dignes d’intérêt est représentée en Figure 4.Les choses se compliquent quand il y une incertitude sur l’issue du test (quand rencontre une personne, n’pas la certitude absolue qu’il s’agit de la personne de notre vie). La raison est due au fait que la zone de délimitation est floue et que la ditribution des hommes géniaux se superpose à la distribution des hommes ayant moins d’intérêt, comme vous pouvez le constater dans la Figure 5.Dans cet exemple, les hommes qui n’arriveront pas à intéresser notre jeune femme pendant au moins \\(4960\\) secondes seront considérés comme banal alors que ceux qui arriveront à intéresser la jeune femme plus longtemps que ce seuil seront considérés comme géniaux.Dans cet exemple, une partie des hommes qui appartiennent au groupe des hommes banals arrivent à passer le seuil à partir duquel les conidère comme géniaux. Vous les connaissez, ce sont les beaux parleurs : vous croyez en leurs doux mots, vous leur proposez votre numéro … et ils ne vous rappellent pas.l’inverse, une part importante des hommes qui appartiennent au groupe des princes charmants se voient affublés de l’étiquette “banal” car ils sont plus timides, moins diserts…En d’autres termes, dans avec un test statistique, il y deux moyens d’avoir raison et deux moyens de se tromper.\nLorsque vous considérez un homme banal comme banal, cela signifie que vous tolérez à raison l’hypothèse nulle selon laquelle il n’est pas significativement différent. Dans ce cas, parle d’intervalle de confiance. Il est généralement à 95%. Cet intervalle de confiance signifie que, si connait les paramètres d’une population, peut estimer un intervalle à l’intérieur duquel 95% des estimations de ce paramètre se situeront.Le complément de l’intervalle de confiance est l’erreur de 1e espèce, appelée \\(\\alpha\\). Elle est habituellement de 5%, et dans notre exemple, ils sont représentés par les beaux parleurs.Lorsqu’considère un homme comme appartenant à la catégorie des princes charmants et qu’il se trouve qu’il est effectivement un prince charmant, il s’agit de la seconde manière d’avoir raison. Cette condition consiste à rejeter à raison l’hypothèse nulle. parle dans ce cas de puissance statistique.Le complèment de la puissance statistique est l’erreur de seconde espèce, appelée \\(\\beta\\).Ces deux manières d’avoir raison et ces deux manières d’avoir tort sont résumées dans le Tableau ??.1 = IC + alpha1 = bêta + puissanceDans notre exemple, les valeurs pour chacun des paramètres sont :Dans cet exemple, se posait la question à un niveau individuel. Cependant, la plupart du temps, se pose la questiion au niveau du groupe (.e., est-ce que tel groupe se distingue de tel autre groupe ?).Pour comprendre la manière dont cela fonctionne au niveau du groupe, il faut comprendre deux notions essentielles en statistiques : la notion d’échantillonnage et le théorème central limit.La manière la plus simple de comprendre ces deux notions consiste à en fait la démonstration par un exemple.va commencer par créer une population artificielle dont la distribution est plate. obtient ce type de distribution en faisant par exemple une séquence de 1 à 10 000.peut obtenir la distribution de cette population à l’aide la fonction hist.Deux constats doivent être dressés :\n1) la distribution est plate ;\n2) les valeurs vont de 1 à 10 000.La moyenne de cette distribution vaut \\(5000.5\\) et l’écart-type vaut \\(2886.8956799\\). Si prend un échantillon de 50 personnes au hasard dans cette population grâce à la fonction sample, va se rendre compte que la moyenne de cet échantillon ne sera pas exactement égal à la moyenne de la population.Si répète un grand nombre de fois cette opération, va obtenir systématiquement une moyenne différente, quelque fois très proche de la moyenne de la population, d’autres fois plus éloignée.\nLa manière dont les moyennes d’échantillon se distribuent autour de la vraie moyenne (.e., la paramètre) est ce qu’appelle la distribution d’échantillonnage\nL’appli sur le lien suivant permet de voir comment la distribution des moyennes d’échantillons :\nhttps://ihstevenson.shinyapps.io/sample_means/Dans notre cas, nous allons faire une boucle grâce à la fonction qui va permettre d’obtenir 100 000 moyennes d’échantillons que nous allons stocker dans un vecteur, appelé M_ech.Nous pouvon faire une présentation graphique de la manière dont les moyennes d’échantillons se répartissent autour de la moyenne de la population grâce à la fonction hist.présent, 3 constats doivent être faits :La distribution n’est plus plate ;La distribution n’est plus plate ;La moyenne des moyennes vaut \\(5002\\) ;La moyenne des moyennes vaut \\(5002\\) ;L’écart-type de la distribution des moyennes d’échantillon vaut \\(411\\).L’écart-type de la distribution des moyennes d’échantillon vaut \\(411\\).Pour le premier point, ce phénomène s’explique par le théorème centrale limite, selon lequel, indépendamment de la distribution initiale des données, si l’échantillon est suffisamment grand (la notion de suffisamment grand étant relative), la distribution des moyennes d’échantillon suivra une distribution normale.\nCe théorème explique un argument qu’observe régulièrement concernant le non respect de la normalité de la distribution : ce n’est pas grave parce que, comme mon échantillon est suffisamment grand, la distribution d’échantillonnage suit une distribution normale grâce au théorème central limit.\nCet argument n’est qu’à moitié correct : il ne tiendrait pas si une distribution fortement asymétrique avait été utilisée au lieu d’une distribution plate.Pour le second point, cela illustre que le fait de répéter un grand nombre de fois une expérimentation permet d’avoir une estimation relativement précise du paramètre de la population.Enfin, le 3e point amène à comprendre la notion d’erreur type. L’erreur-type, standard error (se) en anglais, est l’écart-type divisé par la racine carrée de la taille de l’échantillon.\\[se = \\frac{s}{\\sqrt{n}}\\]\nEt nous constatons que l’erreur-type simulé est assez proche de l’erreur-type estimée de manière théorique.Ce principe est donc à la base d’un test comme le t de Student comparaison à une norme. En effet, la formule du t de Student comparaison à une norme est la suivante :\\[t=\\frac{\\bar{x}-\\mu}{ \\frac{s}{\\sqrt{n}}}\\]\noù \\(\\bar{x}\\) correspond à la moyenne de l’échantillon, \\(\\mu\\) correspond à la moyenne de la population, s correspond à l’écart-type de l’échantillon et n correspond à la taille de l’échantillon.\nSi reprend la formule du z, identifie que c’est exactement la même formule avec un échantillon d’une personne : la moyenne d’une observation est cette observation, et faire la racine de 1 vaut 1.\\[z=\\frac{x_i-\\mu}{\\frac{s}{\\sqrt{1}}}=\\frac{x_i-\\mu}{s}\\]Ainsi, quand réalise un test statistique, un probabilité va être associée à la valeur de la statistique. Si cette probabilité vaut 0.70, cela signifie que, sur la base d’un tirage aléatoire dans une population initiale, 70% des échantillons vont présenter une différence égale ou plus grande que la différence observée. Comme la probabilité est en l’occurrence élevée, il est tout à fait possible que la différence observée soit due à l’erreur (en termes d’estimation) d’échantillonnage. ne peut donc pas rejeter l’hypothèse nulle avec un risque de se tromper suffisamment faible.Si vous avez suivi le raisonnement, vous devriez avoir compris que 5% des moyennes d’échantillons (.e., 500) se répartiront de manière équitable au-dessous du percentile 2.5 et au-dessus du percentile 97.5 évalués estimé sur base d’une distribution normale.La fonction qnorm permet de tester cela.constate qu’il y effectivement environ 500 échantillons sur 10 000 qui ont dépassé le seuil de significativité, ce qui représente le pourcentage d’erreur de première espèce que nous admettrons.Donc, bien que ces 5% d’échantillons qui se trouvent aux deux extrêmes appartiennent à la population des que nous avons créée, va considérer (à tort) qu’ils sont significativement différents de la population créée.","code":"\ngenial <- rnorm(3000, 5400, 1000)\nhist(genial, main =\"Figure 3. Distribution des hommes géniaux, dignes d'intérêt\")\nbanal<- rnorm(3000, 3000, 1000)\nhist(banal, main=\"Figure 4. Distribution des hommes peu dignes d'intérêt\")\n## Histogrammes non-imprimés :\nbp1 <- hist(banal, plot=FALSE, nclass=20)  # 20 classes...\nbp2 <- hist(genial, plot=FALSE, nclass=20)  #\n\n## Calcul de minima/maxima :\nhlims <- range(c(bp1$breaks, bp2$breaks))\nvlims <- range(c(bp1$counts, bp2$counts))\n\n## Couleurs de remplissage, dont une avec ajout de transparence :\nhistcol <- sapply(apply(rbind(col2rgb(c(\"orangered2\", \"lightblue\"))/255,\n                              alpha = c(1, 0.5)), # Première couleur opaque, 50% de transparence pour la seconde.\n                              2, as.list),\n                             do.call, what = rgb)\n\n## Création du graphique :\nhist(banal, border=\"darkred\", xlim=hlims, ylim=vlims,\n     nclass=20,                         # 20 classes...\n     col=histcol[1], main=\"Figure 5. Fréquence en fonction du type d'hommes\")\n\nhist(genial, border=\"darkblue\", add=TRUE,\n     breaks=seq(from=min(hlims), to=max(hlims), by=diff(bp1$breaks[1:2])), # largeur de classes égale à celle du précédent graphique\n     col=histcol[2])      # Couleur transparente (en sur-impression).\nseuil<-qnorm(0.975, 3000,1000)\nabline(v=seuil,lwd=3)\ntext(x=3000, y=200, \"banal\")\ntext(x=6000, y=200, \"génial\")## \n## Attachement du package : 'kableExtra'## Les objets suivants sont masqués depuis 'package:flextable':\n## \n##     as_image, footnote## L'objet suivant est masqué depuis 'package:dplyr':\n## \n##     group_rows\npop<-1:10000\nhist(pop, main = \"Figure 6. Représentation de la population créée\")\nsample(pop,50, replace=F)->M_ech\nM_ech##  [1] 3594  334 7295 7434 7762 1804 4887 3000 8094  426 4731 5945 9980 9351 6801\n## [16] 3719 4698 9033 9953 8559 2007 4169 7474 2055 1514 9301 5394 7220 2793 6872\n## [31]  902 9422 5686 8958 8043 4125 1089  507 9244 7090 6206 5067 1040 3199 1304\n## [46] 9603 5950 2568 8578 3577\nmean(M_ech)## [1] 5367.14\nfor(i in 1:99999){\n  mean(sample(pop,50, replace=F))->M_ech2\n  c(M_ech, M_ech2)->M_ech\n  }\nhist(M_ech, main=\"Figure 7.Représentation graphique de la distribution d'échantillonnage\", xlab=\"distribution des moyennes d'échantillon\", \n     xlim=range(3500,6500))\nmean(M_ech)## [1] 5002.093\nsd(M_ech)## [1] 411.2281\nsd(pop)/50^0.5## [1] 408.2687\nlower<-mean(pop)+qnorm(0.025)*sd(pop)/50^0.5 # fixe la limite basse\nupper<-mean(pop)+qnorm(0.975)*sd(pop)/50^0.5 # fixe la limite haute\nn.low<-length(which(M_ech<lower)) # nombre en dessous de la limite inférieure\nn.high<-length(which(M_ech>upper)) # nombre en dessous de la limite supérieure\nn.low## [1] 2350\nn.high## [1] 2495\nn.low+n.high## [1] 4845"},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"lien-entre-la-taille-deffet-et-la-significativité","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9.3 Lien entre la taille d’effet et la significativité","text":"Dans l’exemple que nous avons utilisé, les hommes géniaux étaient intéressants pendant 5400 secondes alors que les hommes banals étaient intéressants pendant 3000 secondes. Comme l’écart-type était de 1000, cela signifie que la moyenne des hommes banals et celle des hommes géniaux sont à 2.4 écart-types l’une de l’autre. En effet, l’obtient de la manière suivante :\\[d_{Cohen}=\\frac{5400-3000}{1000}=2.4\\]Cette valeur de 2.4 est ce qu’appelle le d de Cohen. Il correspond à la taille d’effet pour une comparaison de deux moyennes. Pour chaque test statistique (ou pratiquement), il existe une taille d’effet qui lui est propre.\nPour Cohen (1988), une taille d’effet de 0.2 est une petit taille, une taille d’effet de 0.5 est une taille d’effet moyenne et une taille d’effet de 0.8 est une grande taille d’effet. Il faut tout de même avoir conscience que ces limites sont arbitraires et qu’elles dépendant grandement du domaine scientifique dans lequel travaille : les tailles d’effet sont généralement plus faibles en psychologie qu’en biologie. Néanmoins, comprendre qu’avec une taille d’effet de 2.4, la taille d’effet est colossale et qu’il est plutôt rare d’observer de telles tailles d’effets dans la réalité.Pour prendre un exemple plus réaliste, considérons à présent que les hommes banals sont intéressants pendant 5000 secondes alors que les hommes géniaux sont intéressants pendant 5400 secondes, la taille d’effet est à présent de 0.4 et la courbe des hommes géniaux et des hommes banals vont se recouvrir de la manière suivante :constate que la superposition entre les deux courbes est très important et qu’il est difficile de distinguer les hommes géniaux des hommes banals.De manière intéressante, si au lieu de s’intéresser à une personne particulière, fait l’hypothèse que les princes charmants trainent ensemble et que les hommes banals trainent entre eux, mais ils ne se mélangent, vous pourriez rechercher des groupes d’une dizaine d’homme et, dans ce cas, la superposition des deux courbes serait moins importante.Le même raisonnement vaudait pour un échantillon de 50 personnes, les deux courbes se superposeraient encore","code":"\nbanal <- rnorm(3000, 5000, 1000)\ngenial <- rnorm(3000, 5400, 1000)\n\n## Histogrammes non-imprimés :\nbp1 <- hist(banal, plot=FALSE, nclass=20)  # 20 classes...\nbp2 <- hist(genial, plot=FALSE, nclass=20)  #\n\n## Calcul de minima/maxima :\nhlims <- range(c(bp1$breaks, bp2$breaks))\nvlims <- range(c(bp1$counts, bp2$counts))\n\n\n\n## Couleurs de remplissage, dont une avec ajout de transparence :\nhistcol <- sapply(apply(rbind(col2rgb(c(\"orangered2\", \"lightblue\"))/255,\n                              alpha = c(1, 0.5)), # Première couleur opaque, 50% de transparence pour la seconde.\n                        2, as.list),\n                  do.call, what = rgb)\n\n## Création du graphique :\nhist(banal, border=\"darkred\", xlim=hlims, ylim=vlims,\n     nclass=20,                         # 20 classes...\n     col=histcol[1], main=\"fréquence en fonction du type d'hommes\", xlab=\"banal et génial\")\n\nhist(genial, border=\"darkblue\", add=TRUE,\n     breaks=seq(from=min(hlims), to=max(hlims), by=diff(bp1$breaks[1:2])), # ...largeur de classes égale à celle\n                                                                           # du précédent graphique\n     col=histcol[2])      # Couleur transparente (en sur-impression).\nbanal <- rnorm(3000, 5000, 1000/10^0.5)\ngenial <- rnorm(3000, 5400, 1000/10^0.5)\n\n## Histogrammes non-imprimés :\nbp1 <- hist(banal, plot=FALSE, nclass=20)  # 20 classes...\nbp2 <- hist(genial, plot=FALSE, nclass=20)  #\n\n## Calcul de minima/maxima :\nhlims <- range(c(bp1$breaks, bp2$breaks))\nvlims <- range(c(bp1$counts, bp2$counts))\n\n\n\n## Couleurs de remplissage, dont une avec ajout de transparence :\nhistcol <- sapply(apply(rbind(col2rgb(c(\"orangered2\", \"lightblue\"))/255,\n                              alpha = c(1, 0.5)), # Première couleur opaque, 50% de transparence pour la seconde.\n                        2, as.list),\n                  do.call, what = rgb)\n\n## Création du graphique :\nhist(banal, border=\"darkred\", xlim=hlims, ylim=vlims,\n     nclass=20,                         # 20 classes...\n     col=histcol[1], main=\"fréquence en fonction du type d'hommes\", xlab=\"banal et génial\")\n\nhist(genial, border=\"darkblue\", add=TRUE,\n     breaks=seq(from=min(hlims), to=max(hlims), by=diff(bp1$breaks[1:2])), # ...largeur de classes égale à celle\n                                                                           # du précédent graphique\n     col=histcol[2])      # Couleur transparente (en sur-impression)."},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"quand-cest-encore-moins-évident","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9.4 Quand c’est encore moins évident?","text":"Superposition des courbes quand prend un échantillon de 50 personnesCe qui est illustré dans ce qui précède est la notion de puissance statistique. Plus les courbes se superposent, et moins la puissance est élevée. l’inverse, moins elles se superposent plus la puissance est élevée. Les facteurs qui ont un impact sur la puissance statistique sont : la taille de l’effet et la taille de l’échantillon. pourrait rajouter certains paramètres expérimentaux comme être en intra participant, c’est-à-dire avoir les mêmes personnes/objets/animaux… pour l’ensemble des mesures, ou encore contrôler l’effet de certains effets parasites à l’aide de covariables.La puissance statistique doit être au minimum de 0.80 (Cohen, 1988), mais les recommendations actuelles tendent à l’augmenter à 0.95 (Lakens, 2013). Cependant, atteindre une puissance de 0.95 entraîne une inflation du nombre de participants (ou de sujet, au sens large - bien que ce terme n’est pas très élégant) qu’il faut recruter, ce qui n’est pas toujours possible dans les études.Pour notre exemple, peut fixer la puissance à 0.80. Avec un échantillon pour lequel s’attend à ce que la moyenne soit de 5000 et un autre pour lequel s’attend à ce que la moyenne soit à 5400, et un écart-type de 1000, peut obtenir la puissance à l’aide de la fonction pwr.t.test du package ‘pwr’.Il faut donc un échantillon de 99 personnes par groupe pour que la puissance statistique soit de 0.80.Cependant, il faut avoir conscience qu’il existe deux différences majeures entre mon exemple pédagogique et la réalité :Dans la réalité, ne connait pas la valeur réelle des moyennes de chaque modalité qu’veut comparer.Dans la réalité, ne connait pas la valeur réelle des moyennes de chaque modalité qu’veut comparer.Dans la réalité, il faut calculer la puissance statistique priori, c’est-à-dire avant le début du recueil de données.Dans la réalité, il faut calculer la puissance statistique priori, c’est-à-dire avant le début du recueil de données.Normalement, vous devriez avoir identifié un problème : dans la réalité, vous ne connaissez pas les moyennes des paramètres (ni l’écart-type) alors comment pourriez-vous calculer la puissance priori ? Il existe plusieurs possibilités :vous vous appuyez sur la littérature scientifique, les champs de recherche proche de celui investigué pour déterminer la taille d’effet habituellement observée.vous vous appuyez sur la littérature scientifique, les champs de recherche proche de celui investigué pour déterminer la taille d’effet habituellement observée.vous réalisez une étude pilote pour estimer la taille de l’échantillon nécessaire, mais les données obtenues par l’étude pilote ne doivent pas faire partie du jeu de données final.vous réalisez une étude pilote pour estimer la taille de l’échantillon nécessaire, mais les données obtenues par l’étude pilote ne doivent pas faire partie du jeu de données final.vous estimez la taille d’effet minimum que vous désirez mettre en évidence dans l’étude et vous fixez la puissance en fonction de cette taille d’effet minimum. Par exemple, si vous voulez tester l’effet d’un médicament, et que vous savez que l’effet placebo explique 5% de la variance, alors vous pourriez décider que votre médicament pour être utile doit avoir un effet deux fois plus important que l’effet placebo, c’est-à-dire expliquer 10% de la variance.vous estimez la taille d’effet minimum que vous désirez mettre en évidence dans l’étude et vous fixez la puissance en fonction de cette taille d’effet minimum. Par exemple, si vous voulez tester l’effet d’un médicament, et que vous savez que l’effet placebo explique 5% de la variance, alors vous pourriez décider que votre médicament pour être utile doit avoir un effet deux fois plus important que l’effet placebo, c’est-à-dire expliquer 10% de la variance.","code":"\nbanal <- rnorm(3000, 5000, 1000/50^0.5)\ngenial <- rnorm(3000, 5400, 1000/50^0.5)\n\n## Histogrammes non-imprimés :\nbp1 <- hist(banal, plot=FALSE, nclass=20)  # 20 classes...\nbp2 <- hist(genial, plot=FALSE, nclass=20)  #\n\n## Calcul de minima/maxima :\nhlims <- range(c(bp1$breaks, bp2$breaks))\nvlims <- range(c(bp1$counts, bp2$counts))\n\n## Couleurs de remplissage, dont une avec ajout de transparence :\nhistcol <- sapply(apply(rbind(col2rgb(c(\"orangered2\", \"lightblue\"))/255,\n                              alpha = c(1, 0.5)), # Première couleur opaque, 50% de transparence pour la seconde.\n                        2, as.list),\n                  do.call, what = rgb)\n\n## Création du graphique :\nhist(banal, border=\"darkred\", xlim=hlims, ylim=vlims,\n     nclass=20,                         # 20 classes...\n     col=histcol[1], main=\"fréquence en fonction du type d'hommes\", xlab=\"banal et génial\")\n\nhist(genial, border=\"darkblue\", add=TRUE,\n     breaks=seq(from=min(hlims), to=max(hlims), by=diff(bp1$breaks[1:2])), # ...largeur de classes égale à celle\n                                                                           # du précédent graphique\n     col=histcol[2])      # Couleur transparente (en sur-impression).\nlibrary(pwr)\nd<-(5000-5400)/1000\ndFALSE [1] -0.4\npwr.t.test(d=d,sig.level = 0.05, power=0.80, type=\"two.sample\", alternative=\"two.sided\")FALSE \nFALSE      Two-sample t test power calculation \nFALSE \nFALSE               n = 99.08032\nFALSE               d = 0.4\nFALSE       sig.level = 0.05\nFALSE           power = 0.8\nFALSE     alternative = two.sided\nFALSE \nFALSE NOTE: n is number in *each* group"},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"en-synthèse","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9.5 En synthèse","text":"les tests statistiques testent une hypothèse d’absence de différenceles tests statistiques testent une hypothèse d’absence de différenceLa probabilité fournie par un test est la probabilité d’observer la différence en raison des variations aléatoires dues à la distribution d’échantillonnage.La probabilité fournie par un test est la probabilité d’observer la différence en raison des variations aléatoires dues à la distribution d’échantillonnage.Si la probabilité est faible, cela signifie qu’il est improbable que la différence soit dû au hasard et l’attribuera alors à l’effet de la manipulation expérimental.Si la probabilité est faible, cela signifie qu’il est improbable que la différence soit dû au hasard et l’attribuera alors à l’effet de la manipulation expérimental.Dans ce dernier cas, dit qu’rejette l’hypothèse nulle.Dans ce dernier cas, dit qu’rejette l’hypothèse nulle.peut commettre deux types d’erreurs : l’erreur de 1e espèce et de 2e espèce.peut commettre deux types d’erreurs : l’erreur de 1e espèce et de 2e espèce.Habituellement, fixe l’erreur de 1e espèce à 5% et l’erreur de 2e espèce au maximum à 20%Habituellement, fixe l’erreur de 1e espèce à 5% et l’erreur de 2e espèce au maximum à 20%Même si une population qui n’est pas distribuée normalement, la distribution d’échantillonnage suivra la loi normale si la distribution initiale ne présente pas une asymétrie trop importante.Même si une population qui n’est pas distribuée normalement, la distribution d’échantillonnage suivra la loi normale si la distribution initiale ne présente pas une asymétrie trop importante.diminue l’erreur de type II en augmentant la taille de l’échantillonOn diminue l’erreur de type II en augmentant la taille de l’échantillonIl faut connaître la taille de l’effet pour calculer la puissance priori.Il faut connaître la taille de l’effet pour calculer la puissance priori.Deux facteurs influencent la puissance : la taille de l’effet et la taille de l’échantillonDeux facteurs influencent la puissance : la taille de l’effet et la taille de l’échantillon","code":""},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"une-approche-plus-formelle","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9.5.1 Une approche plus formelle","text":"","code":""},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"la-crise-de-reproductibilité-et-le-p-hacking.","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9.5.1.1 La crise de reproductibilité et le p hacking.","text":"Dans tous les domaines scientifiques, il est nécessaire de publier des articles, et dans les entreprises, d’avoir des résultats intéressants.Au niveau universitaire par exemple, publier permet d’avoir un poste, des promotions, des primes, des financements. Quand des financements, il faut pouvoir les justifier, expliquer ce qu’fait de ces financements. Généralement, résume la nécessité de publier pour les chercheurs par un adage : “publish perish!”.Au niveau privé, montrer qu’un médicament est efficace permet de le commercialiser, montrer qu’un engrais n’est pas dangereux permet de le commercialiser…Bref, tant au niveau public que dans le privé, les enjeux sont énormes, et quand les enjeux sont énormes, les dérives commencent à apparaître.Pour comprendre la cause de ces dérives, il est nécessaire de savoir ce qu’est un résultit dit “positif”. Il s’agit des études pour lesquels les données corroborent l’hypothèse par la présence de résultats significatifs. Ce sont ces résultats qui sont essentiellement publiés en science.En psychologie, par exemple, Fanelli (2010) souligne que 90% des études publiées sont des réultats positifs. Ce nombres d’études avec des résultats positifs suit une tendance constante depuis les année 1990. En effet, dans une étude de 2011, Fanelli montre que la publication de résultats positifs augmenté de 22% depuis les années 1990. comprend dès lors aisément pourquoi il faut des résultats positifs et que cela amène à des comprotements douteux.Un des comportements douteux est évidemment la fabrication de données, ce qui été largement médiatisée par l’affaire Stapel, un psychologue social, qui avait fabriqué les résultats de plusieurs dizaines de ses articles. pourrait penser que ce phénomène est plutôt rare, mais c’est loin d’être certain. En effet, Fanelli (2009), encore lui, posé à des chercheurs deux questions :Avez-vous déjà trafiqué vos données ?Avez-vous déjà trafiqué vos données ?Avez-vous déjà adopté une pratique douteuse (il n’est pas nécesaire de dévolopper ici les différentes pratiques douteuses) ?Avez-vous déjà adopté une pratique douteuse (il n’est pas nécesaire de dévolopper ici les différentes pratiques douteuses) ?Ce qui est intéressant, c’est que, sur le panel de chercheurs, la plupart indique s’être toujours comporté de manière intègre, mais que les 3/4 connaissent un collègue qui adopté une pratique douteuse, et qu’un tiers des chercheurs connaissent même quelqu’un qui fabriqué ou falsifié ses données. Table 2. Résultats obtenus par Faneli (2009) Parmi les pratiques douteuses que la plupart des personnes n’identifient même pas comme problématique, il y le p-hacking.","code":""},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"le-p-hacking.","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9.5.1.2 Le p hacking.","text":"Le p-hacking consiste à analyser les données qu’obtenu de différentes manières, en ajoutant parfois des observations, en supprimant des observations dites “outliers”, en ajoutant des covariables, ou simplement en prenant un grand nombre de mesures et en ne présentant les résultats que d’une minorité de ces mesures.Pour comprendre le p hacking, il faut comprendre un principe statistique essentiel : la multiplication de l’erreur de 1e espèce.Ce principe se résume à une phrase : plus vous faites d’analyses, plus vous augmentez le risque d’obtenir un effet significatif alors que vous ne devriez pas en avoir un.Pour l’illustrer, nous allons reprendre l’exemple que nous avions réalisé plus haut pour montrer la distribution d’échantillonnnage. La simulation montrait que 5% des échantillons étaient considérés comme dépassant le seuil de significativité.\nPour reprendre le raisonnement complet, le t de Student comparaison à une norme est obtenu à l’aide la formule suivante :\\[t= \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt n}}\\]peut obtenir le seuil précis de significativité grâve à la fonction qt. Dans cette fonction, il faut préciser le seuil unilatéral (.e., 0.025) et les degrés de liberté (ddl). Pour le t comparaison à une norme, les ddl valent la taille de l’échantillon -1. Comme nous avons pris des échantillons de 50 personnes, les ddl valent 49.Nous pouvons à présent calculer le t de Student pour l’ensemble des moyennes d’échantillons que nous avions obtenues pour illustrer la distribution d’échantillonnage. Le code ci-dessous fournit les t premiers tLe code ci-dessous fournit les 10 premiers t obtenus pour les 10 premières moyennes d’échantillons.peut comptabiliser le nombre de t qui amènent à rejter à tort l’hypothèse nulle (erreur de première espèce).En l’occurrence, l’erreur de 1e espèce est d’environ 5% comme attendu.Imaginons à présent que nous faisions deux analyses. peut le simuler avec la fonctino sample qui va échantilloner deux t dans la liste des t disponibles.Si nous répétons un grand nombre de fois (1000 par exemple) l’opération pour 2 échantillons, le nombre de situations où au moins un des t est significatif serait bien plus élevé que 5%. Le code ci-desous crée une boucle qui permet de faire cette simulation.Dans ce cas, l’erreur de première espèce serait de \\(7.5\\) pourcents, donc bien supérieur au 5% intialement accepté.Ce phénomène est accenté lorsqu’fait encore plus d’analyses, comme 5 ou 10 analyses.En effet, le taux d’erreur de 1e espèce pour 5 analyses est de \\(20.8\\) pourcents.En effet, le taux d’erreur de 1e espèce pour 10 analyses est de \\(39.4\\) pourcents.En d’autres termes, le problème est que, si multiplie le nombre de comparaisons, augmente également le risque de première espèce. Ainsi, il est nécessaire de distinguer l’erreur par comparaison (EC) et l’erreur par famille (EF).\nL’erreur par comparaison est le seuil auquel travaille sur une comparaison (par exemple 5%).\nL’erreur par famille représente le seuil d’erreur pour l’ensemble des comparaisons.\nEn réalité, il n’était pas nécessaire de faire ces simulations car il est très simple de prédire le taux d’erreur de première espèce qu’va avoir en fonction du nombre d’analyses. Il suffit d’appliquer la formule suivante :\n\\[EF = 1 - (IC)^n \\]\noù EF représente l’erreur par famille, IC représente l’intervalle de confiance et n le nombre d’analyses.Comme attendu, ces valeurs sont proches des pourcentages d’erreurs qu’obtenu par simulation.\nAu-delà de l’erreur par comparaison et de l’erreur par famille. Il existe un troisième type d’erreur, appelée erreur par expérience. Cette terminologie un peu désuète à l’heure actuelle doit attirer votre attention sur le fait que, si compare deux groupes, par exemple, sur un très grand nombre de variables dépendantes différentes, le test qui sera utilisé sera probablement un test de Student qui sera répété sur chacune des variables. Etant donné qu’il s’agit d’analyses différentes, pourrait penser que le seuil de significativité est maintenu constant. En fait, il n’en est rien. Le taux d’erreurs par expérience inclut toutes les analyses conduites au sein d’une expérience.En pratique, aucune correction particulière n’est appliquée entre des analyses différentes, excepté quelquefois lorsqu’une même analyse est répétée un grand nombre de fois (des t de Student successifs par exemple). Néanmoins, il vous est grandement recommandé de réaliser le moins de comparaisons possibles.Lorsque vous ne pouvez pas éviter d’avoir un grand nombre de variables dépendantes, et donc de comparaisons, il est souhaitable de se tourner vers les analyses multivariées : MANOVA, extraction de facteurs par l’analyse factorielle exploratoire, ou analyse en composante principale pour réaliser une réduction de données.Une autre manière pour éviter ce problème consiste à corriger la probabilité. Il existe une multitude de corrections des probabilité, dont la plus connue est sans aucun doute la correction de Bonferroni. Cette correction consiste à multiplier les probabilités obtenues pour chaque analyse par le nombre d’analyses réalisées. Les probabilités qui restent inférieures au seuil de significativité sont considérées comme significatives alors que celles qui sont supérieures au seuil de significativité après la correction doivent être considérées comme non significatives car pouvant être dues à une erreur de 1e espèce.Imaginons le vecteur de probabilités organisées par ordre croissant ps :Sur ces 6 probabilités, les 5 premières doivent être considées comme révélant la présence d’un effet significatif. Cependant, 3 de ces probabilités deviennent non significatives après la correction de Bonferroni.Cela signifie que, pour 3 analyses, il était tout à fait possible que l’effet était dû à une erreur de première espèce. Evidemment, il n’est pas nécessaire de réaliser cette correction à la main mais peut-être obtenu à l’aide d’une fonction R, qui est p.adjustSi la correction de Bonferroni permet de maintenir l’erreur de 1e espèce constante, cette correction augmente le risque de seconde espèce. Pour éviter ce problème, il est préférable d’utiliser la correction de Holm. Le principe de cette correction est le même que celui utilisé pour la correction de Bonferroni, mais en prenant en compte le nombre d’analyse qu’il reste encore à corriger. Pour le formuler autrement, multiplie chaque probabilité par son rang en ordre décroissant.Notez que, contrairement à Bonferroni, seule une probabilité est considérée comme non significative après correction alors qu’elle était significative avant la correction.nouveau, il est possible d’utiliser la fonction p.adjuste pour obtenir cette correction automatiquement :Peut-être avez-vous remarquer que, dans la correction de la probabilité à la main, la 4e probabilité corrigée manuellement vaut 0.03, alors qu’elle vaut 0.036 quand elle est corrigée avec p.adjust. Ce phénomène s’explique aisément par le fait que la 4e probabilité non corrigée vaut 0.01 alors que la 3e vaut 0.009. Comme il n’est pas normal qu’une probabilité corrigée ait une valeur supérieure à une autre probabilité qui lui était supérieure quand elles n’étaient pas corrigées, la règle est qu’une probabilité corrigée ne peut pas être inférieure à une autre probabilité si cette probabilité lui était inférieure avant la correction, raison pour laquelle la 3e et la 4e probabilité ont la même valeur, à savoir 0.036.Pour ceux qui veulent développer leurs compétences de p-hacker et voir comment vous pouvez obtenir un effet significatif alors que vous n’auriez pas dû en avoir un, vous pouvez utiliser l’application shiny présente sur le lien suivant :\nhttp://shinyapps.org/apps/p-hacker/Au finale, les situations minimales où il faudra utiliser ces corrections sont dans les matrices de corrélations et les comparaisons posteriori dans les anova où les hypothèses.En réalité, toutes les analyses d’une même recherche devraient avoir leur probabilité corrigée, mais cette pratique est peu courante.Une dernière remarque concernant l’erreur par comparaison et l’erreur par famille.\nIl existe un troisième type d’erreur, appelée erreur par expérience.\nCette terminologie un peu désuète à l’heure actuelle doit attirer votre attention sur le fait que, si compare deux groupes par exemple, sur un très grand nombre de variables dépendantes différentes, le test qui sera utilisé sera probablement un test de Student qui sera répété sur chacune des variables. Etant donné qu’il s’agit d’analyses différentes, pourrait penser que le seuil de significativité est maintenu constant.\nEn fait, il n’en est rien. Le taux d’erreurs par famille inclut toutes les analyses conduites au sein d’une expérience.\nEn pratique, aucune correction particulière n’est appliquée entre des analyses différentes, excepté quelquefois lorsqu’une même analyse est répétée un grand nombre de fois (des t de Student successifs par exemple). Néanmoins, il vous est grandement recommandé de réaliser le moins de comparaisons possibles.\nLorsque vous ne pouvez pas éviter d’avoir un grand nombre de variables dépendantes, et donc de comparaisons, il est souhaitable de se tourner vers les analyses multivariées : MANOVA, extraction de facteurs par l’analyse factorielle exploratoire, ou analyse en composante principale pour réaliser une réduction de données.","code":"\nqt(0.025, 49)## [1] -2.009575\nt<-(M_ech-mean(pop))/(sd(pop)/50^0.5)\nM_ech[1:10]##  [1] 3594  334 7295 7434 7762 1804 4887 3000 8094  426\nround(t[1:10], 2)##  [1]  -3.45 -11.43   5.62   5.96   6.76  -7.83  -0.28  -4.90   7.58 -11.20\nlibrary(plyr)\nn<-count(abs(t>2.009575))\nn##   x  freq\n## 1 0 97822\n## 2 1  2227\nn/length(t)##              x       freq\n## 1 0.000000e+00 0.97774091\n## 2 9.995102e-06 0.02225909\ndeux<-sample(t, 2, replace=F)\ndeux## [1] -0.9578006 -0.2075594\nfor(i in 1:1000){sample(t, 2, replace=F)->deuxb\nrbind(deux, deuxb)->deux}\n(abs(deux)>2.009575)->sig\nrowSums(sig)->n_sig2\ncount(n_sig2>0)-> n_sig2\nn_sig2##       x freq\n## 1 FALSE  926\n## 2  TRUE   75\nsample(t, 5, replace=F)->cinq\nfor(i in 1:1000){sample(t, 5, replace=F)->cinqb\nrbind(cinq, cinqb)->cinq}\n(abs(cinq)>2.009575)->sig\nrowSums(sig)->n_sig5\ncount(n_sig5>0)-> n_sig5\nn_sig5##       x freq\n## 1 FALSE  793\n## 2  TRUE  208\nsample(t, 10, replace=F)->dix\nfor(i in 1:1000){sample(t, 10, replace=F)->dixb\nrbind(dix, dixb)->dix}\n(abs(dix)>2.009575)->sig\nrowSums(sig)->n_sig10\ncount(n_sig10>0)-> n_sig10\nn_sig10##       x freq\n## 1 FALSE  607\n## 2  TRUE  394\n1- (0.95)^2## [1] 0.0975\n1-(0.95)^5## [1] 0.2262191\n1-(0.95)^10## [1] 0.4012631\nps<-c(0.001,0.002,0.009,0.01,0.03,0.08)\nps*6## [1] 0.006 0.012 0.054 0.060 0.180 0.480\np.adjust(ps, \"bonferroni\")## [1] 0.006 0.012 0.054 0.060 0.180 0.480\n rangs<-rank(-ps)\nrangs## [1] 6 5 4 3 2 1\nps*rangs## [1] 0.006 0.010 0.036 0.030 0.060 0.080\np.adjust(ps, \"holm\")## [1] 0.006 0.010 0.036 0.036 0.060 0.080"},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"dautres-manières-de-penser-les-statistiques","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9.5.2 D’autres manières de penser les statistiques","text":"","code":""},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"la-taille-deffet","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9.5.2.1 La taille d’effet","text":"Que ce soit en utilisant la valeur de la probabilité ou l’intervalle de confiance, peut adresser comme critique que, aussi infime que soit la différence, avec un échantillon suffisamment grand, un effet significatif sera observé.Pour illustrer ce phénomène, regarder les deux photos ci-dessous et décider celle que vous préférez. Celle de gauche représente les lacs de Plivitce en Croatie et à droite, le coucher du soleil sur le Grand Canyon.peut imaginer que la préférence pour le payasage dépend de la filière à laquelle vous appartenez. Ainsi, pourrait imaginer comparer des étudiants en psychologie avec des étudiants en statistiques. Les données sont présentées dans la table 4.Table 4. Table des effectifs concernant la préférence pour les images en fonction de la filière Pour tester s’il existe une dépendance entre la préférence et le type d’études, va réaliser un \\(\\chi^2\\) d’indépendance.Cette analyse révèle qu’il n’y pas de lien entre la préférence et le type d’étude, \\(\\chi^2\\)(1) = 0.267, p= .6056.Néanmoins, l’échantillon sur lequel l’analyse été faite est relativement faible. Si cet échantillon était 100 fois plus grand, voici ce que nous obtiendrions.Donc, contraitement à la première analyse, cette analyse indique qu’il existe un lien entre la formation et la préférence pour la photo. Cela indique que, pour des jeux de données de taille différente, la valeur de la statistique n’est pas informative.Pour rendre les choses comparable, il faut calculer une taille d’effet. Il existe deux familles de tailles d’effet : la famille de d et la famille des \\(R^2\\). La famille des d consiste à présenter la taille d’effet comme un “distance” alors que dans la famille des \\(R^2\\), présente la taille d’effet comme un pourcentage de variance expliquée.Pour le \\(\\chi^2\\) d’indépendance, une des mesures de la taille d’effet est le V de Cramer (qu’pourrait élever au carré pour avoir un pseudo-pourcentage de variance expliquée). obtient ce V de Cramer grâce à la fonction cramersV du package ‘lsr’.constate que, peu importe la taille d’échantillon, la valeur est identique : le V de Cramer vaut 0.067. Ainsi, la taille d’effet rend les études comparables entre elles.Pour comprendre en quoi la taille d’effet peut être considéré comme un pourcentage de variance expliqué. Intéressons-nous à la corrélation.Une étude (réelle) montré que le nombre d’arrestations pour consommation de marijuana aux USA était significativement corrélé au nombre de colonies d’abeilles productrices de miel (les données peuvent être trouvées ici : http://tylervigen.com/view_correlation?id=1582)La corrélation entre les deux variable vaut -0.911:Cela signifie que le pourcentage de variance expliquée est la valeur de cette corrélation au carré.Donc, 83% de la variance est expliquée par le lien entre le nombre de colonies d’abeilles et le nombre d’arrestation pour possession de marijuana (vous savez à présent ce qu’il vous reste à faire si vous consommez de la marijuana).Pour comprendre qu’il s’agit d’un pourcentage de variance, il faut réaliser le modèle linéaire correspondant à cette corrélation (en l’occurrence, le sens peu d’importance, l’objectif n’est pas de déterminer la variable qui prédit l’autre).partir de ce modèle, peut voir que la variance de la variable ‘Miel’ est la somme entre la variance des valeurs prédites par le modèle (obtenues par la fonction <code<fitted) et la variance des résidus (obtenus par la fonction residuals).peut donc déterminer quel est le pourcentage de la variance de la variable ‘Miel’ qui est prédit par la variance expliquée par le modèle :Cette valeur correspond à la valeur de la corrélation au carré.Le fait que la taille d’effet soit comparable entre les études fait que plusieurs revues et sociétés savantes (telle que l’American Psychological Association ) incitent à fournir les tailles d’effet. Certains auteurs recommandent même d’abandonner le report de la valeur de la probabilité (qui finalement peu d’intérêt et dépend de la taille de l’échantillon) pour focaliser sur une analyse plus minitieuse de la taille d’effet.Une illustration intéressante de l’importance de la compréhension de la taille d’effet provient de l’analyse de l’effet de la mémantine, un traitement contre les démences de type Alzheimer. Cette question est fortement politisée à l’heure actuelle car le gouvement décidé de ne plus rembourser les frais liés à ces médicaments. Des associations, comme l’association France Alzheimer, s’insurge en arguant que ces médicaments ont un effet, bien qu’il soit modéré.\nSans vouloir entrer dans un cours complet sur la mémantine, une méta-analyse plutôt récente été réalisée pour tester l’efficacité de la mémantine sur la sphère comportementale (Kishi & Iwata, 2017). Les auteurs concluent que le médicament est efficace sur la plupart des aspects de la sphère comportementale. Cependant, ces auteurs ne semblent pas comprendre leur taille d’effet. En effet, la taille d’effet est aux alentours de 0.1 (il s’agit d’un d de Cohen). Présenté ainsi, il est difficile de se faire une opinion sur l’efficacité du médicament.\nBien sûr l’effet est significatif puisque, agrégées, les études de la méta-analyse porte sur plusieurs milliers de patients. La question qu’il faut se poser est quel est le vrai impact de la mémantine sur les aspects comportementaux des patients. En moyenne, une personne atteinte de la maladie d’Alzheimer va vivre avec sa maladie 10 ans. La mémantine coûte environ 36 euros pour 28 pillule à raison d’une pillule par jour. Le coût pour le patient est donc de 4741 euros au bout de 10 ans.Si vous décidiez de soudoyer un enseignant et qu’il vous demandait 4741 euros pour augmenter votre moyenne de 0.25 points (données réelles sur les notes des étudiants de psychologie), est-ce que vous estimeriez que c’est un bon investissement ? Pourtant, voilà à quoi correspond l’effet de la mémantine et voilà pourquoi il est indispensable de comprendre la notion de taille d’effet.","code":"\nlibrary(lsr)\ndata.frame(c(\"europe\", \"europe\", \"usa\", \"usa\"), c(\"psycho\",\"stat\", \"psycho\", \"stat\"),c(16,14,14,16))->data1\nnames(data1)<-c(\"lieu\", \"etudiants\", \"effectifs\")\ntab<-tapply(data1$effectifs,list(data1$lieu,data1$etudiants),sum,na.rm=TRUE) \nkable(tab)\nchisq.test(tab, correct = FALSE)## \n##  Pearson's Chi-squared test\n## \n## data:  tab\n## X-squared = 0.26667, df = 1, p-value = 0.6056\ntab*100->tab2\ntab2##        psycho stat\n## europe   1600 1400\n## usa      1400 1600\nchisq.test(tab2, correct = FALSE)## \n##  Pearson's Chi-squared test\n## \n## data:  tab2\n## X-squared = 26.667, df = 1, p-value = 2.418e-07\ncramersV(tab, correct = FALSE)## [1] 0.06666667\ncramersV(tab2, correct = FALSE) ## [1] 0.06666667\nrequire(psych)\ncorr.test(miel$Miel, miel$Marijuana)## Call:corr.test(x = miel$Miel, y = miel$Marijuana)\n## Correlation matrix \n## [1] -0.91\n## Sample Size \n## [1] 200\n## These are the unadjusted probability values.\n##   The probability values  adjusted for multiple tests are in the p.adj object. \n## [1] 0\n## \n##  To see confidence intervals of the correlations, print with the short=FALSE option\n round(corr.test(miel$Miel, miel$Marijuana)$r^2,4)## [1] 0.8295\nmodele<-lm(miel$Miel~miel$Marijuana)\nvar(miel$Miel)## [1] 75406.48\nvar(fitted(modele))+var(resid(modele))## [1] 75406.48\n var(fitted(modele))/var(miel$Miel)## [1] 0.8294704"},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"lintervalle-de-confiance","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9.5.2.2 L’intervalle de confiance","text":"Normalement, il est acquis à présent que, lorsque vous faites une analyse statistique, le test que vous réalisez teste l’absence de différence et que, si la probabilité renvoyée est inférieure au seuil de significativité alors vous considérez que la différence est significative. peut appliquer le principe de manière similaire avec l’intervalle de confiance.Généralement, un intervalle de confiance est à 95% (le complément de l’erreur de \\(1^e\\) espèce). Il s’agit des limites entre lesquelles le paramètre que vous estimez va se situer dans 95% des situations. La Figure 8 illustre l’intervalle de confiance pour l’estimation d’une moyenne. Cinquante échantillons ont été tirés aléatoirement dans une population et estime la moyenne de chacun de ces échantillons ainsi que l’intervalle de confiance autour de la moyenne estimée pour chacun d’eux. observe que 95% (96% pour être précis) des estimations l’intervalle de confiance autour de la moyenne estimée coupe l’axe verticale de la moyenne de la population. Ainsi, quand l’intervalle de confiance recouvre le paramètre, cela indique que la moyenne estimée ne se différencie pas significativement de la moyenne de la population.Figure 8. Illustration de l’intervalle de confiance.Vous pouvez reproduire ce type de graphique à l’aide du package ‘TeachingDemos’Il est possible de calculer un intervalle de confiance sur pratiquement tout. Lorsque l’intervalle de confiance porte sur une statistique, considère qu’il n’y pas de différence significative lorsque l’intervalle de confiance comprend la valeur de l’hypothèse nulle. Par exemple, pour un t de Student, l’hypothèse nulle est qu’il n’y pas de différence entre deux moyennes, et si c’est le cas, la valeur du t vaudra 0.Pour le comprendre, nous allons l’illustrer à l’aide d’une étude que nous avons réalisée et qui visait à déterminer pourquoi les super-héros gagnent toujours à la fin. Nous avons émis l’hypothèse que c’était tout simplement parce qu’ils étaient plus forts. l’aide d’un matériel adapté complètement expérimental et top secret, nous avons donc mesuré la force de super-héros et de super-vilains. Les données sont présentées dans le Tableau 2Pour comparer la force de ces deux groupes, l’analyse la plus naturelle est de comparer la force moyenne des super-héros à celle des super-vilains. Le test adapté pour faire cette analyse est le t de Student pour échantillons indépendantsCette analyse fournit un intervalle de confiance indiquant que la “vraie” valeur du t de Student est située entre -8.72 et 5.60. Comme cet intervalle recouvre le zéro, ne peut pas exclure que les deux groupes ne se différencient pas sur leur force.Cette manière d’envisager les statistiques permet d’identifier rapidement une différence significative grâce aux représentations graphiques représentation graphique.Dans ce graphique, la ligne horizontale du milieu représente la moyenne, tandis que les limites supérieures et inférieures de la boîte représente la moyenne plus une erreur-type et la moyenne moins une erreur-type. Enfin, les extrémités des barres verticales représente l’intervalle de confiance à 95%.Il existe deux manière de lire ce graphique : la première manière consiste à regarder la barre verticale. Si elle dépasse la barre horizontale représentant la moyenne de l’autre groupe, cela signifie que la différence n’est pas significative. Si elle ne l’atteint pas, c’est que la différence est significative. Comme l’intervalle de confiance à 95% est calculé en multipliant l’erreur-type par 1.96, peut arriver à la même conclusion avec l’erreur-type comme information uniquement. En l’occurrence, comme les super-vilains ont une moyenne supérieure en force, peut savoir si cette différence est significative en regardant la limite inférieure de la boîte (la ligne horizontale inférieure). Si cette ligne inférieure est au-dessus de la ligne supérieure de la boîte des super-héros, cela signifie qu’il y au moins 2 erreurs-types qui séparent les deux groupes, et donc que la différence est significative. Dans notre exemple, la limite inférieure des super-vilains est à un niveau inférieur à la limite supérieure des super-héros. La différence est donc non significative.Il est de plus en plus souvent demandé de fournir les intervalles de confiance dans les articles. Si certains chercheurs ne sont pas convaincus de leur utilité, pour Thompson (2002), l’intervalle de confiance est un moyen pour les tailles d’effets. Néanmoins, à partir de la probabilité, peut retrouver sans difficulté l’intervalle de confiance (Altman, 2011).\npeut raisonnablement penser que ces intervalles de confiance sont utiles dans les représentations graphiques, et que son utilité de fournir des informations sur l’intervalle à l’intérieur duquel se trouve le vrai paramètre dans 95% des situations (Jiroutek, & Turner, 2016).note : il s’agit ici d’un abus de langage. En réalité, devrait dire que, étant donné une moyenne d’une population, l’estimation de la moyenne d’un échantillon 95% de chance d’être inclus dans l’intervalle de confiance. Ainsi, comme ne peut avoir accès directement aux paramètres, peut créé un intervalle autour de l’estimation du paramètre qui englobera le paramètre dans 95% des situations.","code":"\nlibrary(\"TeachingDemos\")\nci.examp()## \n##  Welch Two Sample t-test\n## \n## data:  Force by Groupe\n## t = -0.44532, df = 29.959, p-value = 0.6593\n## alternative hypothesis: true difference in means between group super_héros and group super_vilains is not equal to 0\n## 95 percent confidence interval:\n##  -8.728694  5.603694\n## sample estimates:\n##   mean in group super_héros mean in group super_vilains \n##                     83.3750                     84.9375"},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"les-bootstraps","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9.5.2.3 Les bootstraps","text":"Dans de nombreuses situations, l’hypothèse faite sur la distribution des données est que la distribution suit une distribution normale. Cependant, cette condition est loin d’être systématiquement respectée. Une manière de contourner ce problème pour estimer un paramètre de la manière la plus optimale possible est de réaliser un bootstrap.Cette technique généraliste consiste à échantillonner dans un échantillon un grand nombre de fois. L’échantillon créé par bootstrap est de taille identique à la taille initiale de l’échantillon. Les échantillons sont néanmoins différents à chaque itération car l’échantillonnage est réalisé avec remise. Ainsi, l’échantillon est composé de Pierre, Paul, Jacques. Comme il y remise, Pierre peut donc être tiré plusieurs fois et votre premier échantillon de bootstrap pourrait être Pierre, Pierre, Paul. Evidemment, pour que le bootstrap ait du sens, il faut que la taille de l’échantillon soit suffisamment importante pour que le même échantillon ne revienne pas dans la répétition de l’opération.L’intérêt de réaliser un boostrap est qu’se dégage de la distribution des données puisqu’va créer notre intervalle de confiance par simulation. Donc, peu importe la distribution initiale des données, le bootstrap s’y adapatera puisque la simulation de la distribution se basera sur la forme spécifique de la distribution des données de notre échantillon.Dans une de nos études, nous n’avons pas très bien compris l’expérience de Milgram sur la soumission à l’autorité. Dans l’étude originale de Milgram, les participants doivent soumettre des chocs électriques à un autre participant chaque fois qu’il commet une erreur à une question. Ces chocs deviennent de plus en plus immportant au fur et à mesure que le nombre d’erreurs augmente. l’insu des participants, la personne qui reçoit les chocs est un comparse de l’expérimentateur qui joue la comédie car elle ne reçoit pas vraiment des chocs.Dans notre étude, nous n’avions pas vraiment compris que c’était un comparse, alors nous avons administré à nos participants des chocs électriques pour qu’ils apprennent à jouer aux fléchettes (pour ceux/celles qui auraient le moindre doute sur la question, et qui voudraient signaler aux comités d’éthique ou aux CPP, nous tenons à leur préciser que cette étude est un fake … notre laboratoire ne dispose pas de dispositif pour administrer des chocs électriques).\nNous avons voulu savoir si les personnes soumises à un choc électrique envoyaient leurs fléchettes plus près du centre de la cible. sait que la distance habituelle du centre de la cible est de 34 mm dans notre cas. Nous avons donc réalisé cette étude auprès de 50 personnes. Voici les données :Pour déterminer si le groupe soumis à un choc électrique est plus proche du centre de la cible, il faut réaliser un t de Student comparaison à une norme.Dans le bootstrap, va tirer au hasard avec remise un nombre d’observations de notre échantillon qui correspond à la taille de l’échantillon. Pour faire un tirage au sort, utilise la fonction sample. indique dans cette fonction la taille d’échantillon qu’veut échantillonner et l’argument ‘replace’ permet de préciser si c’est un tirage avec remise ou non. L’argument ‘T’ (pour TRUE) indique que c’est avec remise.\nComme notre échantillon fait 50 personnes, il faut que chaque échantillon du boostrap soit composé de 50 personnes.Sur ce nouvel échantillon, peut donc calculer un nouveau t de Student comparaison à une norme qui va avoir une valeur différente de l’échantillon initial.Le principe du bootstrap est de répéter cette opération un grand nombre de fois (par exemple 1000, et au minimum 500) pour créer l’intervalle de confiance sur un paramètre ou une statistique. La boucle permet de réaliser cette opération. va créer un vecteur avec 1000 t de Student comparaison à une norme basé sur des échantillonnage avec remise. Notez qu’peut également faire le bootstrap sur la moyenne pour connaître l’intervalle à l’intérieur duquel la vraie moyenne à 95% de chance d’être.présent, l’objet t2 dans la mémoire de R est composé de 1000 valeurs, qui correspondent chacune à un t de Student comparaison à une norme qu’obtenues par échantillonnage avec remise.\nNous pouvons obtenir l’intervalle de confiance sur ces t de Student. Il existe plusieurs méthodes pour obtenir cet intervalle de confiance. L’une d’entre elle consiste à utiliser les percentiles. Comme nous voulons les 95% pourcents des t au centre de la courbe, répartit les 5 pourcents de manière équitable sur les deux extrémités, c’est-à-dire 2.5% de chaque côté. obtient ces centiles grâces à la fonction quantile en précisant les valeurs des centiles désirées, donc 0.025 et 0.975 dans notre cas.En l’occurrence, l’intervalle de confiance ne recouvre pas le 0, ce qui signifie qu’il est improbable que la valeur du t soit égale à 0. peut donc interpréter cet intervalle comme révélant la présence d’un effet significatif.Dans notre exemple, nous avons réaliser le boostrap sur la valeur de la statistique, mais aurait pu le faire sur la moyenne. Dans ce cas, si l’intervalle ne recouvrait pas 34 (la norme), c’est-à-dire que 34 n’était pas compris entre la limite inférieure et la limite supérieure de l’intervalle de confiance, alors c’est qu’il y avait une différence significative. Dans le cas contraire, la différence était non significative.Comme d’habitude, il n’est pas nécessaire de programmer soi-même le bootrap. Il existe un ensemble de packages dans R qui le font à votre place et, pour certains packages ont développé des fonctions spécifiques de bootstrap, qui sont prêt à l’emploi.En l’occurrence, la fonction boot du package ‘boot’ permet de très bien faire cela. Nous allons en l’occurrence faire un boostrap sur une moyenne.Notez que, comme les échantillons ne sont pas les mêmes d’une fois à l’autre, l’intervalle de confiance sera sensiblement modifié entre deux répétitions.Pour terminer sur le bootstrap, comme précisé plus haut, il existe différentes manières de calculer l’intervalle de confiance. La méthode la plus robuste est le bootrap corrigé pour des biais d’asymétrie (BCa) (Efron & Gong, 1983).","code":"##  [1] 40 53 37 47 35 32 47 41 53 35 56 47 43 55 40 38 43 23 36 58 46 34 37 39 29\n## [26] 45 46 29 54 42 38 36 50 50 32 37 40 44 41 59 33 55 47 51 46 31 43 51 11 39\nmean(fleche)## [1] 41.88\n(mean(fleche)-34)/(sd(fleche)/50^0.5)->t.value # le 34 mm est la norme connue dans la population\nt.value## [1] 5.911544\nsample(fleche,50,replace = T)##  [1] 43 43 36 32 38 32 33 47 43 38 23 35 29 58 55 50 47 37 53 37 35 34 43 29 47\n## [26] 47 50 39 41 40 56 34 40 46 35 53 37 38 35 32 47 32 32 53 32 53 45 59 42 58\nf2<-sample(fleche,50,replace = T)\nt2<-(mean(f2)-34)/(sd(f2)/50^0.5)\nt2## [1] 8.344773\nfor(i in 1:999){\n  f2<-sample(fleche,50,replace = T)\n  t3<-(mean(f2)-34)/(sd(f2)/50^0.5)\n  t2<-c(t2, t3)\n}\nquantile(t2, c(0.025,0.975))##     2.5%    97.5% \n## 3.565388 9.138221\nlibrary(boot)\n# on commence par créer une fonction où il faut des données et les itérations. \nmeanfun <- function(data, i){\n  d <- data[i] # on stocke une itération donnée dans l'objet d \n  return(mean(d)) # on fait moyenne  \n}\nboot.out<-boot(fleche, statistic = meanfun, R=1000)\nboot.ci(boot.out)## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\n## Based on 1000 bootstrap replicates\n## \n## CALL : \n## boot.ci(boot.out = boot.out)\n## \n## Intervals : \n## Level      Normal              Basic         \n## 95%   (39.26, 44.55 )   (39.18, 44.64 )  \n## \n## Level     Percentile            BCa          \n## 95%   (39.12, 44.58 )   (39.09, 44.56 )  \n## Calculations and Intervals on Original Scale"},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"lapproche-par-maximum-de-vraisemblance.","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9.6 L’approche par maximum de vraisemblance.","text":"Le maximum de vraisemblance est un concept essentiel au niveau statistique car il amène la personne qui veut réaliser des statistiques à penser en termes de modèle. Il s’agit habituellement d’un estimateur qui est utilisé dans les techniques statistiques plus avancées telles que la régression logistique, les analyses factorielles ou les modèles linéaires mixtes, et plus généralement dans tous les modèles non linéaires. Néanmoins, peut l’utiliser pour estimer n’importe quel paramètre.En général, le maximum de vraisemblance est un méthode pour obtenir les estimations de paramètres inconnus en optimisant la fonction de vraisemblance.Le principe la vraisemblance est d’estimer la probabilité d’observer les données de manière itérative. Ainsi, part d’une valeur “aléatoire”, qui va être la solution initiale, et va chercher à améliorer cette solution initiale de sorte à augmenter, c’est-à-dire optimiser, la probabilité d’observer cette distribution de données par simulation. Le maximum de vraisemblance est la valeur de la probabilité pour laquelle les paramètre seront optimisés pour avoir la probabilité maximale. Quand cette probabilité atteint un maximum (moyennant une certaine tolérance), dit que le modèle convergé.","code":""},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"explication-mathématique","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9.6.1 Explication mathématique","text":"AVERTISSEMENT : cette section peut être ignorée, en particulier pour ceux à qui les formules mathématiques donnent la nausée. Dans ce cas, vous pouvez vous reporter à la section suivante sur le ratio de vraisemblance. Pour les autres, que Dieu ait votre âme.D’un point de vue mathématique, la vraisemblance est la densité de probabilité associée aux données observées :\\[L_x(\\theta)= f(x)\\]Pour illustrer le concept, nous allons reprendre l’exemple des chocs électriques reçus par les participants quand ils s’éloignent du centre de la cible. peut calculer la vraisemblance de la moyennePour une variable numérique, la vraisemblance est maximale lorsque la moyenne et l’écart-type sont identiques à la moyenne et l’écart-type calculés.Dans cet exemple assez simpliste, nous voulons donc estimer la moyenne et l’écart-type. Fondamentale, peut les calculer l’un après l’autre assez aisément. Cependant, dans des situations plus complexes, ce ne sera pas le cas. Nous allons donc essayer de deviner la moyenne et l’écart-type sur la base des données :Les données sont comprises grosso modo entre 20 et 68. Il est raisonnable de considérer que la vraie moyenne se situe quelque part entre ces deux valeurs et que l’écart-type vaut environ \\(1/4\\) de cette distance puisque 95% des observations sont comprises entre -1.96 et +1.96 écart-types. Ainsi, je devine que la moyenne est à 44 et l’écart-type vaut 12.Je peux calculer la vraisemblance de cette estimation en calculant la densité de probabilité associée aux données (en considérant que les données suivent une distribution normale)Ainsi, pour chaque observation d’une variable numérique, peut estimer la densité de probabilité de cette observation sur la base d’une distribution normale (pour simplifier un peu, la hauteur de la courbe normale).La fonction de vraisemblance est obtenue en appliquant le logarithme à chacune des observations.obtient la vraisemblance en additionnant le logarithme des probabilités d’observer cette distribution.La valeur de la vraisemblance est ici particulièrement faible. Nous allons tenter de l’améliorer. Pour cela, je vais modifier les paramètres et déterminer si peut augmenter la vraisemblance. Par exemple, je peux tester si la moyenne n’est pas à 43 plutôt qu’à 44.La vraisemblance à augmenter. Je vais donc continuer et tenter de voir si la moyenne n’est pas à 42 au lieu de 43.La vraisemblance continue à augmenter. va donc continuer à estimer nos deux paramètres de manière itérative jusqu’à atteindre un maximum. Ce maximum sera atteint quand la moyenne testé sur la moyenne de nos données et l’écart-type sera l’écart-type des données :peut évidemment obtenir directement ces valeurs. Le package ‘EstimationTools’ permet de manipuler les différents paramères avec la fonction maxlog.Remarquez qu’il y une toute petite différence. Cela est dû au fait que l’estimation de l’écart-type est légèrement différente de celle calculée.Dans ce modèle, la vraisemblance est égale à -188.7698. Cette valeur n’est pas directement utilisable. Il est nécessaire de calculer la déviance en la multipliant par -2.","code":"\nmean(fleche)## [1] 41.88\nsd(fleche)## [1] 9.425627\nfleche##  [1] 40 53 37 47 35 32 47 41 53 35 56 47 43 55 40 38 43 23 36 58 46 34 37 39 29\n## [26] 45 46 29 54 42 38 36 50 50 32 37 40 44 41 59 33 55 47 51 46 31 43 51 11 39\ndnorm(fleche, mean= 44 , sd= 12, log=F)[1:10] # on obtient la densité de probabilité pour chaque observation ##  [1] 0.03144860 0.02509479 0.02804390 0.03222234 0.02509479 0.02016423\n##  [7] 0.03222234 0.03222234 0.02509479 0.02509479\ndnorm(fleche, mean= 44 , sd= 12, log=T)[1:10] # on obtient la fonction de vraisemblance pour chaque observation ##  [1] -3.459401 -3.685095 -3.573984 -3.435095 -3.685095 -3.903845 -3.435095\n##  [8] -3.435095 -3.685095 -3.685095\nsum(dnorm(fleche, mean= 44 , sd= 12, log=T)) ## [1] -186.0881\nsum(dnorm(fleche, mean= 43 , sd= 12, log=T)) ## [1] -185.5256\nsum(dnorm(fleche, mean= 42 , sd= 12, log=T)) ## [1] -185.3103\nsum(dnorm(fleche, mean= 39.72 , sd= 10.876, log=T)) ## [1] -184.6622\nlibrary(\"EstimationTools\")## Le chargement a nécessité le package : survival## \n## Attachement du package : 'survival'## L'objet suivant est masqué depuis 'package:boot':\n## \n##     aml## \n## ><<<<<<<<<<<<<<<<<<<<<<<<   EstimationTools Version 4.3.1   >>>>>>>>>>>>>>>>>>>>>>>><\n##   Feel free to report bugs in https://github.com/Jaimemosg/EstimationTools/issues\nfit1 <- maxlogL(x = fleche, dist = 'dnorm', start=c(2, 3),lower=c(20, 5), upper=c(65, 20)) \nfit1$fit$par##      mean        sd \n## 41.880000  9.330895\nfit1$fit$objective## [1] -182.6135\n-2*-188.9068 # calcul de la déviance## [1] 377.8136"},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"le-ratio-de-vraisemblance-lrt","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9.6.2 Le ratio de vraisemblance (LRT)","text":"Le ratio de vraisemblance (LRT) est la différence entre deux déviances. Cette différence se distribue approximativement comme un \\(\\chi^2\\) ayant comme degrés de liberté le nombre de paramètres estimés.\\[LRT = -2 [l(\\theta-l(\\hat\\theta))]\\]Ainsi, il faut se demander un \\(\\chi^2\\) de 2.851758 ayant 2 degrés de liberté est significatif.","code":"\nLRT<-(-2*(sum(dnorm(fleche, mean= 44 , sd= 12, log=T)) - sum(dnorm(fleche, mean= 39.72 , sd= 10.876, log=T)) ))\nLRT## [1] 2.851758\npchisq(LRT, df=2, lower=F) # calcule la probabilité de la valeur LRT avec 2 degrés de liberté, en donnant la probabilité d'avoir une valeur supérieure à cette valeur (lower = F)  ## [1] 0.2402972"},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"les-critères-dinformation.","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9.6.3 Les critères d’information.","text":"Les critères d’information fournissent une manière d’évaluer l’ajustement d’un modèle sur la base de la valeur optimale du log de vraisemblance tout en pénalisant les modèles qui ne sont pas parcimonieux.L’intérêt des critères d’information est qu’ils permettent de comparer n’importe quels modèles ajustés sur le même jeu de données. Il n’est donc pas nécessaire que les modèles soient emboîtés.Il existe deux critères d’information qui sont particulièrement utilisés : le AIC (Critère d’Informations d’Akaike) et le BIC (Critère d’Information de Bayes).Dans les deux cas, la logique est la même : va s’appuyer sur le calcul de la vraisemblance, mais en pénalisant le modèle à chaque paramètre estimé.Pour le AIC, la déviance va être pénalisée en additionnant 2 fois le nombre de prédicteur.\\[ AIC=-2×l(β ̂,θ ̂ )+2p \\]Dans notre modèle, le log de vraisemblance vaut 2.851758, la déviance vaut donc -2$$2.851758, ce qui donne -5.703516. Les degrés de liberté résiduels sont de 2. Le AIC vaut donc :qu’obtient directement avec la fonction AIC :Pour le critère d’information de Bayes, le principe est assez similaire, la différence porte sur la manière de calculer la pénalité pour le manque de parcimonie. Ainsi, le nombre de paramètres estimés sera multiplié par le logarithme de la taille de l’échantillon.\\[BIC=-2×l(β ̂,θ ̂ )+p×ln(n) \\]qu’obtient directement avec la fonction BIC :Puisque ces deux critères d’informations s’appuient sur la déviance pour le calcul, à laquelle va ajouter une pénalité aux modèles pour lesquels de nombreux paramètres sont estimés, cela signifie que, comme pour la déviance où, plus la valeur est faible, plus les données sont ajustées, plus la valeur pour ces deux critères d’informations est petite, meilleur est l’ajustement.","code":"\n-2*fit1$fit$objective + 2 * 2 # ou le second 2 représente le nombre de paramètres estimés ## [1] 369.2269\nAIC(lm(fleche~1)) # on réalise un modèle linéaire où on ne calcule que la constante, ce qui permet d'estimer la moyenne et la variance (ou l'écart-type)## [1] 369.2269\n-2*fit1$fit$objective + 2 * log(50, base=exp(1))## [1] 373.051\nBIC(lm(fleche~1))# on réalise un modèle linéaire où on ne calcule que la constante, ce qui permet d'estimer la moyenne et la variance (ou l'écart-type)## [1] 373.051"},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"lapproche-bayesienne","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9.7 L’approche bayesienne","text":"Comme nous l’avons évoqué, pour publier, il faut des résultats, résultats dits positifs. La raison de ce phénomène est que lorsqu’tolère l’hypothèse nulle, nous n’avons pas d’information sur le fait qu’n’pas assez d’information pour avoir un résultat significatif ou s’il n’y vraiment pas d’effet.Pour le formuler autrement l’absence de preuve n’est pas la preuve de l’absence. Cette difficulté est une des grosses limites des tests d’hypothèse nulle.Une autre critique fréquemment adressée aux tests d’hypothèse nulle est que le seuil à 0.05 est arbitraire, c’est pourquoi un certain nombre de chercheurs encouragent l’utilisation des facteurs bayesiens.","code":""},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"les-facteurs-bayesiens-un-rapport-entre-deux-vraisemblances","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9.7.1 Les facteurs bayesiens : un rapport entre deux vraisemblances","text":"Supposons un amphi pour lequel se demande si la parité est respectée. Dans cet amphi, il y 200 personnes, dont 160 femmes. La probabilité que la parité soit respectée s’obtient par une binomiale\\[ \\begin{pmatrix} 200 \\\\160  \\end{pmatrix} 0,5 ^{160} (1-0,5)^{40}\\ \\]et vautL’hypothèse alternative à cette hypothèse est que la parité n’est pas respectée.\nLa probabilité que la parité ne soit pas respectée (le ratio h/f est n’importe quelle autre valeur que 0,5) s’obtient par :\\[\\int_{0}^{1} \\begin{pmatrix} 200 \\\\160  \\end{pmatrix} p ^{160} (1-p)^{40}\\, \\mathrm{d}p \\]** note : pour être parfaitement précis, il faudrait retirer de l’intégrale la probabilité d’avoir 0.5)et vaut :Selon les tests d’hypothèse nulle, est amené à rejeter à la fois l’hypothèse de la parité et l’hypothèse selon laquelle la parité n’est pas respectée.Le facteur bayesien peut résoudre ce problème. En effet, il s’agit du rapport entre ces deux probabilités.Ainsi, sait quel est l’hypothèse la plus probable.Un facteur bayesien s’interprète de la manière indiquée dans le Tableau 4. Notez que ce tableau teste l’hypothèse nulle. Les critères doivent être inversés si teste l’hypothèse alternative.Cette approche permet donc de résoudre le problème des tests d’hypothèse nulle consistant à pouvoir tester une absence de différence.Pour le comprendre, prenons deux groupes de 1000 personnes ayant exactement la même moyenne (.e., 0) et le même écart-type (.e.,1).Pour comparer ces deux groupes, va classiquement réaliser un t de Student pour échantillons indépendants.Nous allons à présent pouvoir comparer l’évolution de la décision lorsque la taille d’échantillon augmente par rapport tant lorsqu’adopte un test d’hypothèse que lorsqu’utilise les facteurs bayesiens.Commençons pour regarder l’évolution de la probabilité, qui est présentée en Figure 10. se rend compte que peu importe la taille de l’échantillon, la probabilité associée au test de Student peut se situer entre n’importe quelle valeur sur une échelle allant de 0 à 1, avec un certain nombre de valeurs inférieures au seuil de significativité à 0.05, identifié par la droite horizontale. Ceci amène donc à la conclusion que peu importe la probabilité, nous ne sommes pas en mesure de tirer une conclusion.Si s’intéresse à présent à l’évolution du Facteur Bayes (Figure 11). constate que le FB ne dépasse jamais le seuil de 3 (ligne horizontale du haut) qui indique la présence d’une différence significative entre les deux groupes. En revanche, à partir d’un échantillon suffisamment élevé, le FB ne dépasse plus le seuil de 0.33 (la ligne horizontale du bas) qui indique des éléments en faveur de l’absence de différence. Il s’agit donc d’une grosse plus-value des facteurs bayesiens.Pour utiliser les facteurs bayesiens, applique le même raisonnement que pour les tests d’hypothèse nulle : donc si vous devez un t de Student, vous ferez un t de Student avec une approche bayesienne aussi.Le package R qui permet d’adopter cette approche est le package ‘BayesFactor’ (il en existe d’autres, mais ils sont un peu plus complexes d’utilisation).Nous pouvons reprendre notre exemple sur la force des super-héros pour illustrer la manière dont la fonction doit être utilisée.Vous avez peut-être identifié que, par rapport à la fonction du test de Student classique, il y un argument supplémentaire, qui est le rscale.Le r-scale correspond à la taille d’effet en valeur absolue que le chercheur estime pouvoir observer dans plus de 50% des situations. La valeur par défaut (“medium”) pour le test t est de 0.707.\nEn d’autres termes, dans les facteurs bayesiens, il faut explicitement préciser la taille d’effet. C’est ce qu’appelle le prior. Pour les utilisateurs des tests d’hypothèse nulle, il est raisonnable d’utiliser comme prior la valeur du d de Cohen qu’s’attend à observer (Lakens, 2018).Une autre différence par rapport aux tests d’hypothèse nulle classique est que la distribution qui est utilisée est une distribution Cauchy plutôt qu’une distribution normale. Cependant, comme le montre la Figure 12, la différence entre les deux distribution est suffisamment subtile pour ne pas devoir s’en soucier. En effet, il n’est pas moins ou plus raisonnable de faire l’hypothèse d’une distribution normale et d’une distribution Cauchy pour distribution des données.","code":"\np.h0<-dbinom(160, 200, 0.5,FALSE)\np.h0## [1] 1.275816e-18\nfdbinom<-function(p=NULL){\n         r<-dbinom(160, 200, p,FALSE)\n        return(r)\n        }\n\np.h1<-integrate(fdbinom, 0,1)\np.h1$value## [1] 0.004975124## 1.275816e-18 / 0.004975124 = 3.899561e+15\ngroupe1<-rnorm(1000, 0, 1)\ngroupe2<-rnorm(1000, 0, 1)\nlibrary(BayesFactor)## Le chargement a nécessité le package : coda## Le chargement a nécessité le package : Matrix## ************\n## Welcome to BayesFactor 0.9.12-4.7. If you have questions, please contact Richard Morey (richarddmorey@gmail.com).\n## \n## Type BFManual() to open the manual.\n## ************\nlibrary(ggplot2)\nlist.t<-c()\nlist.bf<-c()\nfor(i in 5:1000){\n  ttest<-t.test(groupe1[1:i], groupe2[1:i])\n  list.t<-c(list.t,  ttest$p.value)\n  bf<-ttestBF(x = groupe1[1:i], y = groupe2[1:i], rscale = \"medium\")\n  list.bf<-c(list.bf, extractBF(bf)$bf)\n}\nsimul<-data.frame(index=5:1000, \"Bayes.Factor\"=list.bf,\"test.t.p\"=list.t)\n\ncutoff.bf <- data.frame( x = c(-Inf, Inf), y =0.33, cutoff = factor(50) )\ncutoff.bf2<-data.frame( x = c(-Inf, Inf), y =3, cutoff = factor(50) )\np<-ggplot(simul, aes(x=index, y=Bayes.Factor))+geom_point()+\n  ylim(0,3.5)+geom_line(aes( x, y, linetype = cutoff ), cutoff.bf)+\n  geom_line(aes( x, y, linetype = cutoff ), cutoff.bf2)+\n  xlab(\"Taille de l'échantillon\") + ylab(\"Valeur du facteur bayesien\")+\n   theme(legend.position=\"none\")+\nlabs(title=\"Figure 11. Evolution de la valeur du Facteur Bayesien pour un t de Student \\n en fonction de la taille de l'échantillon.\")+\n  theme(axis.line.x = element_line(color=\"black\", size = 0.6),\n                         axis.line.y = element_line(color=\"black\", size = 0.6))+\n  theme(text = element_text(size=10))\ncutoff <- data.frame( x = c(-Inf, Inf), y = 0.05, cutoff = factor(50) )\np2<-ggplot(simul, aes(x=index, y=test.t.p))+geom_point()+\n   geom_line(aes( x, y, linetype = cutoff ), cutoff)+\n   xlab(\"Taille de l'échantillon\") + ylab(\"Valeur de la probabilité du test t\")+\n   theme(legend.position=\"none\")+\n labs(title=\"Figure 10. Evolution de la probabilité pour un t de Student \\n en fonction de la taille de l'échantillon.\")+\n  theme(axis.line.x = element_line(color=\"black\", size = 0.6),\n                         axis.line.y = element_line(color=\"black\", size = 0.6))+\n  theme(text = element_text(size=10))\n bf<-ttestBF(formula=Force~Groupe, data=super, rscale = \"medium\", paired=F)\nbf## Bayes factor analysis\n## --------------\n## [1] Alt., r=0.707 : 0.3628706 ±0%\n## \n## Against denominator:\n##   Null, mu1-mu2 = 0 \n## ---\n## Bayes factor type: BFindepSample, JZS"},{"path":"quel-outil-choisir.html","id":"quel-outil-choisir","chapter":"10 Quel outil choisir","heading":"10 Quel outil choisir","text":"“many statistical tests statisticians.”","code":""},{"path":"quel-outil-choisir.html","id":"lapproche-par-comparaison-de-modèles-ou-lapproche-par-test","chapter":"10 Quel outil choisir","heading":"10.1 L’approche par comparaison de modèles ou l’approche par test ?","text":"Quand il s’agit d’enseigner les statistiques, et en particulier, apprendre aux personnes à choisir l’outil adapté à leurs besoins, deux grandes approches peuvent être utilisées. La première consiste à penser les choses en termes de modèles. La seconde consiste à penser les choses en termes d’outils en fonction des variables qui doivent être analysées.Dans l’approche par modèle, l’idée est de pouvoir résoudre l’équation (10.1)\\[\\begin{equation}\nDonnées=Modèle + Erreur\n\\tag{10.1}\n\\end{equation}\\]Cette approche pour avantage qu’elle est élégante et synthétique pour expliquer les statistiques. Elle est également l’approche que vous devriez privilégier si vous préférez une approche bayésienne ou un approche de vraisemblance pour mener vos inférences. Cependant, ^pour les utilisateurs débutants, elle pourrait présenter deux écueils :l’objectif quand se forme aux statistiques est d’être capable de réaliser ses statistiques soi-même (ce que cette méthode permet), mais également de pouvoir comprendre ce que les autres ont fait (ce qui n’est pas possible si n’pas une culture sur les outils statistiques qui existent) ;si, pour les personnes qui ont déjà une bonne maîtrise des outils statistiques, cette approche est limpide, formaliser le modèle requiert des capacités d’abstraction concernant les données qui peuvent représenter un vrai challenge, en particulier pour les modèles plus complexes.Nous laisserons le lecteur choisir l’approche qui lui convient le mieux en indiquant dans les chapitre correspondant comment formaliser le modèle pour les différents cas de figures, et dans tous les cas, nous encourageons les lecteurs à pouvoir avoir suffisamment de flexibilité pour penser les statistiques des deux manières.Dans ce chapitre, nous proposerons un outil d’aide à la décision dont la finalité est de guider les utilisateurs des statitiques à utiliser l’outil qu’il sera le plus naturel d’utiliser en fonction de ses besoins. Il est important ici d’avoir identifié que le terme utilisé est naturel et non adapté. En effet, certains outils peuvent représenter de meilleures alternatives mais sont plus complexes à la fois en terme de conceptualisation théorique et en termes de mise en oeuvre pratique. Par exemple, les modèles linéaires mixtes représentent une meilleure alternative aux analyses de variances en mesures répétées, mais comprendre les modèles linéaires mixtes et le mettre en oeuvre de manière effective est plus complexe. Ainsi, dans le cadre de cet outil d’aide à la décision, des étudiant·es de licence ou de master pourront se satisfaire de l’analyse à privilégier, tandis que des doctorant·es ou des chercheur·ses devraient privilégier les outils proposés dans le point **remarques importantes* quand elles sont présentes.Avant d’aborder la manière d’utiliser cet outil, il faut avoir conscience qu’il est quasiment impossible de faire la liste complète des outils existants. Cet outil d’aide à la décision représente donc une option acceptable, robustesse et correspondant aux pratiques de la recherche en psychologie.","code":""},{"path":"quel-outil-choisir.html","id":"comment-utiliser-loutil-daide-à-la-décision","chapter":"10 Quel outil choisir","heading":"10.2 Comment utiliser l’outil d’aide à la décision ?","text":"La clé centrale pour identifier correctement la bonne analyse à réaliser est d’être capable d’identifier correctement les variables. Plusieurs étapes devront être menées à bien pour pouvoir être guidé vers la bonne analyse :Identifier le nombre de variables dépendantesIdentifier la nature de la variable dépendanteIdentifier le nombre de variables indépendantes et de contrôleIdentifier la nature des variables indépendantes et de contrôleSi les variables indépendantes sont qualitatives, il vous faudra aussi identifier le nombre de modalités de la variable indépendante ainsi que la nature inter11 ou intra-indivudelle12 de ces variables.Cet outil vous proposera alors l’analyse que vous devriez réaliser si vous êtes en licence ou master, fera un rappel sur les conditions d’application et proposera des alternatives à l’analyse de prédilectionà envisager dans l’hypothèse où les conditions d’application de l’analyse à privilégier ne sont pas respectées.Pour finir, des informations supplémentaires peuvent être fournies : des points d’attention ou des alternatives à envisager pour les personnes qui ont un niveau de maîrise des outils statistiques plus avancé.","code":""},{"path":"quel-outil-choisir.html","id":"une-seule-variable-dépendante","chapter":"10 Quel outil choisir","heading":"10.2.1 Une seule variable dépendante","text":"Étape 1 : identifier la nature de la variable dépendanteÉtape 2 : Identifier le nombre de variables indépendantesAnalyse à privilégier : score zCondition d’application :\n✓\n Normalité de la distributionQue faire en cas de non respect des conditions d’application ? :\n✓\n Utilisation des centilesPoints d’attention :\n✓\n Il est nécessaire d’avoir à disposition un échantillon de référence auquel peut comparer l’observationRemarque importante :\nSi l’usage, en particulier des cliniciens, du score z s’est imposé, c’est surtout en raison de sa facilité de calcul. Cependant, cette manière de faire est en réalité un usage impropre du score z car, une méthode qui autorise des inférences doit prendre en compte la taille de l’échantillon de référence sur lequel s’appuie, ce qui n’est pas le cas dans le cadre du score z (Crawford & Howell, 1998). C’est pourquoi l’utilisation du t modifé que Crawford et Howell (1998) proposent est une meilleure alternative. peut également utiliser des méthodes basées sur les régressions (Shirk et al., 2011; Timmerman et al., 2021)Analyse à privilégier : t de Student comparaison à une normeCondition d’application :\n✓\n Normalité de la distribution\n\n✓\n Indépendance des observationsQue faire en cas de non respect des conditions d’application ? :\n✓\n Test de WilcoxonPoints d’attention :\n✓\n Il est nécessaire de disposer d’une norme. Cette norme peut être fournie et peut représenter le niveau hasard quand, par exemple, peut répondre correctement à des questions en raison du hasard comme dans les cas de questions à choix multiples.Analyse à privilégier :\n✓\n Corrélation de Bravais-PearsonCondition d’application :\n✓\n Normalité du résidu\n✓\n Homogénéité des variances\n✓\n Indépendance des observationsQue faire en cas de non respect des conditions d’application ? :\n✓\n Rho de Spearman\n✓\n Tau de Kendall\n✓\n Présenter l’intervalle de confiance obtenu par boostrapAnalyse à privilégier :\n✓\n Rho de Spearman\n✓\n Tau de KendallCondition d’application :\n✓\n Indépendance des observationsPoints d’attention :Le rho de Spearman est également adapté lorsque la relation est croissante mais non linéaire.\n✓\n Présenter l’intervalle de confiance obtenu par boostrapÉtape 4 : identifier le nombre de modalités sur la variable indépendanteÉtape 5 : déterminer si la variable est intra ou inter-participant13Analyse à privilégier :\n✓\nt de Student pour échantillons appariésCondition d’application :\n✓\n Normalité du résidu\n✓\n indépendance des observationsQue faire en cas de non respect des conditions d’application ? :\n✓\n Test de Wilcoxon\n✓\n Test t sur les moyennes tronquéesAnalyse à privilégier :\n✓\nt de Student pour échantillons indépendantsCondition d’application :\n✓\n Normalité du résidu\n✓\n Homogénéité des variances\n✓\n Indépendance des observationsQue faire en cas de non respect des conditions d’application ?\n✓\n Si l’homogénéité des variances n’est pas respectée, le choix de prédiction est d’appliquer la correction de Satterthwaite14\n✓\n Test de Mann-Whitney\n✓\n Test t sur les moyennes tronquéesÉtape 5 : déterminer si la variable est intra ou inter-participantAnalyse à privilégier :\n✓\nAnova à mesure répétéeCondition d’application :\n✓\n Normalité du résidu\n✓\n Sphéricité de la matrice de covarianceAlternative en cas non respect des condition d’application :\n✓\n Si seule la sphéricité de la matrice de covariance n’est pas respectée, application de la correction de Greenhouse-Geisser (Greenhouse & Geisser, 1959) ou de Huynh-Feldt (Huynh & Feldt, 1976).\n✓\n Anova de Friedman\n✓\n Anova de Friedman\n✓\n Anova à mesure répétée sur les moyennes tronquéesPoints d’attention :La correction de Greehouse-Geisser est souvent appliquée de manière automatique. Elle se manifeste par une correction des degrés de libertéLa correction de Greehouse-Geisser est souvent appliquée de manière automatique. Elle se manifeste par une correction des degrés de libertéLes modèles linéaires mixtes sont, dans bien des situations, une meilleure alternative à l’anova à mesure répétée.Les modèles linéaires mixtes sont, dans bien des situations, une meilleure alternative à l’anova à mesure répétée.Analyse à privilégier : Anova simpleCondition d’application :\n✓\n Normalité du résidu\n✓\n Homogénéité des variances\n✓\n Indépendance des observationsAlternative en cas non respect des condition d’application :\n✓\n Correction de Box\n✓\nTest de Kruskal-Wallis\n✓\n Anova sur les moyennes tronquéesAnalyse à privilégier :\n✓\n Corrélation partielle de Bravais-PearsonCondition d’application :\n✓\n Normalité du résidu\n✓\n Homogénéité des variances\n✓\n Taille d’échantillon suffisanteAlternative en cas non respect des condition d’application :\n✓\nRho partiel de SpearmanPoint d’attention :\n✓\n si une ou plusieurs variables à contrôler sont ordinales, il est préférable de privilégier le rho partiel de Spearman\n✓\n Une alternative à envisager est la régression multiple, en particulier si veut voir l’impact de plusieurs variables indépendantes sur la variable dépendante. Dans ce cas, les analyses de régression hiérarchiques devraient être privilégiées.Analyse à privilégier :\n✓\n Rho partiel de Spearman (ou tau partiel)Condition d’application :\n✓\n Indépendance des observationsAnalyse à privilégier :\n✓\n Analyse de covarianceCondition d’application :\n✓\n Normalité du résidu\n✓\n Homogénéité des variances\n✓\n Sphéricité de la matrice de covariance si la variable indépendante est intra ET plus de deux modalités\n✓\n Indépendance des observationsAlternative en cas non respect des condition d’application :\n\n✓\n Analyse de covariance robustePoint d’attention :\n✓\n Il est en réalité statistiquement imprécis de considérer que l’analyse de covariance permet de contrôler l’effet de variables parasites. Seule une analyse de régression hiérarchique est en mesure de le faire.\n✓\n Les modèles linéaires mixtes représentent dans la très grande majorité des cas une meilleure alternative à l’analyse de covariance, en particulier s’il y une variable indépendante qualitative intra, et ne représente jamais une moins bonne option.Étape 3 : Identifier si les variables sont inter ou intra-participantsAnalyse à privilégier :\n✓\n Anova factorielle inter-participantsCondition d’application :\n✓\n Normalité du résidu\n✓\n Homogénéité des variances\n✓\n Indépendance des observationsAlternative en cas non respect des condition d’application :\n✓\n Anova à 2 ou 3 facteurs sur les médianesPoint d’attention :\n✓\n Les anovas factorielles permettent de tester les interactions entre les variables indépendantesAnalyse à privilégier :\n✓\n Anova factorielle intra-participantCondition d’application :\n✓\n Normalité du résidu\n✓\n Sphéricité de la matrice de covariance si au moins un des variables indépendantes plus de deux modalitésAlternative en cas non respect des condition d’application :\n✓\n Utilisation de bootstrap\n✓\n Modèles linéaires mixtes généralisésPoint d’attention :\n✓\n Les modèles linéaires mixtes représentent dans la très grande majorité des cas une meilleure alternative à l’anova à mesure répétée (factorielle) et ne représente jamais une moins bonne option.Analyse à privilégier :\n✓\n anova mixteCondition d’application :\n✓\n Normalité du résidu\n✓\n Sphéricité de la matrice de covariance si au moins une des variables indépendantes intra plus de deux modalitésAlternative en cas non respect des condition d’application :\n✓\n anova sur les moyennes tronquées\n✓\n Modèles linéaires mixtes généralisésPoint d’attention :\n✓\n Les modèles linéaires mixtes représentent dans la très grande majorité des cas une meilleure alternative à l’anova à mesure répétée et ne représente jamais une moins bonne option.Analyse à privilégier :\n✓\n régressions (appelées aussi modèles linéaires)Condition d’application :\n✓\n Normalité du résidu\n✓\n homogénéité des variances\n✓\n Absence de multicolinéarité\n✓\n Absence d’autocorrélationAlternative en cas non respect des condition d’application :\n✓\n régressions sur les M-estimators\n✓\n Utilisation de bootstrapPoints d’attention\n✓\n Dans les régressions linéaires multiples, fait l’hypothèse que la relation qui explique le mieux le lien entre la variable dépendante et les variables indépendantes sont des relations linéaires. Il est possible néanmoins de modéliser des relations non linéaires, comme des relations quadratiques ou cubiques. parle dans cas de régressions non linéaires.Analyse à privilégier :\n✓\n régressions (appelées aussi modèles linéaires)Condition d’application :\n✓\n Normalité du résidu\n✓\n homogénéité des variances\n✓\n Absence de multicolinéarité\n✓\n Absence d’autocorrélationAlternative en cas non respect des condition d’application :\n✓\n régressions sur les M-estimators\n✓\n Utilisation de bootstrapPoint d’attention\n✓\n si l’accent est plutôt mis sur l’impact de la variable indépendante qualitative et qu’veut contrôler les variables quantitatives, choisira plutôt l’analyse de covariance, même si en réalité, les deux approches reviennent au même.\n✓\n Dans les régressions linéaires multiples, fait l’hypothèse que la relation qui explique le mieux le lien entre la variable dépendante et les variables indépendantes sont des relations linéaires. Il est possible néanmoins de modéliser des relations non linéaires, comme des relations quadratiques ou cubiques. parle dans cas de régressions non linéaires.Étape 2 : Identifier le nombre de variables indépendantesAnalyse à privilégier :\n✓\n Utilisation des centilesPoints d’attention :\n✓\n Il est nécessaire d’avoir à disposition un échantillon de référence auquel peut comparer l’observationRemarque importante :\n✓\n Si l’usage, en particulier des cliniciens, des centiles s’est, comme pour le score z, imposé, c’est surtout en raison de sa facilité de calcul. Cependant, cette manière de faire est en réalité un usage impropre centiles car, une méthode qui autorise des inférences doit prendre en compte la taille de l’échantillon de référence sur lequel s’appuie, ce qui n’est pas le cas dans le cadre centiles. C’est pourquoi l’utilisation basées sur des modèles linéaires généralisés devraient être envisagés (Henry et al., 2023)Analyse à privilégier :\n✓\n Test de WilcoxonCondition d’application :\n✓\n Indépendance des observationsPoints d’attention :\n✓\n Il est nécessaire de disposer d’une norme. Cette norme peut être fournie et peut représenter le niveau hasard quand, par exemple, peut répondre correctement à des questions en raison du hasard comme dans les cas de questions à choix multiples.Analyse à privilégier :\n✓\nRho de Spearman\n✓\nTau de KendallCondition d’application :\n✓\n Indépendance des observationsPoints d’attention :\n✓\n Le rho de Spearman est également adapté lorsque la relation est croissante mais non linéaire.Analyse à privilégier :\n✓\n Rho de Spearman\n✓\n Tau de KendallCondition d’application :\n✓\n Indépendance des observationsPoints d’attention :\n✓\nLe rho de Spearman est également adapté lorsque la relation est croissante mais non linéaire.Étape 4 : identifier le nombre de modalités sur la variable indépendanteÉtape 5 : déterminer si la variable est intra ou inter-participantAnalyse à privilégier :\n✓\n Test de WilcoxonCondition d’application :\n✓\n indépendance des observationsAnalyse à privilégier :\n✓\n Test de Mann-WhitneyCondition d’application :\n✓\n Indépendance des observationsÉtape 5 : déterminer si la variable est intra ou inter-participantAnalyse à privilégier :\n✓\n Anova de FriedmanCondition d’application :\n✓\n AucunePoints d’attention :\n✓\n ne peut faire une anova de Friedman que pour les situations où aucune valeur n’est manquante.Analyse à privilégier :\n✓\n Test de Kruskal-WallisCondition d’application :\n✓\n Indépendance des observationsAnalyse à privilégier :\n✓\n Rho partiel de Spearman\n✓\n tau partiel dfe KendallCondition d’application :\n✓\n Indépendance des observationsPoint d’attention :\n✓\n Une alternative à envisager est le modèle linéaire généraliséAnalyse à privilégier :\n✓\nRho partiel de Spearman\n✓\n tau partiel de KendallCondition d’application :\n✓\n Indépendance des observationsPoint d’attention :\n✓\n Une alternative à envisager est le modèle linéaire généraliséAnalyse à privilégier :\n✓\n Modèles linéaires généralisésCondition d’application :\n✓\n Indépendance des observationsPoint d’attention :\n✓\n L’utilisation des modèles linéaires généralisés requiert d’être en mesure de choisir la bonne distribution.\n✓\n Les modèles linéaires mixtes généralisés représentent une alternative si la variable indépendante qualitative est intra-participant.Analyse à privilégier :\n✓\n Modèles linéaires généralisés s’il n’y aucune variable intra-participant\n✓\n Modèles linéaires mixtes généralisés s’il y au moins une variable intra-participantCondition d’application :\n✓\n Indépendance des observationsPoint d’attention :\n✓\n L’utilisation des modèles linéaires généralisés requiert d’être en mesure de choisir la bonne distribution.\n✓\n Les modèles linéaires mixtes généralisés représentent une alternative si la variable indépendante qualitative est intra-participant.Étape 2 : Identifier le nombre de variables indépendantesAnalyse à privilégier :\n✓\nkhi² d’ajustementCondition d’application :\n✓\n Indépendance des observations\n✓\n Pour le khi² d’ajustement, les effectifs théoriques doivent être supérieurs à 5 dans au moins 80% des cases du tableauPoints d’attention :\n✓\n Il est nécessaire de disposer d’un modèle théorique. Ce modèle théorique peut être une répartition équiprobable entre les différentes modalités\n✓\n Si les conditions d’applications ne sont pas respectées, peut envisager de regrouper des modalités si cela fait sens.Analyse à privilégier :\n✓\nModèle linéaire généralisé\n✓\nrégression logistique (il s’agit d’un cas particulier du modèle linéaire généralisé où la variable dépendance à 2 modalités)Condition d’application :\n✓\n Indépendance des observationsAnalyse à privilégier :\n✓\n khi² d’indépendance\n✓\n Modèle linéaire généraliséCondition d’application :\n✓\n Pour le khi² d’indépendance, les effectifs théoriques doivent être supérieurs à 5 dans au moins 80% des cases du tableau\n✓\n Indépendance des observationsQue faire en cas de non respect des conditions d’application ? :\n✓\n Fisher exact testÉtape 4 : identifier le nombre de modalités sur la variable indépendanteÉtape 5 : déterminer si la variable est intra ou inter-participantAnalyse à privilégier :\n✓\n Test de McNemarCondition d’application :\n✓\n La somme des deux cases pour lesquelles il y eu un changement doit être supérieure à 10\n✓\n indépendance des observationsAnalyse à privilégier :\n✓\n khi² d’indépendanceCondition d’application :\n✓\n Indépendance des observations\n✓\n Pour le khi² d’indépendance, les effectifs théoriques doivent être supérieurs à 5 dans au moins 80% des cases du tableauQue faire en cas de non respect des conditions d’application ? :\n✓\n Fisher exact testÉtape 5 : déterminer si la variable est intra ou inter-participantAnalyse à privilégier :\n✓\n Modèles linéaires mixtesCondition d’application :\n✓\n AucuneAnalyse à privilégier :\n✓\n khi² d’indépendanceCondition d’application :\n✓\n Indépendance des observations\n\n✓\n Pour le khi² d’indépendance, les effectifs théoriques doivent être supérieurs à 5 dans au moins 80% des cases du tableauQue faire en cas de non respect des conditions d’application ? :\n✓\n Fisher exact testAnalyse à privilégier :\n✓\n Modèles linéaires généralisésCondition d’application :\n✓\n Indépendance des observations\nPoint d’attention :\n✓\n L’utilisation des modèles linéaires généralisés requiert d’être en mesure de choisir la bonne distribution.Analyse à privilégier :\n✓\n Modèle linéaire généraliséCondition d’application :\n✓\n Indépendance des observationsPoint d’attention :\n✓\n L’utilisation des modèles linéaires généralisés requiert d’être en mesure de choisir la bonne distribution.Analyse à privilégier :\n✓\nModèles linéaires généralisésCondition d’application :\n✓\n Indépendance des observationsPoint d’attention :\n✓\n L’utilisation des modèles linéaires généralisés requiert d’être en mesure de choisir la bonne distribution.Analyse à privilégier :\n✓\nModèles linéaires généralisésCondition d’application :\n✓\n Indépendance des observationsPoint d’attention :\n✓\n L’utilisation des modèles linéaires généralisés requiert d’être en mesure de choisir la bonne distribution.","code":""},{"path":"quel-outil-choisir.html","id":"plusieurs-variables-dépendantes","chapter":"10 Quel outil choisir","heading":"10.2.2 Plusieurs variables dépendantes","text":"Les situations qui ont été décrites pour les analyses de variances, les modèles linéaires, les corrélations, les modèles linéaires généralisés et les modèles linéaires mixtes ont des équivalents multivariés où l’objectif est de déterminer comment les variables dépendantes impactent un ensemble de variables dépendantes simultanément.Ainsi, l’outil d’aide à la décision décrit plus haut peut être retranscrit dans le Tableau 10.1 :Table 10.1: Correspondance multivariée des analyses univariéesAnalyse univariéeÉquivalent multivariéanova, ancovamanovacorrélationsanalyse canonique de corrélationsrégressionsrégressions multivéesmodèles linéaires (mixtes) généralisésmodèles linéraires (mixtes) généralisés multivariés","code":""},{"path":"quel-outil-choisir.html","id":"les-analyses-statistiques-sur-de-petits-échantillons","chapter":"10 Quel outil choisir","heading":"10.2.3 Les analyses statistiques sur de petits échantillons","text":"Il est très fréquents que les psychologues aient à gérer des situations où l’échantillon est de taille restreinte. Dans ce genre de situation, plutôt que de vouloir appliquer des analyses inférentielles décrites ci-dessus, il est utile d’envisager de traiter les données sous l’angle des analyses de cas unique. Il est important néanmoins de faire les choses dans l’ordre et d’être au clair sur les contraintes qu’impliquent ce type d’analyse avant de commencer le recueil pour éviter d’être dans une situation où ces outils seront inexploitables.","code":""},{"path":"quel-outil-choisir.html","id":"les-outils-statistiques-relevant-de-la-méta-science","chapter":"10 Quel outil choisir","heading":"10.2.4 Les outils statistiques relevant de la méta-science","text":"Il semble utile de préciser que certains outils statistiques permettent d’aborder les questions scientifiques d’une manière plus globale.Ainsi, quand veut pouvoir faire une synthèse de l’ensemble des résultats d’une question particulière, la méta-analyse est l’outil de prédilection. Cependant,les résultats des méta-analyses peuvent être trompeur s’il existe un biais de publication. Les courbe p (p-curve en anglais) représente un outil complémentaire qui permettent d’évaluer ce biais.L’autre panoplie d’outils qui méritent également toute notre attention sont les outils qui permettent de développer des modèles et de les tester. S’il existe différents noms en fonction de la manière de formaliser le modèle, peut raisonnablement considérer que l’ensemble des analyses appartiennent à la famille des modèles d’équation structurale.","code":""},{"path":"quel-outil-choisir.html","id":"les-outils-statistiques-relevant-de-la-psychométrie","chapter":"10 Quel outil choisir","heading":"10.2.5 Les outils statistiques relevant de la psychométrie","text":"Le développement d’outils psychométriques requiert d’évaluer toute une série de propriété de ces outils, que ce soit en termes de validité, de fiabilité mais également en termes de leur valeur prédictive. Dans bien des situations, va s’appuyer sur des corrélations pour examiner ces propriétés mais certains outils sont néanmoins plus élaborés. Il ne semble pas opportun de développer dans ce chapitre les différentes moyens qui peuvent être envisagés pour examiner ces propriétés. Il semble plus pertinent de s’orienter vers le chapitre correspondant de cet ouvrage.","code":""},{"path":"quel-outil-choisir.html","id":"les-autres-outils-qui-sappuient-sur-les-statistiques","chapter":"10 Quel outil choisir","heading":"10.2.6 Les autres outils qui s’appuient sur les statistiques","text":"Que ce soit l’intelligence artificielle ou le machine learning, ces outils s’appuient peu ou prou sur des outils statistiques que nous avons déjà décrits. Si ces outils sont de plus en plus utilisés, y compris par des chercheurs en psychologie, il ne semble pas opportun de les décrire ici en détails dans la mesure où il existe une multitude d’algorithme et d’outils qui peuvent être utilisés et en faire une liste exhaustive aurait un effet plus délétère que formateur. Nous limiterons cette section à quelques banalités qui pourront éventuellement éveiller la curiosité du lecteur si ce type d’outils correspond à ses besoins.Parmi ces banalités, la première chose à comprendre est que l’objectif de ces outils de machine learning (parfois aussi appelé apprentissage statistique) est de prédire les nouvelles observations sur la base des informations relatives à ces observations. Par exemple, si s’intéresse à la maladie d’Alzheimer, pourrait se demander, sachant qu’une personne à 75 ans, est fumeur, consommé l’équivalent de 4 verres de vins tous les jours de sa vie, et obtenu le bac, quelle est la probabilité que cette personne développe une maladie d’Alzheimer.Dans ce domaine, il existe deux grandes catégories d’outils : l’apprentissage supervisé pour lequel fournit des données permettant au modèle d’apprendre à prédire le score ou la catégoriser d’une personne (telle personne telles caractéristiques et appartient à la catégorie , telle autre personne présente telles autres caractéristiques et appartient à la catégorie B…) et l’apprentissage non supervisé où ne fournit pas au modèle d’informations sur ce qu’il doit trouver, parfois parce que le chercheur ne le sait pas lui-même. Les outils basés sur les régressions ou l’analyse discriminante font partie de la première catégorie, tandis que les analyses en cluster font partie de la seconde.","code":""},{"path":"références.html","id":"références","chapter":"11 Références","heading":"11 Références","text":"","code":""}]
