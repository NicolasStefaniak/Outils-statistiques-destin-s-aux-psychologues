[{"path":"index.html","id":"section","chapter":"","heading":"","text":"","code":""},{"path":"pourquoi-les-statistiques.html","id":"pourquoi-les-statistiques","chapter":"1 Pourquoi les statistiques","heading":"1 Pourquoi les statistiques","text":"“Statistical thinking one day necessary efficient citizenship ability read write. (Un jour, la pensée statistiques sera aussi indispensable pour pouvoir être un citoyen engagé qu’être capable de lire et d’écrire)”","code":""},{"path":"pourquoi-les-statistiques.html","id":"á-quoi-servent-les-statistiques","chapter":"1 Pourquoi les statistiques","heading":"1.1 Á quoi servent les statistiques ?","text":"Vous êtes-vous déjà demandé pourquoi vous aviez des cours de statistiques ? Avez-vous déjà pensé ou entendu des camarades dire « je ne vois pas pourquoi de tels cours, moi je veux être psy (ou biologiste, chimiste, médecin…) » Vous avez sans doute déjà pensé ou entendu « de toute façon, j’ai jamais rien compris aux maths, je veux juste arriver à compenser », voire affirmé avec force « les statistiques sont là pour nous éliminer et pour nous démotiver ». Au mieux, certain·es voulaient faire preuve de lucidité « en aura besoin pour notre TER », mais leur voix furent rapidement étouffée par « de toute façon, n’en aura jamais besoin dans notre vie pro ».Cependant, la véritable raison des cours de statistiques est bien plus inavouable que vous ne l’imaginez. Pour la comprendre, il faut se poser une question : à quoi sevent les statistiques ? Prenez le temps d’y répondre par vous-même avant de continuer votre lecture.Beaucoup de personnes considèrent que les statistiques permettent de rendre objectif, scientifique, indiscutable des faits qui sont présentés. Cependant, cette vision des statistiques est très étriquées et ne correspond, la plupart du temps, pas à la réalité. L’objectif premier des statistiques de faire passer un message, de convaincre et d’influencer.Comparez ces différentes phrases et questionnez-vous sur leur caractère persuasif :Il y de plus en plus de personnes au chômage ;Depuis 2023, le taux de chômage doublé ;Depuis 2023, le taux de chômage est passé de 4% à 8% ;Depuis 2023, le taux de chômage est passé de 4% à 8%, ce qui représente un niveau historiquement haut qui n’plus été atteint depuis 1975 ;En deux ans, le taux de chômage doublé, passant de 4% à 8%, cette augmentation est la plus forte enregistrée dans des délais aussi court depuis les années 1970.constate que les deux dernières propositions ont un pouvoir persuasif bien plus important et finalement, qui ira vérifier ? L’important est d’être convaincant, pas que ce soit vrai.Cet usage des statistiques est, qu’le veuille ou non, extrêmement fréquente. Il est donc nécessaire de pouvoir avoir un regard critique sur ce que les chiffres signifient pour pouvoir se forger se propre opinion. En l’absence de cette capacité, peut se trouver face à deux sortes d’écueils tout aussi graves l’un que l’autre :accepter sans condition le message transmis, sans que notre esprit critique n’ait quoique ce soit à y redire ;raisonner sur la base d’une incompréhension des statistiques et aboutir à des conclusions qui sont fausses.Au travers de quelques exemples, nous allons explorer comment les statistiques peuvent être utilisées pour duper les esprits qui y sont hermétiques.","code":""},{"path":"pourquoi-les-statistiques.html","id":"lefficacité-du-vaccin-contre-la-covid","chapter":"1 Pourquoi les statistiques","heading":"1.1.1 L’efficacité du vaccin contre la covid","text":"Durant la crise covid qui eu lieu entre 2020 et 2022, la recherche d’un traitement efficace s’est avéré être d’une importance capitale pour pouvoir reprendre un mode de fonctionnement identique à celui qui précédait le confinement.Parmi ces traitements, le vaccin à ARN s’est vite imposé comme étant une solution fiable et robuste pour lutter contre les dégâts de la maladie. Cependant, à cette époque, de nombreuses critiques issues des antivax se faisaient entendre. Parmi les raisonnements les plus fréquents, pouvait avoir celui-ci : la preuve que le vaccin ne fonctionne pas est qu’il y autant de personnes qui sont hospitalisées à cause de la covid en étant vaccinée que de personnes qui ne le sont pas.Si analye les chiffres bruts, les antivax ont raison : il y autant de personnes vaccinées que de personnes vaccinées, comme le montre la Figure 1.1.\nFigure 1.1: Nombre de personnes hospitalisées en fonction du statut de vaccination\nCependant, un élève de CM1 sait ce qu’est la notion de proportion. Ainsi, ne peut comparer le nombre de personnes vaccinées de personnes non vaccinées qu’à la condition d’avoir autant de personnes vaccinées et non vaccinées, ce qui n’était pas le cas. En effet, il y avait environ 10 fois plus de personnes vaccinées que de personnes non vaccinées. Ainsi, dans la Figure 1.2, le cadre indique les personnes qui ont été hospitalisées et se rend compte que proportionnellement au nombre de personnes non vaccinées, les personnes vaccinées étaient moins souvent hospitalisées, ou pour le formuler autrement le rapport entre les verts en dehors du cadre sur ceux qui sont dans le cadre est plus important que le rapport des rouges en dehors du cadre par rapport à ceux qui sont à l’intérieur du cadre.\nFigure 1.2: Nombre de personnes hospitalisées vs non hospitalisation en fonction du statut de vaccination\n","code":""},{"path":"pourquoi-les-statistiques.html","id":"la-suède-et-le-paradis-des-violeurs","chapter":"1 Pourquoi les statistiques","heading":"1.1.2 La Suède et le paradis des violeurs","text":"","code":""},{"path":"pourquoi-les-statistiques.html","id":"version-standard","chapter":"1 Pourquoi les statistiques","heading":"1.1.2.1 Version standard","text":"En 2015, pendant la campagne présidentielle aux USA, un site d’information d’extrême droite appelé Breitbart publié un article qui incitait les citoyens américains à regarder ce qui se passait en Suède de la manière suivante. La particularité de cet article était qu’il s’appuyait sur une utilisation trompeuse des statistiques pour nourrir la peur, la haine et l’exclusion. Concrètement, ils effectuaient une comparaison brute de chiffres qui ne renvoyaient pas aux mêmes réalités.Concrètement l’article utilisait les statistiques de viol pour dénoncer des sociétés ouvertes à l’immigration, en particulier la Suède en tenant le raisonnement suivant : ce pays présente l’un des taux de viol les plus élevés du monde occidental ; cette situation serait la conséquence directe de l’accueil d’immigrés issus de cultures supposément plus violentes. Une telle conclusion peut sembler convaincante à première vue, mais elle repose sur une incompréhension profonde de ce que mesurent réellement ces chiffres.Comparer les taux de viol entre pays n’de sens que si l’compare des indicateurs construits de manière identique. ce n’est pas le cas. Comme l’explique von Hofer (Hofer, 2000), les statistiques de viol sont des constructions sociales et juridiques avant d’être des reflets directs de la réalité criminelle. Ainsi, en Suède, toute plainte pour viol est comptabilisée comme telle, y compris lorsque l’affaire est ultérieurement classée sans suite ou requalifiée. Dans de nombreux autres pays, seules les condamnations judiciaires sont prises en compte. Ces deux indicateurs ne mesurent donc pas la même chose et cela ne fait dès lors pas sens de comparer les taux de viols entre des pays qui n’utilisent pas la même définition pour les comptabiliser.En outre, s’ajoutent des différences majeures dans les définitions légales. La notion de viol est plus large en Suède que dans beaucoup d’autres pays : des faits qui seraient qualifiés d’« abus sexuel » ailleurs, notamment en France, entrent dans la catégorie du viol en Suède. De plus, lorsqu’une même victime subit des viols répétés par le même agresseur, comme c’est le cas lorsqu’un mari viole sa femme, chaque acte est comptabilisé séparément dans les statistiques suédoises, alors qu’ils sont souvent regroupés en un seul événement dans d’autres pays.Les facteurs culturels jouent également un rôle déterminant. La propension à déclarer un viol dépend fortement du contexte social, du regard porté sur les victimes et des risques encourus après une plainte. Dans certaines sociétés, dénoncer un viol expose les victimes à la stigmatisation, à des représailles, voire à des violences graves. Dans ces conditions, des taux officiellement faibles ne traduisent pas une moindre prévalence des violences sexuelles, mais un sous-signalement massif. À titre d’illustration, une enquête montre qu’environ un quart des Français considèrent que l’auteur d’un viol est moins responsable si la victime portait une tenue jugée sexy (Le Breton, 2016), ce qui donne la mesure des freins culturels à la dénonciation, y compris dans des pays occidentaux.Les écarts entre statistiques officielles et données issues de la recherche scientifique renforcent ce constat. Koss et al. (1987), dans une enquête menée auprès d’étudiantes américaines, montrent que plus de la moitié rapportent avoir subi des abus sexuels au sens large, et qu’environ 15 % déclarent avoir été violées. Ces chiffres sont sans commune mesure avec ceux issus des statistiques policières ou judiciaires, ce qui illustre à quel point les données administratives sous-estiment la réalité des violences sexuelles.Ainsi, affirmer qu’un pays serait plus dangereux qu’un autre en se fondant uniquement sur des taux de viol publiés sans analyse méthodologique revient à tirer des conclusions infondées. Les chiffres ne sont pas des faits bruts : ils sont le produit de définitions juridiques, de pratiques institutionnelles et de contextes culturels. Sans cette compréhension, les comparaisons internationales deviennent non seulement invalides, mais dangereuses, car elles peuvent servir de support à des discours idéologiques qui instrumentalisent la peur.Comprendre les statistiques, c’est apprendre à se demander ce qui est réellement mesuré, comment cela l’est, et ce que les chiffres ne disent pas. C’est à cette condition seulement que les données peuvent éclairer le débat public au lieu de l’obscurcir.","code":""},{"path":"pourquoi-les-statistiques.html","id":"version-conte-de-fées","chapter":"1 Pourquoi les statistiques","heading":"1.1.2.2 Version conte de fées","text":"Il était une fois une forêt, appelée la forêt de Breitbart. Dans la forêt de Breitbart vivait une femme. dit d’elle qu’elle avait des pouvoirs magiques. l’appelait la sorcière de Breitbart. Cette sorcière n’avait pas plus de baguette magique qu’elle ne préparait de potion à base de griffes de dragons. Son pouvoir était bien plus sournois. Le pouvoir de cette sorcière était l’obscurantisme. Elle se complaisait dans la haine et l’exclusion. Elle avait ce pouvoir d’influencer et de manipuler l’esprit des plus fragiles. Elle se montrait patiente et efficace en attendant de pouvoir semer la graine de la haine dans le coeur des malheureux qui osaient s’aventurer dans cette ténébreuse forêt. Il arrivait que des promeneurs venus du monde entier passaient par la forêt de Breitbart et en ressortaient profondément changés : des agneaux transformés en loups. Quand traversait cette forêt, il y avait comme un souffle, un murmure qui caressait les oreilles des passants. Il était si facile à écouter. C’était tellement apaisant et agréable d’entendre « vous êtes les meilleurs, la race supérieure, vous devez vous défendre » ou encore « montrer la supériorité des hommes sur les femmes » (l’auteur de ces lignes ayant des nausées en écrivant la manière de penser de cette sorcière s’est vu contraint de ne pouvoir donner d’autres exemples au risque d’être atteint de vomissements durables).La perfidie de cette sorcière était sans limite. Elle éprouvait une haine sans fin pour un pays appelé Ouvertuland. Dans ce pays, les hommes considéraient les femmes comme leurs égales, les habitants étaient convaincus que l’endroit de notre naissance n’était pas un choix, mais un état de fait et que chacun avait le droit d’aspirer à un monde meilleur lorsque la guerre et les persécutions frappaient, que la souffrance n’était pas tolérable, qu’importe l’endroit où était né.La sorcière de Breitbart étalait cette haine au grand jour. Elle criait à qui voulait l’entendre : « Ecoutez-les se plaindre, regardez ce qui se passe là-bas, ils accueillent les étrangers, leurs femmes et leurs filles se font violer ; ne laissons pas des immigrés faire, protégeons-nous ; nous sommes supérieurs, ils ne méritent pas notre plus petite considération ». Regardez ce qu’il s’y passe, leur taux de viol est le plus élevé du monde occidental (et si vous faites une petite recherche sur le taux de viol en Suède notre statistique d’intérêt, vous trouverez que, en 2010, les statistiques de viol montrent que la Suède arrive en \\(3^{ème}\\) position derrière l’Afrique du Sud et le Botswana et si vous vous dites tout cela pour cela, vous auriez raison), tout cela parce qu’ils ont laissé des immigrés ayant une culture du viol entrer dans leur pays, voyez ce qui s’y passe et ne surtout ne faisons pas comme eux ».Et les esprits fragiles (dont vous ne faites pas partie… ou dont vous ne ferez bientôt plus partie) se laisseraient évidemment envahir par ces pensées, et le coeur des hommes vaillants deviendrait alors sombre et froid, intolérant et xénophobe. Comment était-il possible que ce pays tant admiré pour ses avancées sociales soit l’objet d’une situation aussi abjecte ? Et les passants qui entraient dans la forêt de Breitbart en sortaient avec la certitude que les immigrés étaient la cause de tous les problèmes de ces peuples trop ouverts, trop tolérants, trop naïfs.Un jour, un homme qui était connu sous le nom de Gandwells traversa la forêt de Breitbart. La sorcière de Breitbart exerça son pouvoir maléfique contre Gandwells. Le coeur de Gandwells vacilla, la haine commençait à le submerger, mais Gandwells avait l’esprit vif et incisif. Il concentra ses pensées pour tenter de combattre les pouvoirs de la sorcière. Il lui dit : « Donalda (c’était le prénom de la sorcière de Breitbart, et à nouveau toute ressemblance avec des personnes existant ou ayant existé est purement … volontaire), peux-tu m’expliquer comment les pays dont sont issus ces immigrés ont des taux de viols bien plus faibles que les pays dont proviennent ces immigrés ? » (Vous pouvez constater que des pays comme le Pakistan, l’Albanie ou l’Azerbaïdjan ont des taux de viols particulièrement faibles, alors que ce sont des pays qui correspondent aux critères dénoncés par Breitbart). La sorcière Breitbart argumenta :« C’est parce que, dans ces pays, le viol est tellement commun, qu’ils ne les comptabilisent même pas. »Gandwells demanda alors :« Et dans les autres pays, comment le taux de viol est-il calculé ? ».La sorcière venait d’être touchée, elle tenta de bredouiller mal à l’aise« Ben, euh, comme partout.-Je ne suis pas sûr de comprendre, reprit Gandwells. Vous avez dit que certains pays ne comptabilisent pas les viols, car c’est dans leur nature d’en perpétrer, et puis vous dites que tout le monde comptabilise les viols de la même manière… »La sorcière se tut un instant, ne sachant que répondre. Elle tenta :« Ça n’strictement rien à avoir. Ce n’est pas pareil.»Gandwells ignora la sorcière et continua sa démonstration :« En réalité, je crois que le peuple d’Ouvertuland est particulièrement attentif au respect des êtres-humains, et du corps des femmes (Je focalise sur les femmes car la très grande majorité des viols concerne les femmes). peut penser qu’ils ne comptabilisent pas du tout les viols de la même manière » [Von Hofer (2000) explique en détail pourquoi les statistiques de viol ne peuvent pas être comparées d’un pays à l’autre. En effet, en Suède, est comptabilisé comme viol, toute plainte pour viol, même si par la suite l’affaire est classée ou que les charges sont réduites. Dans la plupart des autres pays, les statistiques de viols portent sur les condamnations uniquement. De même, en Suède, une personne qui va être violée 30 fois par son mari va compter comme 30 viols dans les statistiques, alors que, dans la plupart des autres pays, ce ne sera comptabilisé qu’une seule fois étant donné que ce sont les mêmes personnes qui sont impliquées dans chacun des viols. Par ailleurs, la définition de viol est plus large en Suède que dans d’autres pays. Ainsi, ce qui serait classé comme un abus sexuel dans un autre pays, serait considéré comme viol en Suède. peut rajouter à cela que la culpabilisation des femmes dans la dénonciation des viols est très dépendante de la culture, avec dans certains pays, un réel risque d’être persécutée, voire tuée. Pour comprendre cet effet de la culture, 1/4 des Français estiment que l’auteur d’un viol est moins responsable si la victime portait une tenue sexy (Le Breton, 2016). Il n’est dès lors pas étonnant de ne pas oser dénoncer un viol dans ces conditions. La Suède étant très en avance sur la considération de l’être humain, les femmes reportent plus facilement les abus sexuels dont elles ont été victimes. Pour illustrer encore plus ces différences, sans entrer dans le débat sur la manière dont doit comptabiliser les taux de viols, peut regarder les différences qui existent entre les statistiques officielles et celles issues des articles scientifiques, comme le montre l’article de Koss et al. (1987) qui montre que 53% des étudiantes américaines ont été victimes d’abus sexuels au sens large, et 15% rapportent avoir été violées, ce qui ne correspond absolument pas aux chiffres très inférieurs publiés par les autorités des pays].C’est ainsi que Gandwells mena son combat contre l’obscurantisme en illustrant avec force et conviction que les nombres ne renvoyaient pas aux mêmes réalités et que vouloir comparer les statistiques criminelles entre les pays et Ouvertuland n’avait pas de sens !La sorcière de Breitbart se réfugia dans une grotte cachée au fin fond de sa forêt, humiliée par Gandwells, mais le combat n’était pas gagné pour autant, le plus dur restait à venir. Il fallait que le monde sache la perfidie de Donalda, il fallait que le monde ne se laisse pas manipuler par cette sorcière, il fallait apprendre aux hommes à avoir l’esprit vif et incisif, à se poser les bonnes questions. C’était la seule issue.","code":""},{"path":"pourquoi-les-statistiques.html","id":"pourquoi-les-statistiques-sont-indispensables-pour-de-futurs-psychologues","chapter":"1 Pourquoi les statistiques","heading":"1.2 Pourquoi les statistiques sont indispensables pour de futurs psychologues ?","text":"En tant que futur3es psychologues, vous serez amené·e à utiliser des statistiques de manière récurrente dans plusieurs contextes :Identifier les propriétés psychométriques des outils que vous utilisez. En l’absence de cette analyse, vous risquez d’utiliser des outils dont les fondements scientifiques sont largement insuffisants (Lilienfeld et al., 2000; Strauss et al., 2006) ;Identifier les propriétés psychométriques des outils que vous utilisez. En l’absence de cette analyse, vous risquez d’utiliser des outils dont les fondements scientifiques sont largement insuffisants (Lilienfeld et al., 2000; Strauss et al., 2006) ;Être capable de réaliser des inférences relatives au niveau de fonctionnement des usagers auxquels vous serez confronté·e, c’est-à-dire déterminer si l’usage présente un déficit, un trouble, un risque pour la société … selon l’environnement de travail dans lequel vous travaillerez ;Être capable de réaliser des inférences relatives au niveau de fonctionnement des usagers auxquels vous serez confronté·e, c’est-à-dire déterminer si l’usage présente un déficit, un trouble, un risque pour la société … selon l’environnement de travail dans lequel vous travaillerez ;Vous former en lisant des articles scientifiques, des livres ou des rapports, ou en suivant des formations. qu’importe le moyen vous devrez pouvoir vous demander “comment est-ce que cela été montré ?” et “est-ce que cette manière de le montrer est convaincante ?”.Vous former en lisant des articles scientifiques, des livres ou des rapports, ou en suivant des formations. qu’importe le moyen vous devrez pouvoir vous demander “comment est-ce que cela été montré ?” et “est-ce que cette manière de le montrer est convaincante ?”.Ainsi, la nature même d’un cours de statistiques n’est pas de vous apprendre une suite interminable d’outils dont vous devrez maîtriser les tenants et les aboutissants mais d’identifier les fondements qui sont communs à l’ensemble des outils statistiques afin de pouvoir donner du sens aux nombres.","code":""},{"path":"pourquoi-les-statistiques.html","id":"exercice","chapter":"1 Pourquoi les statistiques","heading":"1.3 Exercice","text":"En 2020, la covid19 fait rage, la population est confinée et aucun traitement ne semble efficace. Des milliers de personnes meurent tous les jours dans le monde à cause de cette infection. Dans cette course effrénée visant à rechercher un traitement, Raoult et son équipe ont publié une étude qui vantait l’efficacité de l’hydroxychloroquine associée à l’azythromycine pendant au moins trois jours pour lutter efficacement contre la covid-19 (Lagier et al., 2020). Imaginez que vous, ou une personne qui vous est chère, développe une forme grave de la covid19. Allez-vous prendre ce remède miracle ? Afin d’éclairer votre réflexion, vous trouverez dans la Figure 1.3 et dans le Tableau 1.4 et 1.5 les informations nécessaires pour prendre votre décision. Il est à noter que pour analyser ces résultats, nul besoin d’avoir des compétences en médecine. Les notions qui sont enseignées dans un cours d’introduction à la méthodologie, quelques connaissances d’ordre général et un peu de logique suffisent amplement à se faire une opinion.\nFigure 1.3: Diagramme de flux des participants permettant de représenter les allocations entre les différents groupes.\n\nFigure 1.4: Tableau des données démographiques.\n\nFigure 1.5: Tableau des résultats.\nNormalement, vous devriez avoir remarqué des problèmes qui remettent considérablement en cause les conclusions de Raoult et de ses collaborateurs.Une des remarques qui est souvent faite est que l’effectif n’est pas le même dans les différents groupes. En réalité, ce point ne pose pas réellement souci car il existe des outils statistiques en mesure de prendre cet aspect en compte. Par ailleurs, si de bonnes raisons de penser qu’un traitement est efficace, il serait éthiquement douteux de ne pas le proposer à une plus grande proportion de personnes que celles qui ne le reçoit pas.En revanche, ce qu’observe :la proportion de personnes hospitalisées qui reçoivent le traitement (13.8%) est largement inférieure à la proportion de personnes hospitalisées qui ne le reçoivent pas (39.3%) ;l’indicateur NEWS-2 est un indicateur est un indicateur permettant d’évaluer l’état du patient au moment où il est examiné. Plus cette valeur est élevée et plus son état est dégradé. Dès lors que le score est supérieur à 5, les risques d’une réponse urgente deviennent important. Pour cet indicatuer, il apparaît que les risques sont beaucoup plus élevés (19.9%) pour les personnes qui ne reçoivent pas le traitement que pour les personnes qui le reçoivent (6.3%)un troisième problème qui saute d’emblée aux yeux est que, parmi les personnes qui font partie de la condition ‘autre traitement’, certains ont reçu l’association hydoxychloroquine (HCQ) et azymothricine (AZ) pendant moins de 3 jours, d’autres ont reçu uniquement de l’hydoxychloroquine, d’autres ont reçu uniquement de l’azymothricine et enfin, certaines personnes n’ont rien reçu. Cette répartition est étonnante car :\ns’attend normalement à ce que le groupe de contrôle ne reçoivent pas le même traitement que le groupe traité (pendant une durée moindre). En effet, peut raisonnablement penser que les personnes pour lesquelles le traitement n’pas durée 3 jours ou plus sont celles dont l’état s’est dégradé et qu’elles ont basculé du bras “traité” au bras “contrôle”. Cette manière de faire est méthodologique douteuse. ne change pas les membres d’une équipe en cours de match. Cela reviendrait à ce que, à un moment du match, un joueur de l’OM se mette à jouer pour le PSG (ou inversément).\nIl est possible de comparer HCQ+AZ >3 jours avec HCQ > 3 jours, AZ > 3 jours et aucun traitement. Cependant, dans ce cas, n’est pas dans un plan avec deux bras parallèles mais dans un plan factoriel. Il est donc anormal de regrouper ces trois groupes ensemble.\ns’attend normalement à ce que le groupe de contrôle ne reçoivent pas le même traitement que le groupe traité (pendant une durée moindre). En effet, peut raisonnablement penser que les personnes pour lesquelles le traitement n’pas durée 3 jours ou plus sont celles dont l’état s’est dégradé et qu’elles ont basculé du bras “traité” au bras “contrôle”. Cette manière de faire est méthodologique douteuse. ne change pas les membres d’une équipe en cours de match. Cela reviendrait à ce que, à un moment du match, un joueur de l’OM se mette à jouer pour le PSG (ou inversément).Il est possible de comparer HCQ+AZ >3 jours avec HCQ > 3 jours, AZ > 3 jours et aucun traitement. Cependant, dans ce cas, n’est pas dans un plan avec deux bras parallèles mais dans un plan factoriel. Il est donc anormal de regrouper ces trois groupes ensemble.Par ailleurs, le Tableau 1.4 montre que les personnes dans “autres groupes” sont plus âgés que dans le groupê HCQ+AZ. Concrètement, 3.6% de personnes de plus de 74 ans dans le groupe HCQ+AZ, alors que cette proportion est de 16% dans “autres traitements”. , sait que les personnes les plus à risque de complication dus à la covid19 sont les personnes âgées. Remarquez que ce phénomène est valable pour chacun des sous-groupe, et en particulier le sous-groupe (à l’exception de l’hydroxychloroquine seule) qui reçu le traitement pendant moins de trois jours. Ce tableau nous indique également que les scores au NEWS supérieur ou égal à 6 (donc requérant des soins urgents) est de 2.6% dans le groupe HCQ+AZ, alors que cette proportion est de 10.5% dans dans les autres groupes, avec une proportion de 14.2% dans le groupe qui reçu le traitement pendant moins de 3 jours. Cette proportion particulièrement importante est interpellante car cela amène à penser que les personnes dont l’état se serait dégradé ou qui serait décédées en ayant reçu le traitement efficace ont été basculées dans le groupe de contrôle. Enfin, observe également que le scanner pulmonaire est proportionnellement plus dégradé chez les personnes appartenant à “autre traitement” qu’au traitement HCQ+AZ. Ceci amène donc à se questionner sur la manière de choisir les personnes qui allaient recevoir le traitement : il semble que les personnes à risque de dégradation avaient moins de chance de recevoir le traitement, ce qui entrainement artificiellement un meilleur résultat pour les personnes traitées.Examinons à présent les personnes qui ont été hospitalisées en soins intensifs (ICU) et celles qui sont décédées dans le Tableau 1.5. Ils sont de 1.1% dans le groupe HCQ+AZ et de 9.4% dans le groupe “autre traitement”, avec un taux de 17% dans le groupe qui reçoit le traitement pendant moins de 3 jours, tandis que les personnes qui ne reçoivent rien ont une évolution vers les soins intensifs et le décès de 3.1%. Pour le formuler autrement, les personnes qui ne reçoivent rien ont un taux d’évolution négative proche du groupe traité, et le groupe pour lequel l’évolution est la plus négative est le groupe ayant reçu le traitement durant moins de trois jours. Donc, même si laissait le bénéfice du doute concernant le fait d’avoir basculé les personnes recevant le traitement miracle dans le groupe de contrôle pour faire apparaître des résultats flatteurs, aurait du mal à trouver une explication rationnelle pour rendre compte du fait que, si l’HCQ+AZ est efficace, les patients qui le reçoivent durant moins de 3 jours voient leur état se dégrader bien plus souvent que n’importe quel autre groupe. devrait s’attendre à ce que cela les aide tout de même un peu (ou minima que cela n’entraîne pas un risque supplémentaire) plutôt que de présenter un risque d’évolution négative plus important que les autres groupes","code":""},{"path":"quel-logiciel-utiliser-et-pourquoi.html","id":"quel-logiciel-utiliser-et-pourquoi","chapter":"2 Quel logiciel utiliser et pourquoi ?","heading":"2 Quel logiciel utiliser et pourquoi ?","text":"“Science advances questioning results, hiding . (La science avance en questionnant les résultats, non pas en les cachant”","code":""},{"path":"quel-logiciel-utiliser-et-pourquoi.html","id":"introduction","chapter":"2 Quel logiciel utiliser et pourquoi ?","heading":"2.1 Introduction","text":"Aborder les statistiques dans la perspective d’une mise en application requiert non seulement de fournir les fondements théoriques pour choisir et comprendre les analyses statistiques mais doit également fournir les bases nécessaires pour utiliser de manière autonome un logiciel de statistiques. Bien qu’il existe une multitude de logiciels statistiques, nous nous consacrerons au logiciel R (R Core Team, 2025) et au package que nous avons développé, easieR (Stefaniak, 2018)Nous commencerons par expliquer pourquoi l’utilisation de R est bon pour vous. Ensuite, nous expliquerons comment l’installer et la logique sous-jacente à R. Nous continuerons avec les règles de bonnes pratiques qui vous seront utiles tout au long de cet ouvrage mais également au-delà si vous étiez amené.e à devoir réaliser des analyses qui ne sont pas couvertes par le contenu de ce livre. Enfin, nous terminerons en présentant quelques outils vous permettant de préparer vos données.","code":""},{"path":"quel-logiciel-utiliser-et-pourquoi.html","id":"quest-ce-r-et-pourquoi-lutiliser","chapter":"2 Quel logiciel utiliser et pourquoi ?","heading":"2.2 Qu’est-ce R et pourquoi l’utiliser ?","text":"Sorti pour la première fois en 1995, R est un langage de programmation ouvert, issu de langage S, spécialisé dans le traitement des données.Á l’heure actuelle, il s’agit d’un des logiciels les plus utilisés dans le monde académique. D’après Choueiry (Choueiry, 2021), non seulement R arrive en seconde position pour traiter les données dans le domaine de la recherche en santé, mais arrive quasiment à égalité de la première place lorsqu’s’intéresse aux articles publiés dans les meilleures revues (si considère que le facteur d’impact d’une revue informe d’une manière ou d’une autre sur la qualité des articles qui y sont publiés). De plus, R fait partie des langages de programmation les plus demandés dans les offres d’emploi de l’IEEE (https://www.facebook.com/48576411181), Á l’heure actuelle, de nombreux mastodontes de l’informatique, en particulier celles qui utilisent l’intelligence artificielle, comme google et Facebook, l’utilisent. Etant donné que R pour vocation première le traitement des données statistiques, il est légitime de se demander ce qui le rend aussi populaire et pourquoi vous devriez utiliser R.Utiliser R, c’est avant tout s’engager vis-à-vis d’un système de valeurs dans lequel considère que l’accès au savoir et à la science doit être ouvert et transparent, que l’évolution des connaissances doit être un processus collaboratif dont le est de faire progresser l’Humanité dans son ensemble. Peut-être pensez-vous que ce projet est utopique, mais de plus en plus de personnes, et en particuliers de plus en plus de scientifiques ont la conviction qu’une science ouverte et transparente est l’avenir de la science. La place que prend R dans la publication dans les revues les plus prestigieuses, tel que nous venons de l’évoquer, n’est pas un phénomène irrationnel, mais est le fruit d’un double processus qui interagit : d’une part, les lignes éditoriales de ces revues imposent aux auteurs plus de transparence à la fois sur les données, mais également sur la manière dont elles ont été traitées et R est un outil parfaitement adapté pour répondre à ce critère ; d’autre part, les chercheurs qui adoptent les pratiques de la science ouverte vont avoir plus de citations, plus d’attention des médias, plus de collaborateurs potentiels, plus d’opportunités d’emploi et plus de facilités pour obtenir des financements (McKiernan et al., 2016). Pour la seconde raison, la question de la poule et de l’oeuf se pose : est-ce les meilleurs chercheurs qui favorisent la science ouverte ou est-ce le fait de favoriser la science ouverte qui rend un chercheur meilleur? Sans doute un peu les deux.Utiliser R est un pas vers cet objectif de science ouverte car R est un logiciel ouvert. Cela signifie qu’il est gratuit (ce qui est une bonne nouvelle pour vous), que vous pouvez analyser le code utilisé pour programmer les fonctions que vous allez utiliser, et vous pourrez également, si vous le souhaitez, contribuer à son développement. En d’autres termes, R est collaboratif, et ce processus collaboratif n’est pas restreint aux contributions que chacun peut faire. Le côté collaboratif de R se manifeste également par la communauté d’entraide qui existe sur les forums en ligne et par le fait que chaque utilisateur la possibilité d’informer un des contributeurs s’il y une erreur ou un bug dans son code.Pour comprendre en quoi cette manière de penser est avantageuse, prenons le temps de nous créer une représentation mentale de la manière dont vos logiciels commerciaux préférés fonctionnent et comparons ce fonctionnement avec celui de R. Pour le logiciel que vous avez à l’esprit, connaissez-vous les auteurs du logiciel et leurs compétences ? Probablement que non. Dans le cas de R, il y des milliers de contributeurs, certains étant des anonymes qui veulent apporter leur pierre à l’édifice, mais d’autres sont d’éminents spécialistes, de domaines parfois très spécialisés.\nconçoit donc aisément que, si la qualité des contributeurs est plus inégale lorsqu’utilise R, vous pourrez cependant bénéficier des outils les plus performants pour traiter une problématique et ce en étant bien guidé et en cherchant les fonctions les plus recommandées. Cela se fait notamment grâce aux communautés actives sur le web et à la liste des packages que le noyau dur des programmeurs de R met en avant. Rappelons que ce seront des spécialistes du domaine qui auront développé la boîte à outil permettant de répondre spécifiquement à votre question. Utiliser R en ligne de commande offre donc une flexibilité qu’aucun logiciel sous la forme d’interface graphique ne peut proposer.Ainsi, si reprend notre comparaison avec un logiciel commercial, imagine aisément qu’une équipe restreinte d’une entreprise commerciale, aussi bonne que soit cette équipe, ne peut se tenir à la pointe de tous les domaines pour pouvoir rivaliser avec une communauté de scientifiques spécialistes de leur domaine. Imaginons à présent que le logiciel commercial que vous utilisez un bug, que faites-vous ? Probablement, rien. Au mieux, vous enverrez un courriel au SAV de\nla société qui commercialise le logiciel qui, avec un peu de chance, prendra vos remarques en compte pour améliorer la version suivante du logiciel, version suivante que vous serez amenés à acheter et qui sortira plus ou moins tardivement par rapport au moment où le bug été identifié. R fonctionne sur un autre modèle : vous pouvez contacter directement le contributeur de la fonction pour laquelle vous avez identifié un souci. Ce dernier identifiera l’origine du problème et le corrigera en quelques jours. Croyez-en mon expérience, les auteurs des packages fiables sont extrêmement réactifs.Les avantages de R ne se limitent pas à être gratuit. En effet, R vous permet également d’être transparent et reproductible, car vous pouvez aisément partager le code qui servi à analyser les données. Cela deux avantages. Le premier, et pas le moindre, est que cela permet à d’autres personnes d’avoir accès et donc de réutiliser la méthode d’analyse qui été utilisée dans un article donné. Ces personnes pourront alors reproduire cette méthode pour l’appliquer à un autre article. Dans cette section, va vous présenter des lignes de commandes que vous réutiliserez pour vos propres données. Ce chapitre été écrit en utilisant directement R. Ainsi, je suis sûr que les lignes de commandes que je vais vous présenter vont fonctionner en l’utilisant exactement de la manière dont elle est présentée. Les erreurs de copier/coller sont donc évitées.\nLe second avantage est que tout le monde peut vérifier qu’il n’y pas d’erreur dans le traitement des données qui ont été publiées. Ces erreurs peuvent se manifester à la fois dans la manière de mener les analyses, mais également dans le report des indices. En effet, pour les articles publiés entre 1985 et 2013 dans 8 grosses revues de psychologie, Nuijten et ses collaborateurs (Nuijten et al., 2016) ont identifié que la moitié des articles présentaient des inconsistances entre la valeur de la probabilité reportée au regard de la valeur de la statistique et des degrés de libertés 1 qui leur étaient associés.Les erreurs peuvent arriver. Il est tout en fait acceptable d’en commettre mais il est moins acceptable de ne pas permettre aux autres de vérifier qu’il n’y en ait pas.Il faut se rappeler que la recherche en France, mais également dans beaucoup de pays du monde, est financée, au moins en partie, par des fonds publics. Á partir du moment où de l’argent public est dépensé, il y une nécessité de transparence à la fois sur la manière dont cet argent été dépensé, mais également sur le fruit de ces dépenses. Ainsi, la valorisation des recherches, soit les financeurs (les contribuables lorsque la recherche est financée sur les fonds publics), doivent non seulement pouvoir accéder aux résultats de la recherche, mais également à toutes les étapes qui ont permis d’atteindre ces résultats, y compris le traitement des données. R permet cette transparence puisqu’peut partager les données ainsi que le code utilisé et qu’il n’est pas nécessaire d’avoir une licence d’un logiciel payant pour vérifier que tout est correct.Au-delà de ces avantages, en termes de fonctionnalités, R est le champion incontestable en offrant bien plus de fonctionnalités que les logiciels payants, avec plus de 150 fois plus de fonctions que SAS (Muenchen, 2015), en faisant ainsi un outil extrêmement puissant et polyvalent pour l’analyse de données.peut ajouter aux qualités de R qu’il est compatible avec tous les systèmes d’exploitation (Windows, MacOS, Linux) et qu’il existe des serveurs en ligne qui vous permettent d’utiliser R sans devoir l’installer sur votre ordinateur. Cela peut être utile lorsqu’peu de place sur le disque dur ou qu’veut travailler de manière collaborative.Certains aficionados vous diront même que R va vous aider à mieux comprendre les statistiques. Je ne partage pas cet avis : un logiciel en soi n’aide pas à comprendre ou à ne pas comprendre. En revanche, la personne qui vous explique les statistiques peut utiliser R pour faire des simulations ou encore l’utiliser comme outil pédagogique pour l’enseignement des statistiques, ce qui ne peut pas être fait dans d’autres logiciels statistiques où ne peut faire que du click. Par exemple, Snow (2024) développé toute une série d’outils permettant d’expliquer des concepts fondamentaux des statistiques.Par ailleurs, utiliser R requiert de prendre des décisions statistiques et de comprendre ce que l’fait sinon c’est le message d’erreur assuré. Ainsi, si vous ne comprenez pas les statistiques, R n’pas le pouvoir magique de vous faire comprendre, mais si vous comprenez les statistiques, l’utilisation de R vous permettra d’aller encore plus loin. Cependant, que vous compreniez ou non les statistiques, utiliser R de manière efficiente nécessite du temps, car il faudra apprendre à utiliser des lignes de commandes, lignes de commandes qui ont des arguments. Cet apprentissage est assez ténu dans un premier temps mais l’équipe qui développe R fait un effort important pour harmoniser la manière d’utiliser les fonctions. Et si l’apprentissage est lent dans un premier temps, par la suite, vous serez beaucoup plus rapide pour toute une série de tâches, telles que les prétraitements de vos données. Par ailleurs, les outils d’intelligence artificiel ont largement progressé pour vous aider à utiliser les lignes de commande. Il n’est pas possible de ne s’appuyer que ces outils sans comprendre ce qu’fait, mais cela fait gagner un temps considérable quand les bons réflexes et les bonnes bases.Enfin, utiliser R un intérêt indirect : l’utilisation des lignes de commande est un enseignement sans compromis à la rigueur. Comme évoqué juste avant, nous commettons tous des erreurs, quand en commet en utilisant des lignes de commande, le résultat est sans appel : un message d’erreur. Le fait d’être exposé à ce message d’erreur n’est pas anodin, il souligne le nombre de fois que nous sommes susceptibles de nous tromper, indépendamment des statistiques, et de prendre conscience de la nécessité d’être exigeant avec nous-mêmes, avec nos productions lorsque nous rédigeons un document afin d’éviter au maximum ces erreurs car la science, au travers des articles scientifiques, pour vocation d’être exempte autant que peu se faire d’erreurs. Si vous vous dites que vous ne voulez pas utiliser R car vous recevez constamment des messages d’erreur, est-ce que vous êtes disposés à être critiqués, comme c’est le cas lorsque nous sommes relus par des coauteurs ou dans le processus d’expertise, et à élever votre niveau pour éviter de commettre les mêmes erreurs à l’avenir. La vertu pédagogique de cet apprentissage est sans doute inégalable pour quiconque envisage une carrière dans la recherche, et pour les autres, cela apprend l’humilité face à nos certitudes, et notre niveau de compétence.R, dans sa forme brute, n’est pas très conviviale. C’est pourquoi nous utiliserons Rstudio, également gratuit, qui est une interface graphique de R et un éditeur de code.","code":""},{"path":"quel-logiciel-utiliser-et-pourquoi.html","id":"quest-ce-que-easier-et-pourquoi-lutiliser","chapter":"2 Quel logiciel utiliser et pourquoi ?","heading":"2.3 Qu’est-ce que easieR et pourquoi l’utiliser","text":"","code":""},{"path":"quel-logiciel-utiliser-et-pourquoi.html","id":"rationnel-deasier","chapter":"2 Quel logiciel utiliser et pourquoi ?","heading":"2.3.1 Rationnel d’easieR","text":"D’un point de vue pédagogique, utiliser R en ligne de commande requiert de faire le deuil concernant l’autonomie des étudiant·es dans le traitement des données. En effet, il n’est pas pédagogiquement possible, quand le volume horaire est restreint pour enseigner les statistiques, de les aborder de manière suffisamment approfondie pour assurer l’autonomie dans le traitement des données tout en assurant l’accompagnement indispensable aux étudiant·es qui se trouvent dans la situation où ils et elles utilisent pour la première fois des lignes de commande, à savoir faire face aux messages d’erreurs. Ces messages d’erreurs pouvaient également être source de frustration et de découragement pour les étudiant·es, en particulier celles et ceux qui ont une vision de l’intelligence en tant qu’entité (Dweck, 2006)C’est dans ce contexte que easieR (Stefaniak, 2018) commencé être développé, à cette époque où JASP (2025) n’était qu’à ses prémices et Jamovi (2025) n’existait pas encore. Si ces deux derniers logiciels se sont imposés dans de nombreux cours de statistiques des programmes de psychologie en France et à l’étranger, easieR se démarque dans sa conception à plusieurs égards expliquant pourquoi il continue à se justifier par rapport à ces alternatives.","code":""},{"path":"quel-logiciel-utiliser-et-pourquoi.html","id":"philosophie-sous-tendant-easier","chapter":"2 Quel logiciel utiliser et pourquoi ?","heading":"2.3.2 Philosophie sous-tendant easieR","text":"Le package easieR (Stefaniak, 2018) s’articule autour de 3 piliers philosphiques (voir Figure 2.1 :une démarche science ouverte ;un pipeline complet et cohérent dans la présentation des analyses ;des outils permettant de répondre aux besoins les plus fréquents des étudiant·es et des chercheur·euses en psychologie ;une structure pédagogique inhérente à sa conception.\nFigure 2.1: Piliers sur lesquels easieR s’appuie.\n","code":""},{"path":"quel-logiciel-utiliser-et-pourquoi.html","id":"une-démarche-science-ouverte","chapter":"2 Quel logiciel utiliser et pourquoi ?","heading":"2.3.2.1 Une démarche science ouverte","text":"Le package easieR s’inscrit dans une démarche sciences ouvertes de deux manières principales:Permettre d’adopter une approche multiverse de l’analyse des données (Steegen et al., 2016) ;Permettre d’utiliser des scripts reproductibles.En ce qui concerne l’approche multiverse de l’analyse des données, il s’agit d’analyser les données de différentes manières afin de s’assurer de la robustesse des résultats. Si l’interprétation des résultats diffère selon la manière dont l’analyse été réalisée, il semble indispensable de s’interroger sur ce que veulent vraiment dire les données. Cette approche est le contre-pied du p-hacking puisque les auteurs sont invités à présenter les résultats (par exemple en annexe) des différentes méthodes d’analyses qui peuvent être envisagées pour tester l’hypothèse et de montrer la cohérence entre ces différentes manières d’analyser les données et les résultats présentés.Quant au fait d’être reproductible et de faciliter la transition entre les outils statistiques à base de boîtes de dialogues vers ceux à base de les lignes de commande, cela est possible car easieR (Stefaniak, 2018) peut être utilisé tant en boîte de dialogue (ce qui est fait avec les étudiant·es) qu’en ligne de commande. Cette fonctionnalité représente l’originalité de easieR en tout qu’outil statistiques. En effet, si de plus en plus de revues scientifiques demandent de partager les codes des analyses, s’approprier un langage informatique est souvent compliqué. C’est le cas pour R (R Core Team, 2025). Ainsi, confrontés aux erreurs répétées que l’peut commettre en débutant à utiliser R (R Core Team, 2025) en ligne de commande, les apprenants peuvent considérer leur courbe d’apprentissage particulièrement lente, ce qui affectera de manière négative leur motivation. Dans le package easieR (Stefaniak, 2018), les erreurs ne sont pas sanctionnés par un message d’erreur mais par une boîte de dialogue qui prend le relais. Ainsi, même en cas d’erreur, l’analyse peut être réalisée, et surtout en comparant la ligne de commande générée automatiquement avec celle initialement utilisée afin d’identifier les causes qui ont empêché les lignes de commande d’être exécutée correctement.\nPar ailleurs, comme easieR est directement utilisé dans l’interface R et permet dès lors sensibiliser les utilisateur·ices à des notions comme le chargement d’une bibliothèque ou l’utilisation d’une fonction, tout en se familiarisant avec l’environnement.Enfin, il est possible de générer un rapport des résultats en html ou en word, ce qui facilite les copiés collers et éviter les erreurs de reports.","code":""},{"path":"quel-logiciel-utiliser-et-pourquoi.html","id":"une-conception-orientée-vers-la-pédagogie","chapter":"2 Quel logiciel utiliser et pourquoi ?","heading":"2.3.2.2 Une conception orientée vers la pédagogie","text":"Souvent les outils statistiques proposent une multitude d’options, mais les utilisateurs ne porte que très peu leur intérêt sur ces options car ces options ne font pas partie de leurs connaissances statistiques. La conception de easieR est fondamentalement différente à bien des égards :easieR est une suite de boîte de dialogue qui force à prendre une décision. Par défaut, la meilleure option (une des options parmi les meilleurs options possibles) statistiques est choisie. Néanmoins, il est possible d’envisager une alternative. Cette décision requiert alors une décision éclairée ;afin d’accompagner les utilisateurs à prendre leurs décisions statistiques, des informations sont affichées dans la console en parallèle de l’apparition des boîtes de dialogue de sorte à aiguiller la décision des néophytes ;lorsque la meilleure option statistique est peu connue par la plupart des personnes, les résultats plus habituels sont présentés à côté ou au-dessus de la solution optimale, ce qui facilite la comparaison et permet d’être initié à ces options alternatives, meilleures mais moins connues. Par exemple, pour une analyse factorielle, les corrélations polychoriques devraient être préférées pour des données ordinales et les corrélations tétrachoriques pour des données dichotomiques. Ce choix est celui par défaut lorsque le nombre de modalités est inférieur à 8 (pour les corrélations polychoriques) ou sont égales à 2 (pour les corrélations tétrachoriques).","code":""},{"path":"quel-logiciel-utiliser-et-pourquoi.html","id":"un-pipeline-cohérent-dans-la-sortie-des-résultats","chapter":"2 Quel logiciel utiliser et pourquoi ?","heading":"2.3.2.3 Un pipeline cohérent dans la sortie des résultats","text":"La philosophie inhérente à easieR (Stefaniak, 2018) est également quelque peu différente. En effet, contrairement à la plupart des packages de statistiques, la vérification des conditions d’application n’est pas une option et il n’est pas possible de l’éviter, tout comme il n’est pas possible d’ignorer les statistiques descriptives. Ces deux prérequis semblent être la base d’une analyse statistique critique puisqu’il n’est pas possible de donner sens aux résultats si les conditions d’applications des analyses ne sont pas respectées (vu que l’estimation des probabilités de dépassement sont inhérentes au respect de ces conditions d’application). De même, si ne regarde pas les statistiques descriptives, il n’est pas possible de s’assurer que les données correspondent aux données censées être analysées : est-ce que l’effectif est correct ? Est-ce que les valeurs minimales et maximales pour les variables quantitatives sont compatibles avec la mesure ? Ainsi, l’utilisateur d’easieR (Stefaniak, 2018) est guidé dans une démarche systématique d’analyse des données qui ne se limite pas à vérifier si la probabilité de dépassement est inférieure à 0.05. Afin de favoriser l’adopter de cette démarche systématique, toutes les analyses sont structurées selon un même schéma (voir Figure 2.2).\nFigure 2.2: Structure de l’analyse des données avec easieR.\nL’analyse commence systématiquement par la présentation des statistiques descriptives avec pour objectif :\n- d’identifier si travaille sur le bon nombre d’observations ;\n- s’assurer que les minimums et maximums sont compatiables avec les échelles qui ont été utilisées (par ex., ne peut pas avoir un QI de 245) ;\n- identifier les tendances d’un point de vue descriptif ;Ensuite, les conditions d’application sont systématiquement testées de sorte à ce que, en cas de violation de ces conditions d’application, l’utilisateur·trice soit amené·e à utiliser une analyse alternative ou à corriger ces violations quand c’est possible. L’objectif est également de représenter un signal d’alarme sur le fait que les conclusions perdent en robustesse dès lors que ces conditions ne sont pas respectées.La présentation de l’analyse principale avec les tailles d’effet et les intervalles de confiance afin de comprendre que l’essence des statistiques n’est pas d’avoir une valeur p inférieur à 0.05, mais d’avoir une taille d’effet qui fasse sens.La présentation des alternatives non paramétriques et/ou robustes, à savoir des bootsrap, des M-estimator ou encore des anovas sur les médianes, afin d’adopter une démarche multiverse (Steegen et al., 2016), consistant à s’assurer que les conclusions qui sont tirées ne dépendent pas de la manière dont les données ont été analysées mais aboutissent à la même conclusion, qu’importe l’outil statistique utilisé.L’identification de valeurs influentes qui pourraient avoir un impact sur les résultats et, le échéant, une réanalyse sans ces valeurs influentes afin de rester dans cette continuité de démarche multiverse d’analyse des données. Á ma connaissance, easieR est le seul outil utilisant des boîtes de dialogue qui intègre directement cette possibilité.","code":""},{"path":"quel-logiciel-utiliser-et-pourquoi.html","id":"répondre-aux-besoins-les-plus-courants-des-utilisateurs","chapter":"2 Quel logiciel utiliser et pourquoi ?","heading":"2.3.2.4 Répondre aux besoins les plus courants des utilisateurs","text":"Les fonctions principales de easieR sont résumées dans le tableau 2.1Table 2.1: Tableau synthétique reprenant les principales fonctions d'easieRChoix dans le menu principalFonction principaleAnalyse alternativeL'option utileDonnéesImporter (en différents formats)Rxporter (en différents formats)VisualiserGénérer un rapport de résultatsPréparation des donnéesFilrer des observationsSélectionner des variablesTransformer en rangsImputation de valeurs manquantesCentrer réduireTrierRéaliser des opérations mathématiquesPasser d'un format large au format longGraphiquesBoxplotNuage des pointsDensitéViolin plotHistogrammesAnalysesChi carré d'ajustementsimulation de Monte CarloChi carré d'indépendancesimulation de Monte Carlo, correction de continuitéContrastes pour identifier les cases qui s'écartent significativementTest de McNemarCorrection de continuitét de Student comparaison à une normetest de Wilcoxon, M-estimator,bootstrapt de Student pour échantillons appariésTest de Wilcoxon, M estimator, bootstrapGraphique selon les recommandations de Loftust de Student pour échantillons indépendantsMann-Whitney, correction de Satterthwaite, t sur les médianes, t sur les moyennes tronquées (avec ou sans bootstrap)ANOVA simple à groupes indépendantsKruskall-Wallis, correction de Welch, anova sur les médianes ou moyennes tronquéesTous les contrastes sont possiblesANOVA à mesure répétéesANOVA de Friedman, correction de Greenhouse et Geisser, anova sur les médianesTous les contrastes sont possiblesANOVA factoriellesanova sur les médianesTous les contrastes sont possiblesANOVA mixtesanova sur les médianesTous les contrastes sont possiblesAnalyse de covarianceTous les contrastes sont possiblesRégressionsBoostrap,M-estimatorEffets de modérations et  effets non linéairesCoefficient de concordanceAlpha de CronbachOmega de McDonaldCoefficient de corrélation intraclasseCoefficient de corrélations de PearsonRho de Spearman, tau de Kendall, boostrapAnalyse par groupe analyse complète ou matrice de corrélation (rectangulaire ou carrée)Analyse factorielle exploratoireEstimateurs robustesUtilisation des corrélations tétrachoriques/polychoriquesAnalyse factorielle confirmatoireEstimateurs robustesUtilisation des corrélations tétrachoriques/polychoriquesAnalyse en composantes principalesEstimateurs robustesUtilisation des corrélations tétrachoriques/polychoriques","code":""},{"path":"installer-le-logiciel-de-traitement-de-données.html","id":"installer-le-logiciel-de-traitement-de-données","chapter":"3 Installer le logiciel de traitement de données","heading":"3 Installer le logiciel de traitement de données","text":"","code":""},{"path":"installer-le-logiciel-de-traitement-de-données.html","id":"installer-r","chapter":"3 Installer le logiciel de traitement de données","heading":"3.1 Installer R","text":"La première étape consiste en l’installation de R. Le logiciel est en libre\ntéléchargement à l’adresse suivante : https://cran.r-project.org/ où vous trouverez le lien de téléchargement pour les différents systèmes d’exploitation.","code":""},{"path":"installer-le-logiciel-de-traitement-de-données.html","id":"je-suis-un-utilisateur-windows","chapter":"3 Installer le logiciel de traitement de données","heading":"3.2 Je suis un utilisateur windows","text":"Pour les utilisateurs windows, il faut se rendre sur http://cran.r-project.org/bin/windows/base/Une fois sur cette page, il faut cliquer « download R » dont le numéro de version évolue constamment.\nComme le fichier téléchargé est un fichier « exe », il faut double-cliquer sur le fichier.\nIl suffit ensuite de cliquer constamment sur suivant jusqu’à ce que le logiciel soit installé.","code":""},{"path":"installer-le-logiciel-de-traitement-de-données.html","id":"je-suis-un-utilisateur-apple","chapter":"3 Installer le logiciel de traitement de données","heading":"3.3 Je suis un utilisateur apple","text":"Pour les utilisateurs MacOS, il faut cliquer sur ce lienHyper important : vérifiez le type de puce sur votre ordinateur en cliquant sur la pomme en haut à gauche et puis “à propos de ce mac”. Si, à côté de puce, vous avez un nom du type M1, M2 ou M3, alors, vous devez télécharger et installé la version arm de R. Si, à côté de puce, vous avez un nom qui contient intel, vous devez télécharger, la version x86 de R.Important : si votre système d’exploitation (OS) est inférieur à Big Sur, vous ne pouvez pas installer la dernière version de R. Vous devez trouver une version de R compatible avec votre système d’exploitation à l’adresse suivante (voir https://cran.r-project.org/). Néanmoins, vous vous exposez à ce que une ancienne version de R ne soit pas compatible avec des packages que vous utiliserez. L’alternative moins risquée est d’utiliser la même procédure que celle préconisée pour les utilisateurs Linux et ChromebookUne fois téléchargé, il faut double-cliquer sur le fichier « pkg », Il suffit ensuite de cliquer sur suivant jusqu’à ce que le logiciel soit installé.Pour finaliser l’installation, vous devez encore installer XQuartz disponible à l’adresse suivante : https://www.xquartz.org/","code":""},{"path":"installer-le-logiciel-de-traitement-de-données.html","id":"je-suis-un-utilisateur-linux-ou-chromebook","chapter":"3 Installer le logiciel de traitement de données","heading":"3.4 Je suis un utilisateur Linux ou Chromebook","text":"Si, théoriquement, il est possible de faire l’installation (du moins sur les ordinateurs ayant un Linux comme système d’exploitation), n’étant pas expert de ces systèmes d’exploitation, La solution la plus simple est de créer un compte (gratuit) sur Rstudio cloud (https://login.rstudio.cloud). Cela évite toutes les difficultés d’installation.Dans votre cas, aucune installation n’est requise. Notez que ce document est écrit à partir d’une version bureau (donc sur un ordinateur personnel) de RStudio et qu’il peut y avoir des petites divergences entre la version bureau et la version cloud.","code":""},{"path":"installer-le-logiciel-de-traitement-de-données.html","id":"installation-de-rstudio","chapter":"3 Installer le logiciel de traitement de données","heading":"3.5 Installation de RStudio","text":"Pour aucun utilisateur, il n’est indispensable d’installer Rstudio. Néanmoins, pour les utilisateurs de R en ligne de commande, l’installation de Rstudio peut représenter un environnement plus simple d’utiliation.RStudio n’est rien d’autre qu’une interface graphique qui va rendre l’utilisation de R un peu plus agréable. R studio propose également quelques fonctionnalités d’importation, de navigation entre les graphiques et d’accès aux objets stockés dans la mémoire de R. Son utilisation n’est pas obligatoire mais est très vivement recommandée dès lors qu’veut travailler avec des scripts reproductibles.Rstudio est disponible gratuitement en téléchargement à l’adresse suivante : https://rstudio.com/products/rstudio/download/Qu’importe votre système d’exploitation, la procédure est la même : clique sur le lien de téléchargement. Une fois téléchargé, double-clique sur le fichier et sur suivant jusqu’à ce que Rstudio soit installé.","code":""},{"path":"installer-le-logiciel-de-traitement-de-données.html","id":"installation-de-easier","chapter":"3 Installer le logiciel de traitement de données","heading":"3.6 Installation de easieR","text":"eaieR est une interface graphique qui peut être directement utilisée dans R et il n’est pas indispensable d’avoir installé Rstudio.La procédure pour installer easieR est la suivante :vous avez téléchargé et installé R2 en fonction de votre système d’exploitation. Pour les utilisateur·ices macOS, n’oubliez pas d’installer XQuartz.vous avez téléchargé et installé R2 en fonction de votre système d’exploitation. Pour les utilisateur·ices macOS, n’oubliez pas d’installer XQuartz.Télécharger et installer PandocTélécharger et installer Pandocvous ouvrez R et dans la fenêtre, vous copier coller le code ci-dessous :vous ouvrez R et dans la fenêtre, vous copier coller le code ci-dessous :Si vous recevez un message indiquant que certains packages peuvent être mis à jour, choisissez l’option 2 (qu’il faut taper dans la console), CRAN .Il est important de noter que, quand trop d’utilisateurs se connectent en même temps à github, github peut empêcher l’installation car il y trop de requêtes en même temps. Il faut alors utiliser la solution alternative, ci-dessous.","code":"\ninstall.packages(\"devtools\")\ndevtools::install_github(\"nicolasstefaniak/easieR\", type=\"binary\") \n  # 1. Télécharger le zip depuis GitHub\n  url <- \"https://github.com/NicolasStefaniak/easieR/archive/refs/heads/master.zip\"\n  temp_zip <- tempfile(fileext = \".zip\")\n  download.file(url, destfile = temp_zip, mode = \"wb\")\n\n  # 2. Dézipper dans un dossier temporaire\n  temp_dir <- tempdir()\n  unzip(temp_zip, exdir = temp_dir)\n\n  # 3. Identifier le dossier du package\n  package_dir <- file.path(temp_dir, \"easieR-master\")\n\n  # 4. Installer le package\n  devtools::install(package_dir, subdir = subdir)\n\n  # 5. Nettoyer (optionnel)\n  file.remove(temp_zip)"},{"path":"installer-le-logiciel-de-traitement-de-données.html","id":"easier-ne-sinstalle-pas-correctement","chapter":"3 Installer le logiciel de traitement de données","heading":"3.6.1 easieR ne s’installe pas correctement","text":"Si vous êtes sur windows, le problème le plus fréquent est que votre compte windows est associé à des caractères spéciaux, tels que “à”,“ç”,“é”,“è”,“ê”… Par exemple, votre compte s’appelle élève. La solution la plus simple est de créer une nouvelle session sur votre ordinateur en passant par :\n- panneau de configuration\n- comptes d’utilisateurs\n- créer un nouveau compte\nPensez à accorder les droits administrateurs à ce compte pour pouvoir installer tout correctement.Un autre problème que vous pourriez avoir est de manquer d’espace sur votre disque dur. La solution à ce problème est plus technique et nécessite d’avoir un support externe pour stocker les packages. Voici la procédure :créer un dossier sur le support externe où les packages seront stockés (imaginons que le chemin d’accès est D:/packages);créer un script R que vous enregistrerez dans document avec le nom .Rprofile (exactement tel que je l’ai écrit)dans demandez à R où sont stockés les packages avec la fonction .libPaths()Dans le document .Rprofile créé, ajouter les deux chemins d’accès suivants (le premier vous est fourni par .libPaths() et le second est celui que vous avez créé) ;Reprendre l’installation de easieR à install_githubEnfin, pour tous les utilisateur·ices, un des problèmes les plus fréquents est que tous les logiciels n’ont pas été installé. Vous pouvez vérifier que pandoc été installé correctement par :Vous avez que pandoc est installé si vous avez une autre valeur que ‘0’.Remarque : il est possible que vous ayez installé pandoc mais que R ne le prenne pas en compte parce que vous avez installé pandoc après avoir ouvert R. Il suffit de fermer R, de le rouvrir et de revérifier.Pour les utiliteurs macOS, il faut aussi s’assurer que XQuartz est correctement installé. Pour cela, allez dans votre répertoire des applications et dans le dossier Utilitaires (le nom peut potentiellement changer en fonction de la version de l’OS). Si vous ne vous XQuartz ni dans vos applications, ni dans aucun des sous-dossiers, c’est que ce n’est pas installé.","code":"\n.libPaths()\n.libPaths(c(\"C:/Program Files/R/R-4.5.2/library\", #chemin existant préalablement \n            \"D:/packages\"))# nouveau chemin pour les packages\nrmarkdown::pandoc_version()## [1] '3.6.3'"},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"débuter-avec-son-logiciel-de-statistiques","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4 Débuter avec son logiciel de statistiques","text":"","code":""},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"débuter-avec-r","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4.1 Débuter avec R","text":"","code":""},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"lenvironnement-r-dans-rstudio","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4.1.1 L’environnement R dans RStudio","text":"La Figure 4.2 représente la fenêtre qui s’ouvre lorsque vous lancez RStudio,\nFigure 4.1: Fenêtre d’accueil dans RStudio.\nTrois parties apparaissent :la console (grosse flèche noire) ;l’environnement (l’encadré jaune) ;et les graphiques (l’encadré vert).Il est possible de taper des lignes de commandes directement dans la console. Ainsi, vous pourriez par exemple utiliser la console comme calculatrice. Il va de soi que R offre bien plus de possibilités que de l’utiliser comme calculatrice. Se servir de R en l’utilisant comme une calculatrice reviendrait approximativement à acheter le dernier smartphone haut de gamme pour s’en servir comme réveil. Néanmoins, quand débute, il est pédagogiquement nécessaire de comprendre comment R fonctionne et ces exemples simplistes permettent d’y contribuer.Ainsi, si vous tapez dans la console 5+3 et que vous appuyez sur la touche ‘entrée’ de votre clavier, voici ce que vous allez obtenir :","code":"\n5+3## [1] 8"},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"le-script","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4.1.2 Le script","text":"Remarquez que nous avons utilisé R de manière triviale en demandant de réaliser une opération arithmétique simple. Cependant, quand le traitement des données va commencer à se complexifier et qu’il faudra utiliser plusieurs fonctions (la notion de fonction sera explicitée un peu plus loin) pour obtenir les résultats souhaités, vous conviendrez que cette pratique sera loin d’être optimale. Tout d’abord, ce ne sera pas pratique de devoir retrouver, chaque fois que vous en aurez besoin, les fonctions qui vous seront utiles et les réécrire. En d’autres termes, la console ne permet pas de garder aisément une trace des opérations qui ont été faites. , il semble évident qu’il est utile d’avoir une trace de ce que vous faites, avec éventuellement la possibilité d’y faire vos propres annotations pour vous aider à vous rappeler la manière dont vous devez utiliser la fonction ou à quoi elle sert. Par ailleurs,pour aller retrouver un résultat que vous avez obtenu précédemment, naviguer dans la console sera fastidieux. Enfin, si vous vous trompez et qu’un message d’erreur apparaît, il n’est pas commode d’utiliser la console pour la corriger.C’est pour cette raison qu’utiliser R directement dans la console est à proscrire. Votre courbe d’apprentissage serait bien plus lente et la démotivation vous gagnerait rapidement.Une manière bien plus adaptée d’utiliser R dans RStudio est d’utiliser un script. Pour cela, il faut cliquer sur la feuille blanche entourée d’un cercle rouge sur la Figure 4.2, et choisir dans le menu déroulant ‘R script’. Vous obtenez alors une nouvelle fenêtre (voir Figure 4.3) :\nFigure 4.2: Procédure de création d’un script dans R.\n\nFigure 4.3: Procédure de création d’un script dans R.\nNous allons donc commencer par donner un titre à notre script. Toute ligne ou tout information précédée par un dièse (#) est interprété comme du commentaire. Nous pourrions donc donner comme nom au script “Introduction à R”. En l’entourant de dièse, il ressortira et permettra d’identifier rapidement de quoi traite le script.Alternativement, vous pouvez créer des chapitres dans Rstudio en cliquant sur Code et ensuite sur Rstudio. En donnant un nom à la section, vous pourrez par la suite naviguer aisément entre les sections de votre script en cliquant sur code et ensuite sur jump .Vous l’aurez compris, vous devriez faire un script pour chaque chapitre ou faire un seul script. Sachant que les scripts peuvent être aussi long que vous le souhaitez, il est théoriquement possible de ne faire qu’un seul et unique script avec toutes les thématiques. Néanmoins, il vous sera plus facile de faire et d’utiliser des scripts plus courts qui ne traitent que d’une thématique.Á présent que nous avons une section, nous allons pouvoir découvrir les opérations de base qu’fait habituellement avec une calculatrice.Dans le script, contrairement à ce qui se passe avec la console, rien ne se passe lorsque vous écrivez une ligne de commande. La raison est que la console interprète directement la ligne de commande quand appuie sur entrée alors que le script ne le fait pas. La ligne de commande sera exécutée uniquement quand vous en ferez la demande explicite. Pour cela, il y deux possibilités : soit vous sélectionnez une ligne (ou un ensemble de lignes à devoir exécuter), et vous cliquez sur ‘Run’ en haut à droite de la fenêtre du script ; soit vous sélectionnez une ligne (ou un ensemble de lignes à devoir exécuter), et vous appuyez sur les touches ctrl et entrée (pomme et entrée pour les mac users) de votre clavier. Dans les deux cas, les résultats vont s’afficher dans la console et vous obtiendrez ceci :Avant de continuer, pensez à enregisgtrer votre script. Quand vous cliquez sur la petite disquette, RStudio vous propose de choisir un type d’encodage. Vous pouvez laisser la valeur par défaut, ou éventuellement choisir “UTF-8” (voir Encadré 1 pour plus de détails).\nLe type d’encodage des scripts \nSur MacOS et sur Linux, “UTF-8” est la valeur par défaut. Pour les utilisateurs windows, la valeur par défaut est ‘ISO-8859-1’. Concrètement, cela n’pas de réel impact tant que vous n’utilisez pas de caractères spéciaux, c’est-à-dire et sans être exhaustif l’un des caractères suivants : é,è, ê, à, ç. En revanche, dès qu’il y des caractères spéciaux, les choses vont se compliquer. Donc si vous commentez en anglais, pas de souci d’utiliser l’un ou l’autre ; en français, cela peut poser souci. Tant que vous êtes le seul utilisateur du fichier, il sera lisible. En revanche, si vous souhaitez partager votre script, les caractères spéciaux ne seront pas lisibles si l’encodage n’est pas bon. Bref, si un utilisateur MacOS travaille avec un utilisateur Windows, il est préférable d’utiliser “UTF-8” qui est considéré comme un format universel. Si vous ouvrez un script où certains caractères ne sont pas lisibles, vous pouvez le rendre lisible en demandant à Rstudio de rouvrir le script avec le bon encodage. Pour cela, clique sur ‘file’, ensuite sur ‘reopen encoding’ et choisit l’encodage.Á présent que vous avez choisi l’encodage, vous pouvez donner un nom à votre script. Pour pouvoir le réutiliser par la suite, il faut que l’extension du fichier, c’est-à-dire ce qui vient après le dernier point soit un R majuscule. Ainsi, j’ai donné le nom suivant à mon fichier : Introduction_R.R.","code":"\n###################################\n###      Introduction à R      ####\n###################################\n5+3 # Addition \n5-3 # Soustraction \n5*3 # multiplication \n5/3 # division \n5^3 # exposant. Donc, ici, 5 exposant 3## [1] 8## [1] 2## [1] 15## [1] 1.666667## [1] 125"},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"créer-des-sections-et-annoter-votre-script","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4.1.3 Créer des sections et annoter votre script","text":"Peut-être avez-vous remarqué que j’ai mis à côté de chaque opération un dièse avec une information textuelle. Cela m’permis d’expliquer à quoi correspondait chacune des lignes de commande. Cette opération peut se faire en annotant à côté de la ligne de commande ou au-dessus de celle-ci (il est théoriquement possible d’annoter en-dessous mais ce n’est pas une procédure conventionnelle).Même si l’utilité d’une fonction, ou son utilisation vous semble triviale, prenez le réflexe d’annoter votre script. Les annotations servent à vous rappeler à quoi sert une fonction, à expliquer à d’autres personnes (y compris à vous-même plus tard) ce que vous avez fait et à indiquer les difficultés que vous avez rencontrées avec la fonction et la manière d’y faire face.Á présent que nous avons succinctement abordé l’utilisation de R comme s’il s’agissait d’une calculatrice, nous pourrions vouloir commencer à utiliser R de manière un peu plus élaborée. va donc aborder une nouvelle thématique.Vous pourrez (et c’est recommandé) créer des délimitations dans votre script afin d’indiquer ces changements de thématique. Ces délimitations s’appellent des sections. Vous pourriez ainsi utiliser des sections pour identifier chacun des sous-chapitres. Pour créer une section, le plus simple est de cliquer sur ‘Code’ en haut de la fenêtre, et ensuite sur ‘insert section’. L’alternative est de créer la section en appuyant simultanément sur les touches ‘Ctrl’, majuscule et ‘R’. Ainsi, en étant au-dessus des lignes de commandes relatives aux opérations mathématiques, vous pourriez créer une première section intitulée ’utiliser R comme une calculatrice et vous verriez cette ligne apparaître dans votre script (voir Figure 4.4). Il faut noter que vous pouvez directement recopier cette ligne et adapter le titre pour créer de nouvelles sections.\nFigure 4.4: Créer une section intitulée : utiliser R comme calculatrice\n\nNaviguer entre les sections de votre script. \nAstuce : si vous cliquez à présent sur ‘Code’ en haut de votre écran et ensuite sur ‘Jump ’, vous allez voir les différentes sections de votre script et vous pourrez naviguer aisément entre ces différentes parties.","code":"\n# Addition\n5+3  \n# Soustraction \n5-3 \n# multiplication \n5*3 \n# division \n5/3 \n# exposant. Donc, ici, 5 exposant 3\n5^3 \n# Utiliser R comme une calculatrice ---------------------------------------"},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"les-fonctions","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4.1.4 Les fonctions","text":"Les fonctions représentent des lignes de commande qui permettent de réaliser des tâches particulières. Par exemple, il y une fonction pour calculer une moyenne, il y une fonction aussi pour compter le nombre d’observations.\nEn réalité, il y des milliers de fonctions dans R. Dans cette section, nous allons décrire quelques fonctions de manière succinctes, dont certaines feront l’objet d’un approfondissement dans les chapitres suivants.D’un point de vue formel, une fonction un nom. Par exemple, la fonction pour calculer une moyenne est mean. Le nom de la fonction est suivi de parenthèses. Á l’intérieur de ces parenthèses, vous allez spécifier ce qu’appelle les arguments. Si je reviens à la fonction qui permet de calculer la moyenne, un des arguments de la fonction consiste à fournir les valeurs numériques sur lesquelles la moyenne doit être calculée.Commençons avec deux fonctions assez simples car elles n’ont pas d’argument, la fonction getwd permet de savoir dans quel répertoire est en train de travailler, et la fonction dir qui permet d’obtenir la liste des fichiers disponibles dans ce répertoire.Pour aborder ces fonctions dans le script, nous allons commencer par créer une nouvelle section que nous allons intituler Les fonctions de base.Nous pouvons à présent utiliser ces deux fonctions sans argument :Par défaut, R va utiliser le dossier ‘documents’ de votre ordinateur comme répertoire de travail, dont le chemin d’accès prendra approximativement la forme de C:/Users/votre_nom/Documents. Ce répertoire de travail n’est sans doute pas le répertoire dans lequel sont localisées vos données, ni celui où vous souhaiteriez travailler. Avoir comme répertoire de travail le répertoire dans lequel se situe l’ensemble des documents est une règle de bonne pratique. Vous pourriez, par exemple, créer dans vos documents un dossier intitulé Statistiques3 et à l’intérieur de ce dossier, un sous-dossier appelé “Introduction” qui contiendra le script pour ce chapitre et les données qui seront utilisées ultérieurement.Il faut à présent préciser à R que votre répertoire de travail ce situe par exemple dans C:/Users/votre_nom/Documents/Statistiques/Introduction. Pour cela, nous allons utiliser la fonction setwd. Cette fonction un argument. Cet argument (code>dir pour directory en anglais) est le répertoire dans lequel vous souhaitez travaillerVous venez d’essayer de changer le répertoire de travail et vous avez reçu un message d’erreur qui prend la forme suivante :Vous avez probablement commis une ou plusieurs des erreurs suivantes :Le dossier n’existe pas. Vous avez oublié de créer le dossier dans lequel vous voulez travailler. Le dossier doit déjà exister sur votre ordinateur ;Le chemin n’est pas correct. Vous avez fait une faute de frappe, ou oublié un sous-répertoire dans le chemin d’accès ;Vous n’avez pas respecté la casse. Comme pour les mots de passe, R est sensible à la casse, c’est-à-dire au respect des majuscules et des minuscules ;Vous êtes sur Windows et vous avez copier/coller le chemin d’accès depuis la fenêtre de navigation. Cependant, R souhaite le symbole slash pour accéder à un dossier enfant, c’est-à-dire un dossier imbriqué dans un autre dossier alors que windows utilise par défaut le backslash. Il faut donc inverser tous les backslash en slash.\nTravailler directement dans le bon environnement de travail. \nIl est possible de se trouver directement dans le bon environnement de travail sans à avoir à le préciser. Pour cela, il faut créer un projet. Dans Rstudio, il suffit de clquer sur File et ensuite New Project. Après l’avoir sauvegardé à l’endroit pertinent, il suffira d’ouvrir sur le fichier créer pour que le répertoire de travail soit automatiquement celui dans lequel le projet été enregitré.Une partie de ces fonctions sont disponibles directement lorsqu’installe R, mais la grande majorité de ces fonctions ont été programmées par les contributeurs qui ont été évoqués à la section expliquant pourquoi il est pertinent d’utiliser R. Ces contributeurs n’ajoutent pas directement les nouvelles fonctionnalités au logiciel mais vont empaqueter les fonctionnalités qu’ils ont programmées dans ce qu’appelle un package. Ces packages devront être installés en plus pour pouvoir être utilisés.Si vous avez du mal à comprendre ce qu’est un package, pensez à votre smartphone : quand vous l’allumez pour la première fois, il n’y que les fonctionnalités de base et vous allez installer les applications selon vos besoins. R fonctionne sur un principe un peu similaire, à la différence qu’ajoute ces nouvelles fonctionnalités à l’intérieur d’un logiciel. Dans un premier temps, nous allons utiliser le package readxl (Wickham & Bryan, 2022) pour importer des fichiers excel, et le package dplyr (Wickham et al., 2022) pour avoir des fonctionnalités de manipulation des données. Nous allons donc installer et charger ces packages. Pour installer un package, utilise la fonction install.packages. Cette fonction pas mal d’arguments mais il ne semble pas utile de les expliciter tous en détail. Nous nous limiterons à installer un package de manière classique en invoquant simplement le nom du package à installer.Vous savez que les packages sont installés correctement par le fait que R vous indique que le package été décompressé avec succès et que la somme des MD5 été vérifiée (package ‘readxl’ successfully unpacked MD5 sums checked). Il arrive néanmoins que certains packages ne s’installent pas aussi aisément. Nous renvoyons le lecteur vers la section “installer les packages” pour une description plus détaillée de cette fonction et les difficultés qui peuvent être associées à l’installation de certains packages.Remarquez qu’il est possible d’utiliser une fonction avec des arguments sans invoquer l’argument de manière explicite. Dans la fonction install.packages, le premier argument s’appelle ‘pkgs’. Il s’agit du nom du package à installer. Si vous respectez l’ordre des arguments, il n’est pas indispensable de les appeler explicitement (comme je l’ai fait pour l’installation du package dplyr) mais si vous ne respectez pas l’ordre ou si vous omettez des arguments intermédiaires pour lesquels vous souhaitez garder les valeurs par défaut, alors il faut les appeler explicitement. En l’occurrence, l’argument pkgs est le premier argument et je peux donc permettre d’omettre de le spécifier quand j’utilise la fonction. En revanche, c’est une pratique qu’il est préférable d’utiliser uniquement avec les fonctions qu’connait parfaitement car cela pourrait entraîner des messages d’erreur ou des résultats qui ne correspondent pas à ce qui était souhaité.Á présent que nos deux packages sont bien installés, nous pouvons les charger pour pouvoir utiliser leurs fonctionnalités ultérieurement. les charge avec la fonction library en indiquant le nom du package sans les guillemets.Charger les packages dont va avoir besoin en début de script représente également une règle de bonne pratique.\nTrouver de l’aide sur une fonction. \nComme évoqué précédemment, il existe une large communauté d’utilisateurs de R qui s’entraident en ligne. Néanmoins, si vous connaissez la fonction que vous devez utiliser, la stratégie de première intention que vous devriez utiliser pour comprendre comment utiliser une fonction est de consulter l’aide qui lui est associée. En effet, cela fait partie du cahier de charge de R de fournir une documentation détaillée quant à l’utilisation des fonctions, avec une explication de chacun des arguments et des exemples sur la manière d’utiliser la fonction. Pour accéder à cette aide, il existe deux manières : utiliser le point d’interrogation suivi du nom de la fonction ou utiliser la fonction help avec le nom de la fonction entre parenthèse.Cette manière de trouver de l’aide sur l’utilisation d’une fonction est une stratégie indispensable car, pour l’utilisateur débutant, l’utilisation des fonctions n’est pas suffisamment automatique et l’aide permet de fournir les indices indispensables pour savoir comment utiliser une fonction et, pour l’utilisateur expert, il se rappellera du nom d’une fonction qu’il n’utilise pas souvent mais pas forcément du nom des arguments ou de la manière de les utiliser.En revanche, cette stratégie ne fonctionne que si la fonction est accessible dans R. Vous ne pourriez pas trouver de l’aide de cette manière pour une fonction qui est dans un package qui n’est pas installé ou qui n’est pas chargé. Pour trouver de l’aide sur une fonction d’un package installé mais pas chargé, peut utiliser deux points d’interrogation successifs avant le nom de la fonction. R va alors chercher dans tous les packages qui sont installés s’il trouve une fonction qui contient le nom que vous rechercher. Á des fins d’illustrations, installez le package psych (Revelle, 2025) sans le charger et vous pourrez trouver de l’aide sur la fonction describe en utilisant les deux points d’interrogation de la manière suivante :","code":"\n# Les fonctions  de base ---------------------------------------\ngetwd() # permet d'obtenir le répertoire de travail\ndir() # permet d'obtenir la liste des fichiers disponibles dans le répertoire \n# Ligne de commande permettant de préciser le répertoire de travail\nsetwd(dir=\"C:/Users/XXX/Documents/livre de statistiques/Introduction\") # Attention que \"XXX\" renvoie à votre nom et que vous devez adapter ce chemin d'accès à votre ordinateurError in setwd(\"C:/Users/XXX/Documents/Statistiques/Introduction\") : cannot change working directory\ninstall.packages(pkgs = \"readxl\") # installation du package readxl - en utilisant l'argument pkgs\ninstall.packages(\"dplyr\") # installation du package dplyr - en omettant l'argument pkgs\nlibrary(readxl) # chargement du package readxl \nlibrary(dplyr) # chargement du package dplyr \n?setwd # recherche de l'aide pour la fonction setwd en utilisant le point d'interrogation \nhelp(install.packages) # recherche de l'aide pour la fonction setwd en utilisant la fonction help\n??describe # cherche l'aide d'une fonction appelée describe dans n'importe quel package installé sur votre ordinateur, même s'il n'est pas chargé. "},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"les-objets-dans-r","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4.1.5 Les objets dans R","text":"Lorsque vous utilisez une des lignes de commande décrite ci-dessus, une fois le résultat obtenu, ce résultat est devenu inutilisable car les informations n’ont pas été stockées dans la mémoire de R. , il pourrait être utile de pouvoir réutiliser le résultat produit. Par exemple, lorsque vous utilisez la fonction dir, vous avez la liste des fichiers disponibles dans votre dossier. Imaginons que ces fichiers soient les résultats individuels de chacun de vos participants et que vous avez 200 participants. Cela pourrait être plus que fastidieux de devoir importer les données de chaque participant les uns après les autres. voudrait pouvoir les importer en une seule fois. Cependant, si la liste des fichiers n’est plus utilisable, ne peut pas le faire. Il est donc nécessaire de stocker ces informations dans la mémoire de R. fait cela en créant des objets.Il existe trois manières de stocker un objet dans la mémoire de R : le symbole ‘<’ et ‘-’ pour créer une flèche orientée vers la gauche ; le symbole ‘>’ et ‘-’ pour créer une flèche orientée vers la droite ; le symbole ‘=’. Nous allons illustrer la création d’objets avec ces trois méthodes. Dans un premier temps, nous allons attribuer la valeur 5 à un objet appelé ‘’. Je vais attribuer la valeur 3 à un objet que je vais appeler ‘b’ et enfin, l’objet ‘c’ sera l’addition entre et b.De ces 3 méthodes que nous avons évoquées pour créer des objets, l’utilisation du symbole ‘=’ est à déconseiller car il est préférable d’utiliser exclusivement cette procédure pour l’attribution des valeurs à un argument d’une fonction. Par exemple, lorsque nous avons utilisé la fonction setwd, il fallait préciser le répertoire de travail, qui est l’argument ‘dir’, et nous l’avons fait avec le symbole ‘=’.L’utilisation des deux autres méthodes se valent mais préférez faire vos flèches toujours dans le même sens pour vous éviter des écueils. Je vous encourage à les faire toujours vers la gauche (comme pour le ‘’ dans le code ci-dessus) afin d’avoir le nom des nouveaux objets en début de ligne à chaque fois. Quand vous ferez plusieurs opérations à la suite, cela vous évitera de passer du temps à chercher le nom que vous avez attribué à un objet 10 lignes de commandes plus haut.Si exécute les trois lignes de commandes ci-dessus, apparemment rien ne se passe. Il n’y rien qui s’affiche dans la console. Pour voir ce que contiennent les objets, il faut les appeler. Ainsi, en tapant leur nom et en cliquant sur ‘run’, vous allez voir ;Le nom de ces objets est pratiquement complètement arbitraire et vous pourriez l’appeler ‘ficus’, ‘Xmen’, ‘HarryPotter’ si vous le souhaitiez. En revanche, si vous l’appelez ‘HarryPotter’, pour pouvoir l’utiliser, il faudra écrire ‘HarryPotter’. Il est donc préférable d’utiliser un nom court pour éviter de devoir retaper des noms interminables.Et si vous faites une faute de frappe (y compris sur la casse4), alors R va renvoyer un message d’erreur indiquant qu’il ne trouve pas l’objet appelé (Error: object ‘Harrypotter’ found).Même si peut utiliser pratiquement n’importe quel nom pour les objets, certaines règles doivent impérativement être respectées. D’abord, vous ne pouvez pas utiliser un nom qui contient des espaces. Dans ce cas, R va renvoyer un message d’erreur indiquant qu’il y un symbole inattendu. Ce symbole est l’espace. En revanche, vous pouvez utiliser un underscore ou un point (mais il est en revanche prohibé d’utiliser une virgule, un point-virgule ou un deux points) pour faire vos séparations.Ensuite, il faut éviter tous les caractères spéciaux. va commencer par la situation la plus pernicieuse : les accents, les ‘ç’, et autres trémats. Je vais attribuer la valeur “Jean” à un objet appelé “élève”Là, tout va bien. Cependant, certaines fonctions ne vont pas reconnaître les accents et vous ne comprendrez pas pourquoi la fonction renvoie un message d’erreur alors que votre objet ne présente aucun problème en apparence (ici le souci est lié au type d’encodage des caractères spéciaux, dont nous avons parlé précédemment). Vous pourriez passer des heures à vous demander quel est le problème et vous finiriez par vous décourager pour un problème aussi trivial. La règle à appliquer est simple : n’utilisez jamais d’accent ou de caractères spéciaux.La dernière règle à appliquer est qu’il ne faut aucun symbole :pas de parenthèse, R considérerait ce qui précède comme une fonction ;pas d’apostrophe car ce serait interprété comme une chaîne de caractères ;pas de symbole mathématique car il voudrait réaliser une opération mathématique.Voici quelques exemples de ce qu’il ne faut pas faire :Remarquez également que, dans votre environnement global (l’encadré jaune de la Figure 4.2), vous avez à présent plusieurs objets que vous pouvez réutiliser pour des traitements ultérieurs. Si pour des opérations aussi triviales que celle qu’vient de voir, cela peu d’intérêt, n’oubliez pas que l’objectif sera fine de pouvoir utiliser des modèles statistiques sur des jeux de données réels et qu’il sera important de pouvoir passer par des étapes intermédiaires.En l’occurrence, il y peu d’objets dans la mémoire de R. Cependant, il peut y en avoir beaucoup plus et vous pourriez vouloir en obtenir la liste. Pour cela, utilise la fonction ls sans devoir préciser d’argument.","code":"\na<-5 # créer un objet appelé a et ayant comme valeur 5\n3 ->b # créer un objet appelé b et ayant comme valeur 3\nc=a+b # créer un objet appelé c et ayant comme valeur la somme de a et b\n# Identifier ici l'argument 'dir' à l'intérieur de la fonction et que le symbole utilisé pour lui attribuer une valeur est '='\nsetwd(dir=\"C:/Users/XXX/Documents/Statistiques/Introduction\")\na # appeler l'objet a, renvoie 5## [1] 5\nb # appeler l'objet b, renvoie 3## [1] 3\nc # appeler l'objet c, renvoie 8## [1] 8\n# Illustration du côté arbitraire des noms \nficus<-5\nXmen<-5\nHarryPotter<-5\nficus## [1] 5\nXmen## [1] 5\nHarryPotter## [1] 5\nHarrypotter # Harrypotter avec un \"p\" minuscule n'existe pas et renvoie un message d'erreurMa valeur cinq<-5 # renvoie un message d'erreur en raison des espaces \nMavaleurcinq<-5 # ne renvoie pas de message d'erreur \nMa_valeur_cinq<-5 # ne renvoie pas de message d'erreur quand on utilise un underscore\nMa.valeur.cinq<-5 # ne renvoie pas de message d'erreur quand on utilise un point\nélève<-\"Jean\"\nélève## [1] \"Jean\"# Quelques exemples de ce qu'il ne faut pas faire \ninterprete(ma.fonction)<- 5 # ne fonctionne pas et renvoie le message d'erreur indiquant que cette fonction ne peut être trouvée\nc'est_pas_terrible<-5 # l'apostrophe pose souci \nceci+cela+non+plus<-5 # le '+' est interprété comme une opération mathématique, de même s'il y avait '-','/','*','^'. \npas_d_egal=erreur<-5 # mettre un \"=\" pose souci car on crée deux objets différents : pas_d_egal et erreur  \nls() # connaître la liste des objets dans la mémoire de R.##  [1] \"a\"               \"arrows\"          \"b\"               \"boxes\"          \n##  [5] \"c\"               \"cards\"           \"condition_label\" \"easier\"         \n##  [9] \"élève\"           \"ficus\"           \"ft\"              \"HarryPotter\"    \n## [13] \"lines\"           \"tot\"             \"tot2\"            \"Xmen\""},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"les-erreurs-les-plus-courantes-et-les-règles-de-bonnes-pratiques","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4.1.6 Les erreurs les plus courantes et les règles de bonnes pratiques","text":"Le plus difficile quand débute avec R est d’être confronté à un message d’erreur et de ne pouvoir lui attribuer du sens.Il va de soi qu’il n’est pas possible de faire une liste exhaustive des erreurs possibles mais voici les plus fréquentes, et leurs solutions :Table 4.1: Liste des erreurs les plus fréquentes et leur cause/solutionErreurSignification et solutionunexpected symbolLe nom de l'objet ou de la variable est incorrect. Vérifiez les règles énoncées précédemmentobject XXX foundL'objet n'pas été créé ou est mal orthographie. Attention à la casse - càd. aux majuscules et minusculescould find function XXXLa fonction est mal orthographiée ou est dans un package non chargé, voire non installé. Cette erreur est courante quand utilise une fonction qu'trouve sur le internet sans vérifier le package auquel elle appartient. Chargez le package contenant la fonction si elle existe.Error cor.test.default(c(\"\", \"b\")) : x doit être un vecteur numériqueLes arguments doivent avoir certaines caractéristiques. Ici, ils doivent être numériques mais ce n'est pas le cas. Vérifiez avec la fonction class ou str si les objets sur lesquels vous souhaitez faire l'analyse ont le format correctargument length zeroUn argument indispensable pour valeur NULL alors qu'il doit être spécifié. Vérifiez l'aide de la fonctionno default argumentUn argument nécessaire n'pas été spécifié. Vérifiez l'aide de la fonctionaucun package nommé XXX n'est trouvéLe package n'est pas installé ou est mal installé. Si vous avez tenté d'installer le package mais que le souci persiste, voir la section installer les packages récalcitrants ens section 6.Néanmoins, en appliquant les règles de bonnes pratiques suivantes, vous diminuerez considérablement les erreurs :Utiliser un script pour toutes les fonctions que vous utilisez.Avant de commencer les analyses, pensez à charger les packages et vérifier qu’il n’y pas de problème concernant leur chargement5.Annotez votre script pour savoir ce que vous faites, mais également pour expliquer les erreurs auxquelles vous avez été confrontées et comment cela été résolu.Faites attention à la casse6.Utilisez des noms simples et pour les noms complexes, favorisez les copier coller en utilisant la foncton ls pour avoir le nom des objets en mémoire et la fonction names pour avoir le nom des variables. PAr exemple :Enfin, les outils d’intelligence artificiel représente des aides précieuses pour vous aider à identifier pourquoi une erreur est survenue et pour vous aider à corriger une ligne de commande. Le danger est de trop se reposer sur l’IA sans comprendre ce qu’fait. Dans ce genre de situation, il peut arriver que l’IA ne vous fournisse pas une solution adaptée, même avec un bon prompt, et vous ne pourrez pas régler la difficulté à laquelle vous êtes confronté·e. Il est donc indispensable de comprendre en premier et de profiter de l’aide de l’IA dans un second temps.","code":"\ndata(mtcars)\nnames(mtcars)##  [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n## [11] \"carb\""},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"installer-des-packages-récalcitrants","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4.1.7 Installer des packages récalcitrants","text":"Précédemment, nous avons décrit comment installer des packages. Néanmoins, certains peuvent être plus difficiles à installer que d’autres pour différentes raisons. Voici les solutions à envisager pour installer ces packages. Pour l’illustration, nous utiliserons le package ‘devtools’ (Wickham et al., 2025).Ainsi, quand un package ne veux pas se charger, c’est qu’il n’est pas correctement installé. La procédure ci-dessus explique comment forcer cette installation lorsque les choses ne se passent pas correctement. Chaque étape peut se suffire à elle-même. Néanmoins, si l’installation continue à poser problème, alors, il faut passer à l’étape suivante. Pour le formuler autrement : je tente l’étape 1, ça marche, je m’arrête ; ou, je tente l’étape 1, ça ne marche pas, je passe à l’étape 2.Forcer l’installation des dépendances, le site miroir et les packages compilés. Quand nous avons décrit la fonction install.packages, elle été présentée de sorte à pouvoir l’utiliser sans argument. peut néanmoins préciser des arguments supplémentaires. Le premier argument est l’argument dependencies qui permet de forcer l’installation des dépendances en indiquant l’opérateur logique TRUE. Le second argument qu’va utiliser est l’argument type pour forcer à installer les packages sous une forme compilée (binaire) plutôt que sous une forme où le package devra être compilé (source). Ainsi, cela évite de devoir avoir un logiciel de compilation, comme Rtools. Enfin, va préciser le site miroir sur lequel veut télécharger les packages qu’souhaite installer. Dans cet exemple, il s’agit du CRAN de Lyon.Fermer R et refaire l’étape 1. Il arrive que certaines dépendances soient chargées, empêchant dès lors leur mise à jour et l’installation du nouveau package. Le fait de fermer R permet de régler le souci dans la plupart des situations.Fermer R et refaire l’étape 1. Il arrive que certaines dépendances soient chargées, empêchant dès lors leur mise à jour et l’installation du nouveau package. Le fait de fermer R permet de régler le souci dans la plupart des situations.Vérifier l’espace sur le disque dur. Il arrive parfois que les packages ne s’installent pas parce que l’espace sur le disque dur est insuffisant. S’il est suffisant, passez à l’étape 4. S’il est insuffisant, il existe deux solutions, une assez technique qui consiste à choisir comme répertoire d’installation des packages un support externe. Cela implique de forcer R à aller chercher les packages dans ce dossier. L’autre solution est de travailler en ligne en créant un compte Rstudio cloud.Vérifier l’espace sur le disque dur. Il arrive parfois que les packages ne s’installent pas parce que l’espace sur le disque dur est insuffisant. S’il est suffisant, passez à l’étape 4. S’il est insuffisant, il existe deux solutions, une assez technique qui consiste à choisir comme répertoire d’installation des packages un support externe. Cela implique de forcer R à aller chercher les packages dans ce dossier. L’autre solution est de travailler en ligne en créant un compte Rstudio cloud.Le nom d’accès du dossier d’installation des packages contient des caractères spéciaux. Á nouveau, il existe deux solutions, forcer l’installation des packages dans un autre dossier (mais cela implique de préciser le chemin de ce dossier) ou créer un compte en ligne.Le nom d’accès du dossier d’installation des packages contient des caractères spéciaux. Á nouveau, il existe deux solutions, forcer l’installation des packages dans un autre dossier (mais cela implique de préciser le chemin de ce dossier) ou créer un compte en ligne.","code":"\ninstall.packages(\"devtools\", # nom du package\n                 dependencies = TRUE, # force l'installation des dépendances\n                 type= \"binary\", # on installe uniquement les packages sous un format compilé\n                 repos = \"https://mirror.ibcp.fr/pub/CRAN\") # le site miroir CRAN est celui de Lyon "},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"pour-conclure","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4.1.8 Pour conclure","text":"L’objectif de ce chapitre était de fournir les bases indispensables à l’utilisation de R, comprendre ce qu’est un object, pouvoir utiliser une fonction et adopter les bonnes pratiques pour éviter les erreurs.\nÁ présent qu’s’est dégagé des contraintes relatives à l’utilisation du logiciel, il est possible à présent de s’intéresser au raisonnement statistique et de pouvoir réaliser les analyses adaptées.","code":""},{"path":"débuter-avec-son-logiciel-de-statistiques.html","id":"débuter-avec-easier","chapter":"4 Débuter avec son logiciel de statistiques","heading":"4.2 Débuter avec easieR","text":"Comme évoqué dans la section consacrée au choix des logiciels, le package easieR (Stefaniak, 2018) peut être utilisé de différentes manières. En particulier, peut l’utiliser en boîte de dialogue uniquement, en ligne de commande uniquement, ou un mixte entre les deux. Dans cette section, nous fournissons les informations pour l’utiliser en boîte de dialogue uniquement dans sa forme la plus générale. Ainsi, pour utiliser easieR, il faut charger la package avec la fonction library et peut accéder à toutes les fonctions de easieR avec la fonction easieR().","code":"\nlibrary(easieR) #charge le package\n# ouvre la boîte de dialogue permettant d'accéder à toutes les options\neasieR() "},{"path":"les-données.html","id":"les-données","chapter":"5 Les données","heading":"5 Les données","text":"Les statistiques n’ont d’intérêt qu’à deux conditions principales.Premièrement, il faut que ce qui est mesuré ait une issue incertaine. Cela implique qu’il faut mesurer quelque chose ayant un caractère aléatoire (par exemple, ne connait pas à l’avance la couleur des cheveux de la prochaine personne qu’va croiser dans la rue) afin de tester une hypothèse dont l’issue est incertaine (par exemple, peut faire l’hypothèse qu’il y plus de personnes aux cheveux clairs dans le nord de l’Europe que dans le sud de l’Europe, mais san l’avoir mesuré, ne peut pas en avoir la certitude).\nCette mesure aléatoire est ce qu’appelle une variable.La seconde condition est de réaliser les analyses sur des données de qualité. Dès lors qu’il y des erreurs dans les données, les analyses statistiques perdent toute leur utilité. Il est donc nécessaire de préparer les données avec soin, tant lorsqu’les encodage dans le jeu de données, que quand les manipule. Cependant, même si applique le plus grand soin dans la création du jeu de données, des erreurs peuvent s’immiscer. Il est bon de mettre en place des procédure de vérification pour éviter ces erreurs. Par exemple, si utilise une échelle dont les valeurs vont de 45 à 155, alors, ne peut pas avoir une valeur à 255. peut donc vérifier que les données sont compatibles avec ce qui été mesuré. Cette procédure peut sembler longue et fastidieuse alors qu’voudrait savoir si nos résultats étayent ou non les hypothèses. Cependant, si considère le temps qui été investi dans la lecture de la littérature pour émettre une hypothèse, le temps pour construire le protocole expérimental, le temps pour recueillir les données, il semble flagrant que réaliser des analyses sur des données qui n’ont pas été vérifiées est faire bien mauvais usage de tout ce temps investi.","code":""},{"path":"les-données.html","id":"les-variables","chapter":"5 Les données","heading":"5.1 Les variables","text":"Quand veut réaliser des statistiques, il est nécessaire de manipuler et/ou de mesurer quelque chose de variable. Si nous mesurons à moult reprises la même valeur, n’pas besoin de statistiques car nous sommes face à des certitudes. Par exemple, il n’est pas nécessaire de faire des statistiques sur le fait de savoir si un·e étudiant·e peut réussir sans venir en cours, sans ouvrir ses cours, sans se renseigner sur ses cours, le tout en allant à l’ensemble de ses examens sans avoir de quoi écrire.Ainsi, puisque pour faire des statistiques, il faut manipuler et/ou mesurer quelque chose qui varie, ce qu’appelle une variable, il s’ensuit que, pour comprendre les statistiques, il est indispensable de pouvoir distinguer toute une série d’informations concernant ces variables.\nPlus précisément, il faut pouvoir distinguer les variables dépendantes des variables indépendantes et pouvoir distinguer la nature des variables.Pour rappel, une variable indépendante est tout ce qu’un·e expérimentateur·rice manipule est une variable indépendante, tandis que tout ce qu’un·e expérimentateur·rice enregistre est une variable dépendante. Maîtriser cette distinction est un prérequis nécessaire pas suffisant.peut rajouter à cette distinction toutes les variables qu’veut contrôler, que nous appelerons les variables de contrôle (ou controlées).Il est également nécessaire de pouvoir distinguer une variable quantitative (appelée aussi numérique ou métrique), d’une variable ordinale, ou qualitative (appelée aussi nominale ou catégorielle).\nNe pas confondre variables et modalités \nIl apparaît que beaucoup d’étudiant·es confondent la variable avec les modalités avec ses modalités. Il est également indispensable de correctement distinguer les deux. La variable un nom qui est un résumé abstrait de l’ensemble des modalités que la variable recouvre. Par exemple, le genre est une variable qui comme modalités : femme, homme, non binaire, autre. Ainsi, le nom de la variable doit englober l’ensemble des modalités.","code":""},{"path":"les-données.html","id":"les-variables-qualitatives","chapter":"5 Les données","heading":"5.1.1 Les variables qualitatives","text":"Les variables qualitatives (ou catégorielles) sont caractérisée par des étiquettes, telle que le genre, la couleur des cheveux ou l’appartenance à un groupe… Elles ne sont pas dotées d’une structure particulière. Les différentes étiquettes permettent simplement de distinguer les individus les uns pdes autres sur une caractéristique précise. Elles sont caractérisées par la propriété d’exclusivité, c’est-à-dire qu’appartenir à la catégorie des hommes implique qu’n’appartient pas à la catégorie des femmes, non binaire ou autre, et par la propriété d’exhaustivité, c’est-à-dire que les modalités doivent pouvoir prendre en compte l’ensemble des cas de figures possibles.Dans la réalité, ces notions d’exhaustivité et d’exclusivité sont parfois compromises. Par exemple, vous voulez ranger des personnes en fonction de leur couleur de cheveux, certains ayant des cheveux bruns, d’autres des cheveux gris. Imaginons à présent une personne dont les cheveux sont bruns mais avec de nombreuses mèches grises (le fameux poivre et sel). Allez-vous le ranger dans la catégorie des cheveux bruns ou dans celle des cheveux gris ?","code":""},{"path":"les-données.html","id":"les-variables-ordinales","chapter":"5 Les données","heading":"5.1.2 Les variables ordinales","text":"Les variables ordinales sont des variables pour lesquelles il est possible d’attribuer un ordre entre les différentes modalités. Classiquement, les échelles ordinales sont les échelles de Likert, qui peuvent prendre 3, 5, 7 ou parfois 9 modalités différentes. La particularité de ces échelles est que la différence observée entre deux modalités n’est pas représentative d’une différence de même ampleur entre deux autres modalités. Par exemple, si vous pose la question « faites-vous du sport ? » et que vous devez répondre sur une échelle du type « moins d’une fois par mois – une fois par semaine – deux à trois fois par semaine – plus de trois fois par semaine ». Une personne qui répond « moins d’une fois par mois » ne fera pas trois fois moins de sport qu’une personne qui répond « deux à trois fois par semaine ». Il est à noter que les statistiques réalisées sur des variables ordinales se réalisent systématiquement sur le rang des différentes observations. Généralement, le nombre de modalités est limité, et dépasse rarement 9 modalités différentes . Il est aussi à noter que, quand les variables ont peu de modalités, en particulier quand il n’y en que 2 ou 3, tendance à utiliser les propriétés qualitatives de la variables plutôt qu’ordinale.","code":""},{"path":"les-données.html","id":"les-échelles-quantitatives","chapter":"5 Les données","heading":"5.1.3 Les échelles quantitatives","text":"Les échelles quantitatives (ou métrique ou continue ou numérique) sont des variables qui présentent, si pas une infinité, un très grand nombre de modalités. Il est théoriquement possible d’entre dans une distinction supplémentaire : les échelles de rapport et les échelles d’intervalle.Sans entrer dans les détails entre les échelles de rapport et les échelles d’intervalle, une des propriétés des échelles de rapport est que la différence et le rapport entre les différentes modalités ont du sens. Par exemple, si je mets deux poids de 5 kilos sur une extrémité d’une balance et un poids de 10 kg sur l’autre extrémité, la balance sera en équilibre7.","code":""},{"path":"les-données.html","id":"type-de-variable-et-leurs-propriétés","chapter":"5 Les données","heading":"5.1.4 Type de variable et leurs propriétés","text":"Il est à noter que les échelles métriques ont toutes les propriétés des échelles ordinales et des échelles qualitatives et que les échelles ordinales possèdent toutes les propriétés des échelles qualitatives. Par contre, les variables qualitatives n’ont pas les propriétés des variables ordinales et les variables ordinales n’ont pas les propriétés des variables qunatitatives. Ces caractéristiques sont particulièrement importantes à partir du moment où, lorsqu’réalise des statistiques, des opérations sur les variables peuvent avoir été réalisée, de sorte à ce la nature de ces variables été modifiée. Dès lors, il est possible de réaliser toutes les analyses faites sur une variable catégorielle quand une variable métrique ; l’inverse n’est en revanche pas possible. Par exemple, si vous avez les poids de vos participants, vous pouvez traiter cette variable comme une variable quantitative ; vous pouvez également la traiter comme une variable ordinale en attribuant une étiquette aux personnes (par exemple : obèse, en surpoids, corpulence normale, mince, maigre ou anorexique) ; enfin, cette variable peut être transformée en une variable nominale (sans ordonnancement) en classant les individus comme présentant un trouble de la masse corporelle (incluant dès lors les obèses et les anorexiques) ou comme ne présentant pas de troubles de la masse corporelle.Lorsque vous devez choisir un test, il vous faut pouvoir identifier correctement la nature des variables. Si vous éprouvez certaines difficultés pour identifier la nature des variables que vous manipulez, deux règles vous permettront de vous aidez :Si vous hésitez entre une variable ordinale et une variable qualitative, considérez que la variable est qualitative puisque il est acceptable d’appliquer les tests destinés à traiter des variables qualitative sur des variables ordinales ;Si vous hésitez entre une variable ordinale et une variable quantitative, considérez le nombre de modalités différentes qui sont à votre disposition. Si votre variable présente au moins 8 modalités différentes, vous pourrez la considérer comme une variable quantitative. En-dessous de 8 modalités différentes, préférez considérer la variable comme étant ordinale8.","code":""},{"path":"les-données.html","id":"les-objets-et-leur-nature-dans-r","chapter":"5 Les données","heading":"5.2 Les objets et leur nature dans R","text":"","code":""},{"path":"les-données.html","id":"la-nature-des-variables-et-des-objets-dans-r","chapter":"5 Les données","heading":"5.2.1 La nature des variables et des objets dans R","text":"Le chiffre 5 et le mot “bonjour” ne sont pas de même nature et vous ne pourriez pas faire les mêmes traitements statistiques sur des valeurs textuelles que sur des valeurs numériques.\nParmi les natures d’objet, celles qui sont particulièrement importantes sont les valeurs numériques, les valeurs entières, les chaînes de caractères, les valeurs logiques et les facteurs. Nous allons illustrer ces différentes natures d’informations et, grâce à la fonction class, vous pourrez identier la manière dont R interprète chacun des objets que vous allez créer.Commençons par la situation la plus simple : les logiques. Les logiques se résument à deux valeurs possible VRAI (TRUE en anglais) ou FAUX (FALSE en anglais). Remarquez que TRUE et FALSE sont en majuscule sans guillemets.Ensuite, nous avons les valeurs numériques. Les valeurs numériques peuvent être des valeurs avec ou sans virgules. Lorsqu’un ensemble de nombres sans virgule, R va leur attribuer comme nature ‘entier’ (integer en anglais). La raison de cette distinction est que R peut communiquer avec d’autres langages de programmation, notamment C et fortran qui requièrent moins d’espace et vont plus vite pour réaliser des calculs sur les entiers que sur les numériques. Pour vous, cette distinction n’pas d’importance, excepté de savoir que R traite les entiers et les numériques comme des nombres.Enfin, la dernière nature que nous aborderons sont les valeurs textuelles, les chaînes de caractères. Dans R, elles sont toujours entre guillemets, simples ou double.Ces chaînes de caractères peuvent être utilisés pour de l’analyse textuelle par exemple. Cependant, quand veut comparer des conditions expérimentales, il faut indiquer à R qu’il s’agit d’un facteur. Comme pour la distinction entre un entier et un numérique, le facteur est plus gourmand en mémoire, raison pour laquelle R privilégie les chaîne de lettres.Une règle d’que vous devriez toujours appliquer est de vérifier si R interprète correctement vos données : est-ce qu’il ne considère des valeurs numériques comme du texte ou un facteur comme une chaîne de caractères. Lorsque ce n’est pas le cas, peut changer la nature d’une information dans le format souhaité en le faisant précéder par “.”. Dans les exemples ci-dessus, j’ai utilisé la fonction .factor pour transformer une chaîne de caractères en facteur, et j’ai utilisé .integer pour transformer une valeur numérique en entier. Il y deux situations où il faut être particulièrement prudent : la transformation d’un facteur en valeur numérique et la transformation d’une valeur en logique.Commençons par la transformation d’une valeur considérée comme un facteur en valeur numérique. Si je reprends ma valeur numérique et que je la transforme en facteur, je n’ai pas de souci.présent, si j’essaie de transformer la valeur en numérique, la valeur n’est plus 2.3 mais 1 :La raison de ce phénomène est qu’un facteur peut être ordonné. Par exemple, les modalités “petit”, “moyen” et “grand” ont un ordre, et vous pourriez vouloir réaliser des analyses en fonction du rang en attribuant les valeurs 1, 2, et 3 à ces différentes modalités. En l’occurrence, ‘numerique.f’ est la première modalité et la transformation en numérique renvoie dans ce cas le chiffre 1.Pour pouvoir transformer une valeur considérée comme facteur en numérique, il est nécessaire de passer par une étape intermédiaire, qui consiste à transformer l’information en caractère avant de la transformer en numérique.La seconde situation qui mérite notre attention est la situation de la transformation en logique. Toute valeur numérique différente de 0 est considérée comme vraie, et le 0 est considéré comme faux. Pour le comprendre, il faut simplement comprendre que, par convention, faux vaut 0 en langage de programmation, et théoriquement vrai vaut 1. R étend cette logique à tous les nombres différents de 0.\nPour les chaînes de texte, l’information n’est pas interprétable en termes de logique et R renvoie NA. ne peut donc pas l’interpréter en tant que tel mais il est possible de transformer des valeurs textuelles en valeur logique en utilisant d’autres stratégies.\nLes valeurs manquantes \nLes valeurs manquantes sont désignées dans R par NA, pour available. Cette notion est essentielle car il faudra prendre régulièrement des décisions sur les valeurs manquantes. Il est donc important d’être capable de les identifier.","code":"\nlogique<-TRUE # l'alternative est FALSE\nlogique2<-F # TRUE et FALSE peuvent être résumés par T et F\nclass(logique) # indique que c'est une valeur logique## [1] \"logical\"\nclass(logique2) # indique que c'est une valeur logique## [1] \"logical\"\nnumerique<-2.3 # valeur numérique avec virgule. Remarquez que le séparateur de décimale est un point\nnumerique.sans.virgule<-5 # valeur numérique sans virgule.\nentier <-as.integer(5) # valeur qui sera considérée comme un entier par R \nclass(numerique) # considérée par R comme une valeur numérique. (numeric) ## [1] \"numeric\"\nclass(numerique.sans.virgule)# considérée par R comme une valeur numérique. (numeric) ## [1] \"numeric\"\nclass(entier)# considérée par R comme une chaîne un entier. (integer) ## [1] \"integer\"\nchaine_textuelle1<-\"hello world\" # remarquez la présence de guillemets doubles\nchaine_textuelle2<-'hello world' # remarquez la présence de guillemets simples\nclass(chaine_textuelle1)  # considérée par R comme une chaîne de lettres. (character) ## [1] \"character\"\nclass(chaine_textuelle2)  # également considérée par R comme une chaîne de lettres. ## [1] \"character\"\nfacteur<-as.factor(\"hello world\" )\nclass(facteur)## [1] \"factor\"\nnumerique## [1] 2.3\nnumerique.f<-as.factor(numerique ) # transformation de la valeur numérique en facteur\nclass(numerique.f) # R l'interprète comme facteur ## [1] \"factor\"\nas.numeric(numerique.f) # transformation d'un facteur en numérique, renvoie ici la valeur de 1. ## [1] 1\nas.numeric(numerique.f) # transformation d'un facteur en numérique, renvoie ici la valeur de 1. ## [1] 1\nnumerique.f<-as.character(numerique.f) # étape intermédiaire consistant à transformer le facteur en une chaîne de caractère\nas.numeric(numerique.f) # transformation de la chaîne de caractère en numérique, ce qui renvoie 2.3## [1] 2.3\nas.logical(1) # une valeur numérique transformée en valeur logique vaut toujours vrai si elle est différente de 0## [1] TRUE\nas.logical(-1)# une valeur numérique transformée en valeur logique vaut toujours vrai si elle est différente de 0## [1] TRUE\nas.logical(0) # une valeur numérique transformée en valeur logique vaut toujours faux si elle est égale 0## [1] FALSE\nas.logical(\"hello world\") # renvoie NA, pour not available car une valeur textuelle n'est pas interprétable en termes logiques. ## [1] NA\nas.logical(facteur)# renvoie NA, pour not available car une valeur textuelle n'est pas interprétable en termes logiques.## [1] NA"},{"path":"les-données.html","id":"les-différents-types-dobjets","chapter":"5 Les données","heading":"5.2.2 Les différents types d’objets","text":"Jusqu’à présent, nous avons traité d’objets pour lesquels il n’y avait qu’une information, un chiffre par exemple. Cependant, les traitements des données vont impliquer de manipuler plusieurs informations en même temps. La difficulté à laquelle vous allez être confronté.e est qu’il existe plusieurs types d’objets, qui ont chacun leurs propriétés et qu’il faut connaître pour pouvoir utiliser correctement R. Par exemple, pour calculer une moyenne, il vous faut un vecteur de valeurs numériques. Si vous ne savez pas ce qu’est un vecteur, vous ne pourrez pas utiliser la fonction. De même, si une fonction requiert une matrice, il faut savoir à quoi cela correspond.\nLes termes ‘matrices’ ou ‘vecteurs’ font généralement assez peur. Cependant,il s’agit de termes compliqués pour des notions qui peuvent être très largement simplifiées.\nBien que ma description soit volontairement imprécise et incomplète, elle suffira largement à la compréhension de la suite.Pour en revenir à notre propos, il existe dans R une multitude de classes d’objets. Si ces différentes classes d’objet ont leur raison d’être, cela dépasse largement les objectifs d’un chapitre d’introduction à R. Nous focaliserons donc sur les manières les plus fréquentes pour stocker, manipuler et traiter un ensemble de données : les vecteurs, les matrices, les dataframes, les tibbles et les listes.Imaginons qu’au lieu de n’avoir qu’une seule valeur numérique, nous en ayant 10. Par exemple, les résultats à un examen de statistiques. Les données pourraient être 12, 13, 9, 8, 15, 18, 4, 11, 13. Nous voudrions pouvoir les regrouper dans un seul objet. Nous pouvons le faire en créant un vecteur. Un vecteur est un ensemble d’informations de même nature, c’est-à-dire seulement des numériques, seulement des logique ou encore seulement des chaînes de caractères, qui n’ont qu’une dimension. Cela signifie que vous pourriez organiser les informations dans une seule colonne OU dans une seule ligne d’un tableau. Dans R, crée ce vecteur avec la fonction c où chaque élément du vecteur est séparé des autres par une virgule.identifie que stocker un ensemble d’éléments dans un seul objet plutôt que dans plusieurs présentent différents avantages. Premièrement, cela évite d’encombrer la mémoire de R avec une multitude d’objets qui risqueraient de vous perdre. Ensuite, et surtout, cela permet de réaliser des traitements sur ces objets. Par exemple, nous avons évoqué la fonction mean qui permet de calculer des moyennes. Cette fonction comme argument un vecteur de valeurs numériques. Nous pouvons donc calculer la moyenne sur notre objet ‘notes’.peut également créer des vecteurs sur des chaînes de caractères. Le point important ici est qu’il ne faut pas oublier de mettre les guillemets pour chaque élement du vecteur.Notez que si vous mélangez des informations de différentes natures, R va harmoniser la nature des informations en fonction de ce qui semble être le plus cohérent. Par exemple, si mélange des lettres et des chiffres, il va considérer l’ensemble des éléments du vecteur comme étant des chaînes de caractères.Les matrices sont des tableaux de données dans lesquels il n’y que des observations de même nature (que des chiffres, que des chaînes de lettres, que des logiques). Une matrice est une table à deux entrées, ayant donc des lignes et des colonnes. La matrice se différencie donc du vecteur par le fait que la matrice plusieurs lignes ET plusieurs colonnes. Les matrices mériteraient un chapitre complet les concernant mais les compétences requises dépasseraient très largement le niveau d’introduction.Table 5.1: Exemple de matrice ayant 3 lignes et 3 colonnes.V1V2V3147258369\nGénérer automatiquement des valeurs. \nIl existe une multitude de fonctions dans R qui permettent de générer automatiquement des valeurs sans avoir à les répéter Parmi ces fonctions, retrouve seq qui permet de générer une séquence (par exemple, répète 3 fois 4 et ensuite 3 fois 5), rep (qui permet de répéter une information), ou encore gl qui permet de générer un facteur. Lorsque les valeurs à générer est une séquence de chiffres, peut utiliser les deux points.Nous allons recréer cette matrice en utilisant la fonction matrix.Dans la plupart des situations, vos données contiendronsdes valeurs de nature différente. Il y aura des valeurs numériques, mais également des facteurs ou des identifiants de participants. Dans ce cas, la classe d’objet dans R n’est pas une matrice mais un dataframe. Pour le formuler autrement, un dataframe toutes les propriétés d’une matrice (s’il ne contient que des valeurs de même nature, le dataframe pourra être considéré comme une matrice), mais également la propriété de pouvoir gérer des informations de nature différente à la condition que le nombre de ligne soit toujours identique. Ainsi, ne peut pas avoir 5 valeurs dans la première colonne et 4 dans la seconde sans préciser qu’il y une valeur qui manque dans la seconde colonne.\nLe Tableau 5.2 un exemple qui pourrait représenter le genre et la taille de différents individus :Table 5.2: Exemple de dataframe avec une colonne contenant un facteur et une colonne contenant des valeurs numériques.sexetailleHomme176Femme173Homme180Femme165Dans R, votre dataframe prendra la forme suivante :Ce format est donc bien plus flexible que la matrice. Il s’agit du format le plus conventionnel avec lequel vous pouvez réaliser le plus d’opérations. Néanmoins, nous devons aborder une autre manière de gérer des jeux de données qui est les tibbles. Ce format été proposé par l’équipe de tidyverse (Wickham et al., 2019)\npour faciliter la manipulation et améliorer l’affichage dans la console. Néanmoins, tout ce que vous pouvez faire avec un tibble, vous pouvez le faire avec un dataframe, l’inverse n’est pas vrai. Par exemple, vous ne pouvez pas donner des noms aux lignes dans un tibble, ni changer la nature d’une variable. Certaines fonctions vont requérir un dataframe, et les tibbles seront incompatibles. Alors, finalement, pourquoi aborder ce type de format ? Pour deux raisons : la première est que les tibbles est le format dans lequel vos données seront importées si vous utilisez excel pour créer vos données et le package readxl et la seconde raison est que ce format est particulièrement utile pour manipuler les données, notamment avec le package dplyr. Concrètement, voici à quoi va ressembler un tibble par rapport au dataframe.Remarquez que le tibble fourni automatiquement les informations sur le nombre d’observations, le nombre de colonnes, ainsi que la nature des variables (character et double, pour numeric et integer). Vous pouvez obtenir ces informations pour un dataframe grâce à la fonction strEvidemment, de même qu’il est assez aisé de passer d’un dataframe à un tibble, il est assez simple de passer d’un tibble à un dataframe avec la fonction .data.frame.Le dernier format pour stocker des informations dans R que nous allons aborder est la liste. Il y deux raisons qui motivent à aborder ce type de format. La première est que les listes sont les types d’objets qui permettent de stocker des informations de la manière la plus flexible qui soit, plusieurs informations de même taille, plusieurs matrices, plusieurs dataframes ou simplement une valeur ou un vecteur. Cela peut être particulièrement utile quand doit réaliser une tâche itérative. La seconde raison est que les sorties de résultats de la plupart des fonctions sont stockées dans des listes. Ces informations ne seront que rarement directement utilisable. En revanche, des fonctions génériques, comme print ou summary, permettront d’avoir une présentation adaptée des résultats. Illustrons la flexbilité d’une liste en créant un liste où nous allons stocker la plupart des informations que nous avons créé dans la mémoire de R.Par la suite, peut accéder à n’importe quel élément d’un dataframe ou d’une liste en utilisant le symbole “$”.","code":"\nnotes<-c(12, 13, 9, 8, 15, 18, \n         4, 11, 13) # Création d'un vecteur avec 10 valeurs \nnotes## [1] 12 13  9  8 15 18  4 11 13\nmean(notes) # calcul de la moyenne des notes## [1] 11.44444\nmot_de_fin<-c(\"Cordialement\", \"respectueusement\", \"amicalement\", \"amitiés\", \"bonne journée\",\"bien à toi\",\"bien à vous\")\nmot_de_fin## [1] \"Cordialement\"     \"respectueusement\" \"amicalement\"      \"amitiés\"         \n## [5] \"bonne journée\"    \"bien à toi\"       \"bien à vous\"\nc(\"a\",1) # remarquez les guillemets dans la sortie de résultats autour du 1, indiquant qu'il est interprété comme une chaîne de caractères. ## [1] \"a\" \"1\"\n1:5 # crée une séquence de chiffres de 1 à 5## [1] 1 2 3 4 5\nrep(x=1, times = 9) # répète 9 fois le chiffre 1 ## [1] 1 1 1 1 1 1 1 1 1\nseq(from = 1, to = 9, by = 2) # crée une séquence allant de 1 à 9 en ayant un écart de 2 à chaque fois## [1] 1 3 5 7 9\ngl(n= 2, k = 5 , labels = c(\"condition 1\", \"condition 2\")) # crée un facteur ayant 2 modalités, chacune étant répétée 5 fois. ##  [1] condition 1 condition 1 condition 1 condition 1 condition 1 condition 2\n##  [7] condition 2 condition 2 condition 2 condition 2\n## Levels: condition 1 condition 2\nmatrice<-matrix(data = 1:9, # les données sont une séquence de chiffres allant de 1 à 9\n                nrow = 3, # elles doivent être réparties dans 3 lignes\n                ncol = 3, # et dans 3 colonnes - un des deux arguments peut être omis puisque si on a le nombre de lignes on a le nombre de colonnes\n                byrow = F) # logique qui permet d'indiquer si on veut que les valeurs soient organisées par ligne ou par colonne. Ici, par colonne\n\n## Notez que j'ai commenté chacun des arguments à l'intérieur de la fonction \n## et que la fonction a été utilisée sur plusieurs lignes pour plus de lisibilité. \n\nmatrice##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\nsexe<-c(\"Homme\", \"Femme\",\"Homme\", \"Femme\")\ntaille<-c(176,173,180,165)\n\ndt<-data.frame(sexe=sexe, taille=taille) \ndt##    sexe taille\n## 1 Homme    176\n## 2 Femme    173\n## 3 Homme    180\n## 4 Femme    165\n# on utilise ici la fonction tbl pour transformer notre dataframe \n# en tibble mais vous n'aurez normalement jamais à utiliser cette fonction. \ndt_tbl<-tibble(dt) \ndt_tbl## # A tibble: 4 × 2\n##   sexe  taille\n##   <chr>  <dbl>\n## 1 Homme    176\n## 2 Femme    173\n## 3 Homme    180\n## 4 Femme    165## 'data.frame':    4 obs. of  2 variables:\n##  $ sexe  : chr  \"Homme\" \"Femme\" \"Homme\" \"Femme\"\n##  $ taille: num  176 173 180 165\nclass(dt_tbl)## [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\ndt_tbl<-as.data.frame(dt_tbl)\nclass(dt_tbl)## [1] \"data.frame\"\nma.liste<-list(# on crée un liste des objets de taille différentes \n  dataframe = dt, # on ajoute un dataframe \n  vecteur = notes, # un vecteur \n  matrice = matrice, # une matrice \n  valeur = logique2) # et une valeur unique. On peut évidemment continuer\nma.liste## $dataframe\n##    sexe taille\n## 1 Homme    176\n## 2 Femme    173\n## 3 Homme    180\n## 4 Femme    165\n## \n## $vecteur\n## [1] 12 13  9  8 15 18  4 11 13\n## \n## $matrice\n##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\n## \n## $valeur\n## [1] FALSE\ndt$sexe ## [1] \"Homme\" \"Femme\" \"Homme\" \"Femme\"\nma.liste$vecteur## [1] 12 13  9  8 15 18  4 11 13"},{"path":"les-données.html","id":"la-nature-des-variables-avec-easier","chapter":"5 Les données","heading":"5.3 La nature des variables avec easieR","text":"Quand vous importez les données, easieR, en plus de l’importation, fournit plusieurs informations importantes à considérer :Le nombre d’observations manquantes par variable. Si aucune information de cette nature n’apparaît dans la console, c’est qu’il n’y pas de valeurs manquantes.Le nombre d’observations et de variables.La nature des variables qui prendra essentiellement deux formes :\nfactor pour les variables qualitatives ;\nnumeric (ou integer) pour les variables quantitatives.\nfactor pour les variables qualitatives ;numeric (ou integer) pour les variables quantitatives.C’est à cette étape qu’il faut s’assurer que les données correspondent à ce qu’est censé avoir : est-ce le bon nombre d’observations ? Est-ce que les variables sont dans le bon format. Par exemple, est-ce qu’une variable numérique n’est pas considérée comme un facteur parce que le caractère de séparation de décimale n’pas été correctement choisi (la virgule à la place du point ou inversément) ?Ici, il est à noter que les variables ordinales, pour pouvoir être utilisées comme tel, doivent avoir un format numérique. Il est possible de faire la transformation directement dans R. Cependant, si l’objectif est d’éviter d’utiliser R en ligne de commande, il est préférable de l’anticiper en préparant correctement les données dans le tableur.","code":""},{"path":"organiser-les-données.html","id":"organiser-les-données","chapter":"6 Organiser les données","heading":"6 Organiser les données","text":"Les ingrédients centraux pour tester une hypothèse sont les données. Aucun manuel, aucun enseignant soulignera suffisamment à quel point il est essentiel d’assurer le plus grand soin pour avoir des données de qualité.\nSi les données que vous avez récoltées n’ont pas été récoltées avec soin, les traitements statistiques réalisés seront vains. Pire si vous identifiez mal la nature de vos données, les traitements que vous réaliserez seront inadaptés et donc erronés.\nEn revanche, ce qui peut être expliqué est la manière d’organiser correctement votre jeu de données. Quelques règles doivent être respectées pour vous assurer que vos données puissent être utilisables pour les traitements ultérieurs.Vos données représentent un tableau à double entrée, c’est-à-dire un dataframe, où les lignes représentent les observations et les colonnes représentent vos variables. Notez que le mot observation doit être pris au sens large. Dans certains cas, notamment quand veut construire deux formes parallèles d’une tâche, il peut être intéressant de vérifier si chaque stimulus d’une version présente des caractéristiques similaires au stimulus qui lui est apparié dans l’autre version de la tâche. Dans ce cas, chaque stimulus pourra être considéré comme une observation.\nIl y une exception à cette organisation : les données avec des mesures répétées. Si un individu passe par différentes conditions expérimentales, le nom de la colonne va représenter la modalité de la variable indépendante en mesure répétée et le contenu en-dessous de ce nom de colonne va représenter la variable dépendante mesurée dans chacune des conditions expérimentales. Si nous reprenons l’exemple du dataframe que nous avons présenté plus haut (avec la taille et le sexe), pourrait avoir une mesure de la taille à 4 ans, à 10 ans et à 15 ans. Le jeu de données prendrait alors la forme du Tableau 6.1.Table 6.1: Exemple de données pour lesquelles il y aurait plusieurs mesures chez les mêmes individussexeTaille 4 ansTaille 10 ansTaille 15 ansHomme102138176Femme108135173Homme105140180Femme99142165On se rend compte ici que la structure des données nous permet d’identifier si les mesures sont à mesures répétées ou à groupes indépendants. Puisque chaque ligne représente un individu et qu’est sur la même ligne, il s’agit ici de données en mesures répétées. En revanche, ne peut pas être totalement certain qu’il s’agit de groupes indépendants si les données sont organisées de la manière utilisée dans le Tableau 6.2 :Table 6.2: Exemple de données pour lesquelles il peut y avoir ambiguité sur la nature répétée ou à groupes indépendants de l'âge des personnessexeAgeTailleHomme4 ans102Femme4 ans108Homme4 ans105Femme4 ans99Homme10 ans138Femme10 ans135Homme10 ans140Femme10 ans142Homme15 ans176Femme15 ans173Homme15 ans180Femme15 ans165Pour pouvoir interprété avec certitude le jeu de données, il faut absolument avoir un identifiant pour les observations. Dans le Tableau 6.3, les données représentent des données à groupes indépendants alors que dans le Tableau 6.4, elles représentent des données à mesure répétées. le sait car les identifiants des observations ne sont pas les mêmes entre les différents âges dans le Tableau ?? alors qu’ils sont identiques aux différents âges dans le Tableau 6.4.Table 6.3: Données à groupes indépendantsIdentifiantsexeAgeTailledanHomme4 ans102avpFemme4 ans108qafHomme4 ans105hwxFemme4 ans99hrfHomme10 ans138cgtFemme10 ans135dzmHomme10 ans140olfFemme10 ans142aufHomme15 ans176nutFemme15 ans173omkHomme15 ans180qntFemme15 ans165Table 6.4: Données en mesures répétéesIdentifiantsexeAgeTailledhcHomme4 ans102csxFemme4 ans108ecaHomme4 ans105pnmFemme4 ans99dhcHomme10 ans138csxFemme10 ans135ecaHomme10 ans140pnmFemme10 ans142dhcHomme15 ans176csxFemme15 ans173ecaHomme15 ans180pnmFemme15 ans165La différence de présentation entre les données présentées dans le Tableau 6.1 et celles présentées dans le Tableau 6 est ce qu’appelle un format large (Tableau 6.1) et un format long (Tableau 6.4).Pour terminer sur l’organisation des données, il est nécessaire de respecter quelques règles pour que l’importation se passe le mieux possible, respectez les règles suivantes :Votre feuille de données ne contient que les données (pas de graphique, pas commentaires, pas de mise en forme …)Votre feuille de données ne contient que les données (pas de graphique, pas commentaires, pas de mise en forme …)Les noms des variables doivent être courts (pas plus de 8 caractères ) rendant ainsi leur utilisation plus simple, ne doivent contenir ni accents, ni espaces. Pour enlever les espaces, utilisez la fonction rechercher/remplacer. Dans la fenêtre « rechercher », tapez un espace, dans la fenêtre « remplacer », tapez un underscore (le tiret en-dessous du 8 « _ ») ou un point.Les noms des variables doivent être courts (pas plus de 8 caractères ) rendant ainsi leur utilisation plus simple, ne doivent contenir ni accents, ni espaces. Pour enlever les espaces, utilisez la fonction rechercher/remplacer. Dans la fenêtre « rechercher », tapez un espace, dans la fenêtre « remplacer », tapez un underscore (le tiret en-dessous du 8 « _ ») ou un point.Il ne doit pas y avoir de ligne ou de colonne vide avant et dans la matrice de données. Les données commencent à la cellule A1 et s’étendent jusqu’à la dernière variable et la dernière observation sans discontinuer.Il ne doit pas y avoir de ligne ou de colonne vide avant et dans la matrice de données. Les données commencent à la cellule A1 et s’étendent jusqu’à la dernière variable et la dernière observation sans discontinuer.Bien que ce ne soient pas indispensable lorsqu’utilise excel mais est indispensable pour les fichiers csv, une bonne pratique consiste à indiquer dans les cellules vides la valeur NA (available) que R reconnait comme étant une valeur non disponible. nouveau, il est possible d’utilisez la fonction rechercher/remplacer dans le logiciel que vous avez utilisé pour stocker vos données.Bien que ce ne soient pas indispensable lorsqu’utilise excel mais est indispensable pour les fichiers csv, une bonne pratique consiste à indiquer dans les cellules vides la valeur NA (available) que R reconnait comme étant une valeur non disponible. nouveau, il est possible d’utilisez la fonction rechercher/remplacer dans le logiciel que vous avez utilisé pour stocker vos données.Si des modifications doivent être faites avant l’importation des données, ne travaillez jamais sur le fichier de données original. Enregistrez d’abord le fichier sous un autre nom.Si des modifications doivent être faites avant l’importation des données, ne travaillez jamais sur le fichier de données original. Enregistrez d’abord le fichier sous un autre nom.L’importation des nombres avec décimales ne pose pas de souci si vos données sont dans un fichier excel. En revanche, s’il s’agit d’un fichier csv, les nombres avec décimales sont considérés comme tels dans R à partir du moment où l’unité est séparée de la décimale par un point (« . »). Si vos décimales sont séparées des unités par des virgules, il faudra le préciser explicitement lors de l’importation des données.L’importation des nombres avec décimales ne pose pas de souci si vos données sont dans un fichier excel. En revanche, s’il s’agit d’un fichier csv, les nombres avec décimales sont considérés comme tels dans R à partir du moment où l’unité est séparée de la décimale par un point (« . »). Si vos décimales sont séparées des unités par des virgules, il faudra le préciser explicitement lors de l’importation des données.Ayez systématiquement un identifant pour chaque observation (càd. pour chaque participant).Ayez systématiquement un identifant pour chaque observation (càd. pour chaque participant).Si avec le temps, R s’est montré de plus en plus flexible pour la gestion des données, ne peut que vous encourager à être très rigide sur la structure de la base de données. La première raison est que votre base de données représentent les ingrédients de vos analyses. Si les ingrédients sont de mauvaises qualités, les analyses seront de mauvaise qualité. La seconde raison est que faire preuve de laxisme sur les données est le meilleur moyen d’être confronté à des messages d’erreurs dans R que vous ne comprendrez pas.","code":""},{"path":"préparer-les-données.html","id":"préparer-les-données","chapter":"7 Préparer les données","heading":"7 Préparer les données","text":"Jusqu’à présent, les données que nous avons utilisées ne correspondent pas à un vrai jeu de données sur lequel voudrait réaliser effectivement des traitements. R permet de travailler avec des données issues de différents formats. Les deux formats de données les plus fréquemment utilisés sont les fichiers excel et les fichiers csv. Nous focaliserons sur ces deux formats, bien que R soit en mesure d’importer des fichiers d’autres natures grâce au package foreign (R Core Team, 2022).Les données sont rarement utilisable directement et il est, dans la grande majorité des cas, nécessaire de réaliser des opérations pour que les données puissent être utilisées pour tester les hypothèses. Cela peut passer par la suppression des observations d’entraînement, la transformation d’une variable, ou l’addition des items d’un questionnaire pour obtenir un score global.L’objectif de ce chapitre est d’illustrer comment importe des données et comment peut les préparer.","code":""},{"path":"préparer-les-données.html","id":"dans-r","chapter":"7 Préparer les données","heading":"7.1 Dans R","text":"","code":""},{"path":"préparer-les-données.html","id":"limportation","chapter":"7 Préparer les données","heading":"7.1.1 L’importation","text":"présent que vous avez précisé le répertoire de travail où se trouve vos données, que vous avez identifié que R trouve effectivement votre fichier dans le dossier de votre répertoire de travail avec la fonction dir, et que vous vous êtes assuré que les règles de construction des jeux de données ont été scrupuleusement respectées, l’étape suivante consiste à importer les données.\nCette étape peut être réalisé avec la fonction  du package readxl si vos données sont stockées dans un fichier excel, avec la fonction read.csv2, inclue de base dans R, pour un fichier csv.Pour la suite du chapitre, nous allons utiliser des données d’amorçage sémantique (données non publiées mais pour des données publiées avec cette tâche et une description en détail de la tâche, voir Stefaniak et al., 2010). Plus précisément, la tâche que les participants devaient réaliser est une tâche de double décision lexicale. Dans cette tâche, les participants doivent déterminer le plus rapidement et le plus précisémant possible si deux chaînes de lettres présentées simultanément à l’écran sont des mots (par exemple, MOIS - CADEAU) ou non (par exemple, AJÛT - MARRER). leur insu, certaines paires de mots sont reliés sémantiquement (par exemple, MAIN - BAGUE) alors que d’autres ne le sont pas. Dans cette tâche, s’attend à ce que les items reliés sémantiquement soient traités plus rapidement que ceux qui ne le sont pas. La tâche est composée de 185 essais répartis entre 5 essais d’entraînement (les 5 premiers) et 180 essais à inclure dans les analyses.Dans le jeu de données mis à disposition, nous avons les données de 40 participants. Les variables d’intérêt sont :Subject : l’identifiant du participant ;Subject : l’identifiant du participant ;Attribute1 : le mot présenté en première position ;Attribute1 : le mot présenté en première position ;Attribute2 : le mot présenté en deuxième position ;Attribute2 : le mot présenté en deuxième position ;CATEGORIE : la condition expérimentale de la paire de mot (SEMANTIQUE = paire sémantiquement reliés ; contre = paire contrebalancée entre les versions de la tâche ; rempl = paire de mots de remplissage ; pseudo = paire contenant au moins un pseudomot)CATEGORIE : la condition expérimentale de la paire de mot (SEMANTIQUE = paire sémantiquement reliés ; contre = paire contrebalancée entre les versions de la tâche ; rempl = paire de mots de remplissage ; pseudo = paire contenant au moins un pseudomot)Running : la variable qui permet de distinguer les essais d’entraînement des esssais cibles, qui doivent être analysés ;Running : la variable qui permet de distinguer les essais d’entraînement des esssais cibles, qui doivent être analysés ;TextDisplay6.RT : le temps de réactionTextDisplay6.RT : le temps de réactionRC : la réponse correcteRC : la réponse correcteIl est important de noter ici que les données ont été préparées pour qu’il n’y ait aucun piège. Autrement dit, les règles présentées précédemment sont scrupuleusement appliquées ici. Pour vos données, il faut être attentif aux règles décrites précédemment.Il faut également noter que certaines variables d’intérêt dans le cadre de mon étude ne seront pas utilisées car cela n’apporteraient pas une réelle plus-value à l’exercice.","code":""},{"path":"préparer-les-données.html","id":"importer-un-fichier-excel","chapter":"7 Préparer les données","heading":"7.1.1.1 Importer un fichier excel","text":"Pour importer, des données en formation excel, va s’appuyer sur le package readxl (Wickham & Bryan, 2022) et va utiliser les fonctions read_excel et excel_sheetsLa premère étape consiste à vérifier si le fichier est effectivement dans mon répertoire de travail avec la fonction dirCette fonction m’indique que mon fichier “semantic_priming.xlsx” est présent dans le répertoire.partir d’ici,je pourrais ouvrir le fichier excel pour voir quel est le nom de la feuille de calcul que je dois importer mais une manière plus rapide de le faire est d’utiliser la fonction excel_sheets. Deux points sont importants ici. Il faut avoir chargé le package readxl (Wickham & Bryan, 2022), ce qui normalement devrait être fait (voir la ligne 34 du script) et il faut que le fichier excel soit fermé. Dans le cas contraire, vous aurez un message vous indiquant que le fichier excel ne peut pas être ouvert alors que le chemin d’accès est correct.En l’occurrence, il n’y qu’une seule feuille de calcul intitulée “tout_direct”. peut donc importer cette feuille avec la fonction read_excel. Nous devons préciser deux arguments : le chemin d’accès au fichier, c’est-à-dire le nom du fichier et la feuille de calcul que nous souhaitons importer. Ici, il est important de noter que, pour pouvoir utiliser ultérieurement les données, va les stocker dans un objet appelé “data.xls”.Ainsi, en tapant ‘data.xls’, vous identifiez qu’il s’agit d’un tibble composé de 7400 lignes et 59 variables.Pour les personnes souhaitant voir leurs données dans l’ensemble, la fonction View vous permettra d’ouvrir un nouvel onglet dans Rstudio dans lequel vous aurez l’intégralité des données.","code":"\ndir()\nexcel_sheets(\"./introR/semantic_priming.xlsx\")## [1] \"tout_direct\"\ndata.xls<-read_excel(path = \"./introR/semantic_priming.xlsx\", sheet = \"tout_direct\")\ndata.xls## # A tibble: 7,400 × 59\n##    ExperimentName Subject Session Display.RefreshRate Group RandomSeed\n##    <chr>            <dbl>   <dbl>               <dbl> <dbl>      <dbl>\n##  1 direct1              1       1                60.1     1 2136425444\n##  2 direct1              1       1                60.1     1 2136425444\n##  3 direct1              1       1                60.1     1 2136425444\n##  4 direct1              1       1                60.1     1 2136425444\n##  5 direct1              1       1                60.1     1 2136425444\n##  6 direct1              1       1                60.1     1 2136425444\n##  7 direct1              1       1                60.1     1 2136425444\n##  8 direct1              1       1                60.1     1 2136425444\n##  9 direct1              1       1                60.1     1 2136425444\n## 10 direct1              1       1                60.1     1 2136425444\n## # ℹ 7,390 more rows\n## # ℹ 53 more variables: SessionDate <chr>, SessionTime <dttm>, Block <dbl>,\n## #   Attribute1 <chr>, Attribute2 <chr>, CATEGORIE <chr>, essais <chr>,\n## #   essais.Cycle <chr>, essais.Sample <chr>, expe <chr>, expe.Cycle <chr>,\n## #   expe.Sample <chr>, List1 <chr>, List1.Cycle <chr>, List1.Sample <chr>,\n## #   Procedure <chr>, REPCORRECTE <chr>, Running <chr>, TextDisplay1.ACC <chr>,\n## #   TextDisplay1.CRESP <chr>, TextDisplay1.DurationError <chr>, …\nView(head(data.xls))"},{"path":"préparer-les-données.html","id":"importer-un-fichier-csv","chapter":"7 Préparer les données","heading":"7.1.1.2 Importer un fichier csv","text":"Pour importer un fichier csv, la fonction utilisée sera read.csv2. Si la procédure est sensiblement la même, quelques étapes et quelques arguments supplémentaires doivent être réalisées/précisés. Tout d’abord, ne peut pas faire l’économie d’ouvrir le fichier car il faut connaître le caractère qui sépare chacune des colonnes. Il faut ouvrir le fichier avec le bloc note car si l’ouvre avec un logiciel tel qu’excel, il ne sera pas possible d’identifier le symbole qui sert de séparateur de colonnes. En l’occurrence, le séparateur est le point-virgule (“;”). Ouvrir le fichier permet également de s’assurer que la première ligne du fichier correspond au nom des colonnes.Lors de l’importation, va donc indiquer si la première ligne correspond au nom des variables avec l’argument header = T, va indiquer le caractère qui sépare chaque colonne avec l’argument sep. Les valeurs possibles sont l’espace, la tabulation, le point-virgule et la virgule. L’espace est à déconseiller car les valeurs textuelles pourraient être traitées de manière inadaptée. Dans ce cas, il faut réengistrer le fichier avec un autre séparateur. Ceci est possible en utilisant par exemple excel, enregistrer sous et en choisissant le format.Il est également nécessaire de préciser le caractère qui va indiquer la séparation des décimales (le point ou la virgule), ainsi que la manière dont les valeurs manquantes ont été gérées. Fondamentalement peut utiliser n’importe quelle valeur pour signaler les valeurs manquantes. Etant habitué à R, le code que j’utilise est NA. Notez qu’il n’est pas absolument nécessaire de le faire dans excel mais qu’il est indispensable de le faire avec un fichier csv. Enfin, l’argument check.names va permettre de vérifier si les noms utilisés comme nom de variables sont valides et les corriger le cas échéant.nouveau, il est possible de voir les données importées en utilisant la fonction View.","code":"\ndata.csv<-read.csv2(file = \"semantic_priming.csv\", \n                    header = T,\n                    sep = \";\", # valeurs possibles \"\", \"\\tab\" ou \",\"\n                    dec =\".\", # valeurs possibles \".\" ou \",\"\n                    na.string = \"NA\",\n                    check.names=T)"},{"path":"préparer-les-données.html","id":"préparer-les-données-1","chapter":"7 Préparer les données","heading":"7.1.2 Préparer les données","text":"Il n’est pas possible d’explorer toutes les potentialités de prétraitements des données qu’offre R mais l’exemple de travail utilisé permet d’illustrer quelques-unes de ces potentialités.Concrètement, pour que les données soient utilisables (nous allons faire l’hypothèse que l’analyse que nous voulons réaliser est une analyse de variance à mesure répétée d’un côté sur les temps de réaction et de l’autre sur les réponses correctes), nous allons devoir suivre les étapes suivantes :sélectionner les variables d’intérêt pour faciliter la manipulation des données ;sélectionner les variables d’intérêt pour faciliter la manipulation des données ;supprimer les observations relatifs aux essais d’entraînement ;supprimer les observations relatifs aux essais d’entraînement ;calculer le pourcentage de réponses correctes par participant et par conditions (la somme totale ne conviendrait pas ici étant donné qu’il n’y pas le même nombre d’items dans toutes les conditions, càd. 30 items dans chacune des trois conditions mots et 90 dans la condition pseudomot) ;calculer le pourcentage de réponses correctes par participant et par conditions (la somme totale ne conviendrait pas ici étant donné qu’il n’y pas le même nombre d’items dans toutes les conditions, càd. 30 items dans chacune des trois conditions mots et 90 dans la condition pseudomot) ;filtrer les temps de réaction pour ne conserver que ceux pour lesquels la réponse est correcte ;filtrer les temps de réaction pour ne conserver que ceux pour lesquels la réponse est correcte ;vérifier qu’il n’y pas de temps de réaction correspondant à des réponses anticipées ou à des réponses inattentives. Habituellement, gère ce cas de figure en utilisant la médiane mais en l’occurrence, par souci pédagogique, va considérer que les réponses anticipées sont les temps inférieurs à 200 ms et les réponses inattentives ceux supérieurs à 2000 ms.vérifier qu’il n’y pas de temps de réaction correspondant à des réponses anticipées ou à des réponses inattentives. Habituellement, gère ce cas de figure en utilisant la médiane mais en l’occurrence, par souci pédagogique, va considérer que les réponses anticipées sont les temps inférieurs à 200 ms et les réponses inattentives ceux supérieurs à 2000 ms.calculer la moyenne des temps de réaction restant par participant et par condition.calculer la moyenne des temps de réaction restant par participant et par condition.Combiner le jeu de données sur les réponses correctes et sur les temps de réaction.Combiner le jeu de données sur les réponses correctes et sur les temps de réaction.","code":""},{"path":"préparer-les-données.html","id":"sélectionner-des-variables","chapter":"7 Préparer les données","heading":"7.1.2.1 Sélectionner des variables","text":"Pour sélectionner des variables, va utiliser la fonction select du package dplyr. Il suffit de donner le nom du jeu de données et les variables qu’il faut sélectionner en les séparant par une virgule.Il existe néanmoins une difficulté ici. La fonction select existe dans plusieurs packages et peut créer un conflit car elle ne s’utilise pas de la même manière en fonction du package. Pour éviter le conflit, peut préciser le package dans lequel il faut aller chercher la fonction en indiquant le nom du package suivi de deux-points, c’est-à-dire dplyr::.Nous conserverons uniquement les variables qui ont été décrites plus haut, à savoir : Subject, Attribute1, Attribute2, CATEGORIE, Running, TextDisplay6.RT, RC.","code":"\ndata.et1<-data.xls%>%dplyr::select( Subject, Attribute1,Attribute2,CATEGORIE,\n                Running, TextDisplay6.RT,RC)"},{"path":"préparer-les-données.html","id":"sélectionner-des-observations-sur-la-base-dune-valeur-textuelle","chapter":"7 Préparer les données","heading":"7.1.2.2 Sélectionner des observations sur la base d’une valeur textuelle","text":"présent que le jeu de données est plus lisible, nous pouvons supprimer les observations correspondant aux essais. trouve cette information dans la variable ‘Running’. Grâce à la fonction unique, peut identifier les différentes valeurs individuelles dans la variable ‘Running’.Sans surprise, les essais s’appellent les essais. Pour sélectionner des valeurs textuelles, deux possibilités : 1) selectionner les chaînes que nous souhaitons conserver ; 2) supprimer celles que nous ne voulons pas conserver. Etant donné qu’il y 3 possibilités (“essais”, “expe”, “List1”) et qu’veut garder tout ce qui n’est pas “essais”, va donc décider d’utiliser la seconde stratégie. La logique consiste à sélectionner tout ce qui ne correspond pas à un critère. En l’occurrence, notre critère consiste à ne pas être un essai.Pour atteindre cet objectif, en plus d’utiliser la fonction filter, il faut connaître la liste des opérateurs logiques dans R. Le Tableau 7.1 une liste non exhaustive.Table 7.1: Liste des opérateurs logiques les plus fréquentsOpérateurSignification==égal à !=Différent de>plus grand>=plus grand ou égal (le égal se met toujours à droite)<plus petit<=plus petit ou égal (le égal se met toujours à droite)&et|ouDans notre exemple, nous allons devoir utiliser l’opérateur “!=” pour indiquer que nous voulons conserver tous les valeurs qui sont différentes de “essais”.Pour déterminer si l’opération s’est déroulée avec succès, peut vérifier si le nombre d’observations été réduit de manière cohérente avec ce qu’attend. Ainsi, sait que nous avons 40 participants, et nous savons qu’il y 5 essais par participants. doit donc avoir une réduction de 200 observations entre les données à l’étape 1 (data.et1) et les données à l’étape 2 (data.et2). obtient cette information avec la fonction dim, qui nous permet de constater que le nombre d’observations est passé de 7400 à 7200.","code":"\nunique(data.et1$Running)## [1] \"essais\" \"expe\"   \"List1\"\ndata.et2<-filter(data.et1, Running!=\"essais\")\ndim(data.et1)## [1] 7400    7\ndim(data.et2)## [1] 7200    7"},{"path":"préparer-les-données.html","id":"réaliser-des-opérations-mathématiques-calcul-de-pourcentages-de-réponses-correctes","chapter":"7 Préparer les données","heading":"7.1.2.3 Réaliser des opérations mathématiques : calcul de pourcentages de réponses correctes","text":"L’étape suivante consiste à obtenir le pourcentage de réponse correcte par condition. Grâce à la fonction unique, est en mesure d’identifier qu’il y 4 conditions (“contre”, “pseudo”, “rempl”, “SEMANTIQUE”)Connaissant les données, j’avais indiqué qu’il y avait 30 items par condition sauf pour les pseudomots où il y 90 items. peut néanmoins s’assurer que cette répartition est correcte et que, individuellement, il n’y pas d’observation manquante. peut obtenir cette information avec la fonction table.Nous pouvons à présent calculer le pourcentage de réponses correctes par participant et par condition. Cette étape est un peu plus complexe et illustre comment il est possible d’enchaîner les opérations avec le package dplyr. Ainsi, nous allons réaliser l’opération en 3 fois. Dans un premier temps, va indiquer sur quel jeu de données les opérations doivent être réalisées. Dans un second temps, indique les variables sur lesquelles nous souhaitons faire le regroupement. Enfin, calcule le pourcentage de réponses correctes dans une nouvelle variable (‘perc’). Il existe sans doute différentes approches pour cette dernière étape. Nous allons privilégier une approche assez simple. Etant donné que les réponses correctes sont des 1 et des 0, en calculant la moyenne par participant et par condition, nous aurons la proportion de réponses correctes qu’il suffira de multiplier par 100. Les symboles %>% s’appellent des pipes et permettent de lier les opérations entre elles. En d’autres termes, peut réaliser l’ensemble des opérations en une seule fois.","code":"\nunique(data.et2$CATEGORIE)## [1] \"contre\"     \"pseudo\"     \"rempl\"      \"SEMANTIQUE\"\ntable(data.et2$Subject, data.et2$CATEGORIE)##     \n##      contre pseudo rempl SEMANTIQUE\n##   1      30     90    30         30\n##   2      30     90    30         30\n##   3      30     90    30         30\n##   4      30     90    30         30\n##   5      30     90    30         30\n##   6      30     90    30         30\n##   7      30     90    30         30\n##   8      30     90    30         30\n##   9      30     90    30         30\n##   10     30     90    30         30\n##   11     30     90    30         30\n##   12     30     90    30         30\n##   13     30     90    30         30\n##   14     30     90    30         30\n##   15     30     90    30         30\n##   16     30     90    30         30\n##   17     30     90    30         30\n##   18     30     90    30         30\n##   19     30     90    30         30\n##   20     30     90    30         30\n##   21     30     90    30         30\n##   22     30     90    30         30\n##   23     30     90    30         30\n##   24     30     90    30         30\n##   25     30     90    30         30\n##   26     30     90    30         30\n##   27     30     90    30         30\n##   28     30     90    30         30\n##   29     30     90    30         30\n##   30     30     90    30         30\n##   31     30     90    30         30\n##   32     30     90    30         30\n##   33     30     90    30         30\n##   34     30     90    30         30\n##   35     30     90    30         30\n##   36     30     90    30         30\n##   37     30     90    30         30\n##   38     30     90    30         30\n##   39     30     90    30         30\n##   40     30     90    30         30\nRC_percent <- data.et2 %>% # le résultat sera stocké dans RC percent et on effectue des opérations sur data.et2\n  group_by(Subject, CATEGORIE )%>% # un regroupe les données par les modalités des variables Subject et CATEGORIE\n  summarise(perc_RC = mean(RC)*100 ) # On résume les données en calculant le pourcentage de réponses correctes## `summarise()` has grouped output by 'Subject'. You can override using the\n## `.groups` argument."},{"path":"préparer-les-données.html","id":"sélectionner-des-observations-sur-la-base-dune-valeur-numérique","chapter":"7 Préparer les données","heading":"7.1.2.4 Sélectionner des observations sur la base d’une valeur numérique","text":"Nous voulons à présent sélectionner les réponses correctes. va utiliser une procédure similaire à celle utilisée pour supprimer les essais, avec comme différence le fait que nous allons conserver les observations pour lesquelles la réponses vaut 1 en utilisant l’opérateur “==”. La seconde différence est que les valeurs numériques ne sont pas entre guillemets contrairement aux chaînes de caractères.","code":"\ntable(data.et2$RC)  # il y a 5679 observations correctes ## \n##    0    1 \n## 1521 5679\ndata.et4<-filter(data.et2, RC==1) # on sélectionne les observations pour lesquelles la réponse est correcte \ndim(data.et4) # on obtient effectivement 5679 observations## [1] 5679    7"},{"path":"préparer-les-données.html","id":"combiner-plusieurs-critères-de-sélection","chapter":"7 Préparer les données","heading":"7.1.2.5 Combiner plusieurs critères de sélection","text":"L’avant-dernière étape consiste à vérifier qu’il n’y pas de réponses anticipées ou des réponses inattentives. Nous allons fixer comme critères 200 ms pour les réponses anticipées. Cela signifie que nous considérons qu’il n’est pas possible de répondre en moins de 200 millisecondes en ayant traité de manière volontaire les stimuli.Dans cet exemple, ce raisonnement est un peu fallacieux car les items sont présentés pendant 400 ms sans qu’ne puisse répondre et un écran blanc apparaît ensuite. Ainsi, avec un critère à 200 ms, cela laisse 600 ms pour répondre ce qui peut avoir été suffisant. L’objectif ici étant d’être pédagogique, nous conserverons tout de même ce critère.Les réponses inattentives sont les réponses pour lesquelles les latences sont tellement longues qu’doit considérer que le participant répondu suite à une inattention et ne reflète pas réellement la manière dont il se comporte dans ce type de tâche. Nous fixerons de manière arbitraire ce critère à 2000 ms.Nous allons donc filtrer les données en appliquant deux critères de sélection.Remarquez que nous avons ici aussi utilisé les pipes pour réaliser plusieurs opérations en une fois.","code":"\ndata.et5<-data.et4  %>%\n          filter(TextDisplay6.RT>200) %>% # on conserve les temps supérieurs à 200 ms\n          filter(TextDisplay6.RT<2000) # on conserve les temps inférieurs à 2000 ms"},{"path":"préparer-les-données.html","id":"réaliser-des-opérations-mathématiques-calcul-de-la-moyenne-des-temps-de-réaction","chapter":"7 Préparer les données","heading":"7.1.2.6 Réaliser des opérations mathématiques : calcul de la moyenne des temps de réaction","text":"présent, nous pouvons faire la moyenne des temps de réaction sur les items restant. Nous avons déjà vu comment réaliser cette opération à l’étape 3.","code":"\ndata.et6 <- data.et5 %>% # le résultat sera stocké dans RC percent et on effectue des opérations sur data.et2\n  group_by(Subject, CATEGORIE )%>% # un regroupe les données par les modalités des variables Subject et CATEGORIE\n  summarise(M_TR = mean(TextDisplay6.RT) ) # On fait la moyenne des temps de réaction par participant et par condition## `summarise()` has grouped output by 'Subject'. You can override using the\n## `.groups` argument."},{"path":"préparer-les-données.html","id":"réaliser-une-jointure-entre-des-jeux-de-données","chapter":"7 Préparer les données","heading":"7.1.2.7 Réaliser une jointure entre des jeux de données","text":"Il nous reste plus qu’à regrouper dans une base de données finales les données avec les pourcentages de réponses correctes et la moyenne des temps de réaction.obtient ce résultat en utilisant la fonction left_join, où les deux premiers arguments sont les jeux de données à combiner et l’argument correspond au nom des variables qui permettent de faire l’appariement. Ici, je les ai donné de manière explicite en indiquant qu’il faut prendre en compte la variable « Subject » et la variable « CATEGORIE », qui s’appellent de la même manière dans les différentes base de données.","code":"\ndata_final<-left_join(data.et6, RC_percent, by=c(\"Subject\"=\"Subject\",\"CATEGORIE\"=\"CATEGORIE\" ))"},{"path":"préparer-les-données.html","id":"avec-easier","chapter":"7 Préparer les données","heading":"7.2 Avec easieR","text":"","code":""},{"path":"préparer-les-données.html","id":"importer-les-données","chapter":"7 Préparer les données","heading":"7.2.1 Importer les données","text":"Pour commencer à utiliser easieR, nous allons débuter par l’importation de données. L’exemple de travail que nous allons utiliser est le suivant : dans une publicité, Georges Clooney va chercher un café au même moment qu’une jeune femme. Il s’aperçoit que c’est la dernière capsule et la laisse donc à la jeune femme, qui était venue chercher un café pour Jean Dujardin. La question que se pose les chercheur est de savoir comment interprète cette publicité. Est-ce que Georges Clooney se laisse manipuler par les femmes, est-ce qu’il s’agit d’un gentleman ou est-ce que Jean Dujardin eu raison de manipuler Clooney pour arriver à ses fins.Les données sont disponibles dans le feuille de calcul appelée ‘Clooney’.","code":""},{"path":"préparer-les-données.html","id":"avec-les-boîtes-de-dialogue","chapter":"7 Préparer les données","heading":"7.2.1.1 Avec les boîtes de dialogue","text":"Pour importer des données avec les boîtes de dialogue, il suffit de lancer easieR avec la fonction easieRLa boîte de dialogue de la Figure 7.1 apparaît. Il faut choisir “Donnees - (importation, exportation, sauvegarde) et cliquer sur OK.\nFigure 7.1: Fenêtre d’accueil de easieR. Choisir : Donnees - (importation, exportation, sauvegarde)\nDe manière assez transparente, pour importer les données, il faut choisir dans la boîte de dialogue de la Figure 7.2 “importer des donnees”\nFigure 7.2: Choisir : importer des donnees\nLe package easieR permet d’importer des données de 4 types de format : CSV, txt, excel et les fichiers SPSS. En réalité, R est en mesure d’importer d’autres formats de données, mais les 4 formats proposés dans easieR représentent les outils utilisés le plus fréquemment utilisés par les psychologues. En l’occurrence, nous travaillerons avec des fichiers excel (voir Figure 7.3). Il est possible d’accéder directement à cette boîte de dialogue grâce à la fonction import()\nFigure 7.3: Format du fichier de données à importer.\nUne fois le format du fichier décidé, il ne reste plus qu’à choisir le fichier de données (Figure @ref(fig=import4)).\nFigure 7.4: Choix du fichier de données, ‘illustration.easieR.xlsx’ en l’occurrence.\nLa plupart du temps, les chercheurs utilisent la première ligne du fichier de données pour indiquer le nom des variables. Cependant, ce n’est pas toujours le cas. Il est donc nécessaire d’indiquer si la première ligne correspond effectivement au nom des variables (Figure 7.5).\nFigure 7.5: Est-ce que la première ligne correspond aux variables ?\nDe la même manière, quand une valeur est manquante, il y plusieurs manières d’indiquer les valeurs manquantes. Si la cellule est vide, peut laisser “NA” en revanche, si une valeur par défaut (comme -9999) est utilisée alors, il faut l’indiquer dans la Figure 7.6.\nFigure 7.6: Valeurs servant à représenter les valeurs manquantes. Si la cellule est vide, laissez NA.\nL’étape suivante consistera à choisir la feuille de calcul qui nous intéresse. En l’occurrence, il s’agit de la feuille ‘Clooney’ (Figure 7.7).\nFigure 7.7: Format du fichier de données à importer.\nEt il faut terminer en donnant un nom aux données qui sera utilisé par la suite dans R. Par défaut, c’est le nom de la feuille de calcul (Figure 7.8). Ce jeu de données doit avoir un nom qui ne contient pas de caractères spéciaux (excepté _ ou .), pas de caractères avec des accents ou des cédilles, et préférez des noms cours (max 20 caractères).\nFigure 7.8: Nom attribué aux données pour les utiliser ensuite dans R.\nLe jeu de données apparaît à présent dans la console (Figure 7.9).\nFigure 7.9: Tableau du jeu de données.\nAinsi que des informations sur les données. En l’occurrence, il s’agit d’un jeu de données avec 67 observations, 3 variables qui sont toutes les trois des variables catégorielles (Figure 7.10).\nFigure 7.10: Structure du jeu de données.\nRemarques importantesTrois remarques doivent être faites :Les explications des boîtes de dialogue sont fournies dans la console quand utilise easieR, ce qui permet de savoir à quoi elles correspondent même si ne le sait pas (Figure 7.11) ;\nFigure 7.11: Information des boîtes de dialogue qui sont affichées dans la console.\neasieR va changer automatiquement le répertoire de travail pour travailler dans le même répertoire que celui où se trouve le jeu de données ;easieR va changer automatiquement le répertoire de travail pour travailler dans le même répertoire que celui où se trouve le jeu de données ;dans la console apparaît un ligne appelée “call”. Cette ligne correspond à la ligne de commande (il faut enlever les guillemets au début et à la fin) pour importer le fichier de données (Figure 7.12).dans la console apparaît un ligne appelée “call”. Cette ligne correspond à la ligne de commande (il faut enlever les guillemets au début et à la fin) pour importer le fichier de données (Figure 7.12).\nFigure 7.12: Ligne de commande qui correspond à l’importation du fichier. Attention, pour l’utiliser, il faut supprimer les guillemets des extrémités.\n","code":"\neasieR()"},{"path":"préparer-les-données.html","id":"en-ligne-de-commande","chapter":"7 Préparer les données","heading":"7.2.1.2 En ligne de commande","text":"peut également utiliser easieR en ligne de commande. Cette fonction 8 arguments :file : le nom du fichierdir : le répertoire où se trouve le fichiertype : le type de fichierdec : lorsqu’utilise un fichier csv ou txt, le séparateur de décimale peut être soit un point, soit une virgule. Il faut le préciser. Dans excel, la fonction le détecte automatiquement.sep : dans les fichiers txt et csv, les colonnes peuvent être séparés par différents types de caractères (tabulation, virgule, espace ou point-virgule). Il faut alors indiquer quel est le séparateur de colonnes. Dans excel, il l’identifie par défaut.na.strings : caractères utilisés pour indiquer qu’une valeur est manquante. NA est la valeur par défaut.sheet : pour les fichiers excel, il faut indiquer la feuille de calcul dans laquelle se trouve les donnéesname : correspond au nom qu’veut attribuer au jeu de données dans R. Ce jeu de données doit avoir un nom qui ne contient pas de caractères spéciaux (excepté _ ou .), pas de caractères avec des accents ou des cédilles, et préférez des noms cours (max 20 caractères).De manière concrète pour importer le même fichier que celui importé en boîte de dialogue, utiliser la fonction suivante :Le jeu de données apparaît à présent dans la console (Figure 7.9).Ainsi que des informations sur les données. En l’occurrence, il s’agit d’un jeu de données avec 67 observations, 3 variables qui sont toutes les trois des variables catégorielles 7.10.Vous avez donc à présent importer le jeu de données présenté dans le Tableau 1.","code":"\nimport(file='illustration.easieR.xlsx',\n       dir='C:/Users/mon_repertoire/Cours de statistiques/Livre/introR',\n       type='Fichier Excel',\n       dec='.',\n       sep=';',\n       na.strings='NA',\n       sheet='Clooney',\n       name='Clooney')"},{"path":"préparer-les-données.html","id":"préparer-les-données-2","chapter":"7 Préparer les données","heading":"7.2.2 Préparer les données","text":"","code":""},{"path":"les-statistiques-descriptives.html","id":"les-statistiques-descriptives","chapter":"8 Les statistiques descriptives","heading":"8 Les statistiques descriptives","text":"“Si les statistiques vous ennuient, c’est sans doute parce que vous n’avez pas les bons chiffres.”\nRésumé \nL’objectif de cette page est de découvrir comment obtenir les principales statistiques descriptives pour des variables quantitatives et qualitatives en utilisant R. Nous apprendrons à calculer des mesures essentielles telles que la moyenne, la médiane, l’écart-type, les quartiles, ainsi que des résumés de fréquences pour les variables qualitatives.Pour les utilisateurs de R en ligne de commande\nDans un premier temps, nous nous concentrerons sur les variables quantitatives en utilisant principalement deux fonctions du package psych (Revelle, 2025) à savoir describe et describeby. Ce package offre une méthode simple et rapide pour générer des résumés détaillés de vos données numériques. À travers des exemples concrets et des exercices d’application, vous pourrez vérifier votre compréhension et appliquer les concepts abordés.Ensuite, nous traiterons des variables qualitatives. Bien que le package psych (Revelle, 2025) ne soit pas utilisé pour ces variables, nous verrons comment obtenir des résumés de fréquences et des proportions en utilisant d’autres fonctions intégrées dans R, comme table et prop.table. Des exemples pratiques vous permettront de comprendre comment analyser et interpréter la répartition des catégories pour les variables qualitatives.Enfin, pour compléter notre analyse, nous aborderons la visualisation de la distribution des données quantitatives à l’aide de graphiques en violon. Ces graphiques, créés avec le package ggplot2, combinent densité et quartiles, offrant une vue d’ensemble complète de la distribution des variables quantitatives. Nous apprendrons à générer et interpréter ces graphiques pour mieux comprendre la forme et les caractéristiques des distributions de nos données.Pour les utilisateurs de easieR\nPrérequis \nD’un point de vue théoriqueêtre en mesure d’identifier la nature d’une variable.Pour les utilisateurs de R\n- Avoir installé R et Rstudioet les principaux packages psych (Revelle, 2025) et ggplot2 (Wickham, 2016a).Savoir installer et charger des packages dans R.Savoir importer un fichier excel dans R.Connaître et utiliser les règles de bonnes pratiques dans R.Pour les utilisateurs d’easieRavoir installé Ravoir installé easieRavoir installé pandocêtre en mesure d’importer un fichier de données","code":""},{"path":"les-statistiques-descriptives.html","id":"introduction-1","chapter":"8 Les statistiques descriptives","heading":"8.1 Introduction","text":"Commencer ses analyses par les statistiques descriptives reste une étape indispensable quand il s’agit d’analyser les données. Elles permettent en effet d’avoir une représentation de prime abord des résultats avant d’éprouver des hypothèses d’un point de vue inférentiel.Cette étape permet aussi de s’assurer de la compatibilité des valeurs de notre jeu de données avec les données possibles (.e., ne peut pas avoir un âge négatif) ainsi que de la distribution des données (imagine aisément que l’âge de décès ne suivra pas une distribution normale).Par exemple, si un chercheur émet l’hypothèse que lever les mains en l’air rend heureux. Il compare un groupe de personnes qui doivent lever leur main 20 minutes par jour et un groupe qui ne le fait pas. Quel sens cela aurait d’aller vérifier la significativité de la différence entre ces deux groupes si cet expérimentateur s’apercevait de prime abord que la moyenne relative à la sensation de bonheur du groupe qui ne lève pas les mains est supérieure à celle de ceux qui lèvent la main ? Si la différence s’avérait significative, la seule conclusion possible serait qu’il ne faut absolument pas lever les mains pour être heureux.\nUn exemple farfelu… mais peut-être pas tant que cela. \nAu-delà du côté farfelu de cette hypothèse, il faut se rappeler que les hypothèses doivent systématiquement être construites sur des éléments théoriques solides. Il n’est dès lors pas acceptable de penser des statistiques sans les mettre en relation avec les résultats obtenus dans d’autres recherches et sans les mettre en relation avec le plan expérimental qui été utilisé\npour acquérir les données. Nous reviendrons à ce titre sur l’importance de réfléchir aux statistiques lors de la construction du plan expérimental.L’exemple proposé ci-dessus, bien que caricatural, est en réalité très souvent observé dans la littérature scientifique, car beaucoup de chercheur·euses oublient que la science ne consiste pas à confirmer leur hypothèse, mais à proposer des modèles qu’tolérera en attendant de l’infirmer. L’objectif n’est donc pas de confirmer des hypothèses, mais d’infirmer des modèles (Popper, 1959). Cette notion est particulièrement importante, car elle est à la base de la réflexion concernant un protocole expérimental : plutôt que de se demander « comment il est possible de confirmer une hypothèse », il faudrait se demander « quelle est la pire condition à laquelle un modèle pourrait être exposé pour s’assurer qu’il du sens ? ». Si cet encart peut paraître être très lié à de la méthodologie expérimentale plutôt qu’à des statistiques, il pour objectif de mettre en exergue que tout est lié : il n’est pas possible de comprendre correctement les statistiques sans comprendre la méthodologie et sans avoir les bases théoriques qui sous-tendent des hypothèses (Popper, 1959).Comme tout logiciel de statistiques qui se respecte, R permet de réaliser un ensemble de statistiques descriptives telles qu’une moyenne. Il est dès lors nécessaire de présenter brièvement les possibilités offertes par le logiciel.","code":""},{"path":"les-statistiques-descriptives.html","id":"un-exemple-de-travail","chapter":"8 Les statistiques descriptives","heading":"8.2 Un exemple de travail","text":"Il faut noter que l’exemple présenté ici l’est à titre pédagogique. Des exemples pratiques seront mis à disposition de la section ad hoc. Néanmoins, afin de permettre de suivre les exercices, les données brutes utilisées pour l’exemple sont présentées ci-dessous.Des chercheurs en informatique ont développé un indice pour mesurer la sensibilité des souris : le Mickey. Il s’agit du plus petit mouvement détectable par un souris (de l’ordre d’un centième de millimètre). Voici les valeurs de Mickey pour un échantillon de 50 souris. Pour suivre le contenu de ce chapitre, il est possible de copier coller le code ci-dessous dans la console R, ou mieux dans un script.","code":"\nMickey<-c(0.0113330511711523, 0.0105185934305092, 0.0122513865231973, \n0.00799584561999877, 0.0132469458473191, 0.0157068027032564, \n0.00566814411520385, 0.00651748542037685, 0.0136296612222217, \n0.0131472071409701, 0.0112584029308029, 0.0073426743605309, 0.00819412905929006, \n0.0136385776747903, 0.00862091807205004, 0.00281600176335617, \n0.0110708097988487, 0.0166980801865014, 0.010854177755818, 0.0149495260876833, \n0.00718270492413349, 0.0115037061826869, 0.00636648780668218, \n0.00882227263188745, 0.0110918586149659, 0.010824072121312, 0.0140521995836573, \n0.007002510588409, 0.00684296297629619, 0.00846894194399499, \n0.00951172046799207, 0.0035962148433201, 0.0141868125694707, \n0.00833252179279338, 0.0130351412904844, 0.0112662065604392, \n0.0132115045987431, 0.00750178351850188, 0.00605748408387878, \n0.00901230294010093, 0.00756045671627793, 0.0149879411056116, \n0.0132127370139097, 0.0145783151332873, 0.0115756506296022, 0.0117585225313167, \n0.0145584346705547, 0.0112858517472226, 0.00884871032853877, \n0.0159595622769445)"},{"path":"les-statistiques-descriptives.html","id":"aspects-théoriques","chapter":"8 Les statistiques descriptives","heading":"8.3 Aspects théoriques","text":"","code":""},{"path":"les-statistiques-descriptives.html","id":"les-indices-de-tendance-centrale","chapter":"8 Les statistiques descriptives","heading":"8.3.1 Les indices de tendance centrale","text":"Trois indices de tendance centrale sont communément utilisés en statistiques : la\nmoyenne, la médiane et le mode. Plus récemment, un quatrième indice commencé à émerger : les moyennes tronquées (Wilcox, 2012). Il existe par ailleurs des indices moins connus tels que le M-estimator de Huber (Huber, 1981) ou la moyenne winsorisée (Wilcox, 2012).","code":""},{"path":"les-statistiques-descriptives.html","id":"le-mode","chapter":"8 Les statistiques descriptives","heading":"8.3.1.1 Le mode","text":"Le mode représente la valeur la plus représentée dans l’effectif. Par\nexemple, si 2 personnes ont la valeur 5 et 3 personnes ont la valeur 10, le mode sera 10, car il y plus de personnes qui ont 10 que de personnes qui ont 5. Cet indice de tendance central peut être utilisé sur tous les types de variables (catégorielle, ordinale, métrique). Elle est particulièrement adaptée pour les variables qualitatives.Calculer le mode sur une variable numérique relativement peu de sens, du moins si elle est continue, car il est très rare qu’il y ait plusieurs observations avec exactement la même valeur. Nous allons recatégoriser ces valeurs en valeurs inférieures à 0.005, entre 0.005 et 0.1 , entre 0.01 et 0.015 et au-dessus :peut à présent comptabiliser les valeurs uniques différentes :Dans la table ci-dessus, constate que ce sont les valeurs comprises entre 0.005 et 0.01 qui sont les plus fréquentes, ce qui représente donc le mode.","code":"\nMickey.range<-ifelse(Mickey<0.005, \"<0.005\",\n                     ifelse(Mickey>0.005 & Mickey<0.01, \"0.005<x<0.01\", \n                            ifelse(Mickey>0.015, \"<0.015\",\"0.01<x<0.015\" )))\nMickey.range##  [1] \"0.01<x<0.015\" \"0.01<x<0.015\" \"0.01<x<0.015\" \"0.005<x<0.01\" \"0.01<x<0.015\"\n##  [6] \"<0.015\"       \"0.005<x<0.01\" \"0.005<x<0.01\" \"0.01<x<0.015\" \"0.01<x<0.015\"\n## [11] \"0.01<x<0.015\" \"0.005<x<0.01\" \"0.005<x<0.01\" \"0.01<x<0.015\" \"0.005<x<0.01\"\n## [16] \"<0.005\"       \"0.01<x<0.015\" \"<0.015\"       \"0.01<x<0.015\" \"0.01<x<0.015\"\n## [21] \"0.005<x<0.01\" \"0.01<x<0.015\" \"0.005<x<0.01\" \"0.005<x<0.01\" \"0.01<x<0.015\"\n## [26] \"0.01<x<0.015\" \"0.01<x<0.015\" \"0.005<x<0.01\" \"0.005<x<0.01\" \"0.005<x<0.01\"\n## [31] \"0.005<x<0.01\" \"<0.005\"       \"0.01<x<0.015\" \"0.005<x<0.01\" \"0.01<x<0.015\"\n## [36] \"0.01<x<0.015\" \"0.01<x<0.015\" \"0.005<x<0.01\" \"0.005<x<0.01\" \"0.005<x<0.01\"\n## [41] \"0.005<x<0.01\" \"0.01<x<0.015\" \"0.01<x<0.015\" \"0.01<x<0.015\" \"0.01<x<0.015\"\n## [46] \"0.01<x<0.015\" \"0.01<x<0.015\" \"0.01<x<0.015\" \"0.005<x<0.01\" \"<0.015\"\ntab<-table(Mickey.range)\ntab## Mickey.range\n##       <0.005       <0.015 0.005<x<0.01 0.01<x<0.015 \n##            2            3           19           26\ntab[which.max(tab)]## 0.01<x<0.015 \n##           26"},{"path":"les-statistiques-descriptives.html","id":"la-médiane","chapter":"8 Les statistiques descriptives","heading":"8.3.1.2 La médiane","text":"La médiane représente la valeur x dans un échantillon de N individus qui divisera cet échantillon en deux lorsque les valeurs sont ordonnées de manière croissante. Cet indice de tendance central peut être utilisé avec une variable ordinale et une variable métrique. La raison pour laquelle il est admis d’utiliser la médiane sur les variables ordinales est que cet indice ne nécessite aucune présupposition sur les propriétés d’intervalle des échelles. Par ailleurs, la médiane est particulièrement intéressante pour limiter l’impact des valeurs extrêmes. Par exemple, quand enregistre les temps de réaction, ils sont le plus souvent de l’ordre de quelques centaines de millisecondes. Pourtant,certains temps sont supérieurs à plusieurs secondes. Cela arrive quand l’individu été inattentif à cet instant. L’utilisation des médianes permet d’éviter que cette valeur ait un impact trop important sur les résultats. En d’autres termes, la médiane n’est pas sensible aux valeurs extrêmes. Notez néanmoins que cette médiane est souvent utilisée en intra-individuel (lorsque plusieurs mesures de temps de réaction sont réalisées chez le même individu), mais plus rarement sur des comparaisons de groupes (néanmoins, pour outils inférentiels utilisant la médian, voir Wilcox, 2012).Dans notre exemple, peut commencer par trier les données :L’observation qui va diviser notre échantillon en deux échantillons de taille identique est donc comprise entre la \\(25^e\\) et la \\(26^e\\) observations. va donc faire la moyenne entre ces deux valeurs.peut obtenir cette valeur plus rapidement avec la fonction median :","code":"\nMickey<-sort(Mickey)\nMickey##  [1] 0.002816002 0.003596215 0.005668144 0.006057484 0.006366488 0.006517485\n##  [7] 0.006842963 0.007002511 0.007182705 0.007342674 0.007501784 0.007560457\n## [13] 0.007995846 0.008194129 0.008332522 0.008468942 0.008620918 0.008822273\n## [19] 0.008848710 0.009012303 0.009511720 0.010518593 0.010824072 0.010854178\n## [25] 0.011070810 0.011091859 0.011258403 0.011266207 0.011285852 0.011333051\n## [31] 0.011503706 0.011575651 0.011758523 0.012251387 0.013035141 0.013147207\n## [37] 0.013211505 0.013212737 0.013246946 0.013629661 0.013638578 0.014052200\n## [43] 0.014186813 0.014558435 0.014578315 0.014949526 0.014987941 0.015706803\n## [49] 0.015959562 0.016698080\n(Mickey[25]+Mickey[26])/2## [1] 0.01108133\nmedian(Mickey)## [1] 0.01108133"},{"path":"les-statistiques-descriptives.html","id":"la-moyenne","chapter":"8 Les statistiques descriptives","heading":"8.3.1.3 La moyenne","text":"Le troisième indice de tendance centrale est la moyenne, qui est la somme des valeurs\nindividuelles \\(x_i\\) divisé par le nombre d’observations N :\\[\\mu=\\frac{\\sum_{=1}^{N}X_i}{N}\\]Notez que la moyenne est désignée par la lettre grecque \\(\\mu\\) lorsqu’il s’agit d’un paramètre et par la lettre « m » lorsqu’il s’agit d’une estimation du paramètre à partir d’un échantillon. Cette distinction entre lettres romaines et lettres grecques pour distinguer l’évaluation d’un paramètre ou son estimation à\npartir d’un échantillon s’applique à l’ensemble des indices. Ainsi, pour l’estimation d’une moyenne à partir d’un échantillon, la formule est :\\[m=\\frac{\\sum_{=1}^{N}X_i}{N}\\]Concrètement, si prend notre exemple, la moyenne vaut la somme des Mickey :qu’va diviser par le nombre d’observation :Ce qui donne :peut évidemment obtenir cette valeur directement à l’aide de la fonction mean","code":"\nsum(Mickey)## [1] 0.527654\nlength(Mickey) # 50 observations ## [1] 50\nsum(Mickey)/length(Mickey)## [1] 0.01055308\nmean(Mickey)## [1] 0.01055308"},{"path":"les-statistiques-descriptives.html","id":"la-moyenne-tronquée","chapter":"8 Les statistiques descriptives","heading":"8.3.1.4 La moyenne tronquée","text":"La moyenne tronquée est un mélange entre la moyenne et la médiane : la moyenne tronquée représente la moyenne sur les valeurs qui restent après avoir supprimé un certain pourcentage des valeurs les plus extrêmes. fait donc la moyenne sur $k=1 - 2$ où \\(\\alpha\\) représente la probabilité d’observations tronquées. multiplie cette probabilité par 2 puisque la troncature s’applique sur les deux extrémités de la courbe. Notez que, en fonction du contexte, la troncature peut s’appliquer sur une des extrémités de la courbe uniquement.Ainsi, pour une moyenne tronquée à 10%, il faut d’abord identifier le nombre d’observations sur lesquelles il faut faire la moyenne. En l’occurrence, nous avons 50 observations, 10% de 50 vaut 5. Comme fait la troncature sur les deux extrémités, va donc retirer \\(k= 50\\times 2\\times 0.10 = 10\\). Donc, la moyenne sera réalisée sur \\(R=50 - 10 =40\\) observations restantes.\\[\nT=\\frac{1}{n\\times(1-2\\alpha)}\n\\left(\n(1-r)\\left(X_{g+1}+X_{n-g}\\right)\n+\\sum_{=g+2}^{n-g-1} X_i\n\\right)\n\\]où X représente les valeurs ordonnées, n est le nombre d’observation, \\(\\alpha\\) est la troncature à chaque extrémité, g est la part entière de \\(n \\times \\alpha\\) et r est la partie restante après la virgule de \\(n \\times \\alpha\\).Ainsi, commence par trier les observationsOn identifie la \\(6^e\\) observationEt la \\(45^e\\) observationPuisque \\(0.1 \\times 50 = 5.00\\), la part entière de g vaut 5 et la partie restante après la virgule vaut 0.peut à présent compléter la formule avec les valeurs suivantes :\\[\nT=\\frac{1}{n\\times(1-(2\\alpha))}\n\\left(\n(1-r)\\left(X_{g+1}+X_{n-g}\\right)\n+\\sum_{=g+2}^{n-g-1} X_i\n\\right)\n\\]\\[\nT=\\frac{1}{50 \\times (1-(2 \\times 0.10))}\n\\left(\n(1-0)\\left(0.0065 + 0.0146\\right)\n+\\sum_{=7}^{44} X_i\n\\right) = 0.01062119\n\\]Dans R, obtient la moyenne tronquée en précisant l’argument tr (pour trimmed) de la fonction mean.","code":"\nMickey.tri<- sort(Mickey)\nMickey.tri[6]## [1] 0.006517485\nMickey.tri[45]## [1] 0.01457832\nTr<-(1/40 *((1-0)*(Mickey.tri[6]+Mickey.tri[45])+sum(Mickey.tri[7:44])))\nTr## [1] 0.01062119\nmean(Mickey, tr=0.10)## [1] 0.01062119"},{"path":"les-statistiques-descriptives.html","id":"la-moyenne-winsorisée","chapter":"8 Les statistiques descriptives","heading":"8.3.1.5 La moyenne winsorisée","text":"La moyenne winsorisée fonctionne sur un principe similaire à la moyenne tronquée. Elle s’en distingue par le fait que, au lieu de ne pas inclure les valeurs extrêmes dans le calcul, ramène les valeurs tronquées aux dernières valeurs à l’intérieur des limites. D’après Wilcox (2012), il s’agit d’un indicateur plus robuste que la moyenne tronquée.Dans notre exemple, la \\(6^e\\) valeur des Mickey lorsqu’les trie par ordre croissant estOn va donc attribuer cette valeur aux 5 valeurs inférieures à cette valeur :fait la même chose pour les observations au-delà de la 45ème position :Ainsi, les 6 premières d’observations ont la même valeur et les 6 dernières ont la même valeur :peut à présent faire la moyenne sur ces valeursLa fonction winsor.mean du package ‘psych’ permet d’obtenir la moyenne winsorisée :note : peut également calculer une variance ou un écart-type winsorisé.","code":"\nMickey.tri[6]## [1] 0.006517485\nMickey.win<-Mickey.tri\nMickey.win[1:5] <-Mickey.win[6]\nMickey.win[46:50]<-Mickey.win[45]\nMickey.win##  [1] 0.006517485 0.006517485 0.006517485 0.006517485 0.006517485 0.006517485\n##  [7] 0.006842963 0.007002511 0.007182705 0.007342674 0.007501784 0.007560457\n## [13] 0.007995846 0.008194129 0.008332522 0.008468942 0.008620918 0.008822273\n## [19] 0.008848710 0.009012303 0.009511720 0.010518593 0.010824072 0.010854178\n## [25] 0.011070810 0.011091859 0.011258403 0.011266207 0.011285852 0.011333051\n## [31] 0.011503706 0.011575651 0.011758523 0.012251387 0.013035141 0.013147207\n## [37] 0.013211505 0.013212737 0.013246946 0.013629661 0.013638578 0.014052200\n## [43] 0.014186813 0.014558435 0.014578315 0.014578315 0.014578315 0.014578315\n## [49] 0.014578315 0.014578315\nmean(Mickey.win)## [1] 0.01060654\nlibrary(psych)\nwinsor.mean(Mickey.win, tr=0.10)## [1] 0.01060654"},{"path":"les-statistiques-descriptives.html","id":"le-m-estimateur-de-huber","chapter":"8 Les statistiques descriptives","heading":"8.3.1.6 Le M-estimateur de Huber9","text":"Il s’agit d’une fonction de minimisation. Il fonctionne sur la base d’un processus itératif (comme le maximum de vraisemblance si cette notion vous parle). Huber (1981) proposé d’utiliser cette fonction de minimisation pour estimer le paramètre d’une moyenne.\\[\\sum_{=1}^n \\psi \\left(\\frac{X_i-t}{\\sigma} \\right)= 0 \\]où\n\\[ \\psi_k(x) =  \\left \\{\n   \\begin{array}{r c l}\n      k, & x \\geq k \\\\\n      x   &  -k \\geq x \\leq k \\\\\n      -k  & x \\leq -k\n   \\end{array}\n   \\right .\\]Si, d’un point de vue mathématique, cette fonction peut paraître un peu compliquée, se rend compte en termes de calcul qu’il s’agit d’une opération assez simple.commence par définir la moyenne par la médiane.Pour cette fonction, l’idée est de déterminer si, dans un ensemble de données, il existe des valeurs extrêmes et, si c’est le cas, de les remplacer par des valeurs moins extrêmes. Un des moyens pour identifier s’il y des valeurs extrêmes est de déterminer si certaines valeurs s’éloignent de plus de 1.5 écart moyen à la médiane (mad - voir les indices de dispersion). Pour bien comprendre le fonctionnement de cette fonction, nous prendrons des données qui présentent des valeurs extrêmes.La première étape est donc de déterminer un indice de tendance centrale, et cet indice est la médiane. Dans notre exemple, la médiane de vaut :et le madOn se rend compte que les valeurs 100 et 1000 sont bien supérieures à 1.5 mad de la médiane. va donc les remplacer par $médiane + 1.5 mad $ :obtient donc les valeurs suivantes :peut à présent calculer la moyenne sur ces valeurs :va vérifier si l’estimation de notre moyenne s’écarte d’une valeur inférieure à une valeur déterminée de tolérance. Généralement, considère qu’une tolérance acceptable est \\(10^{-6}\\). Ainsi, il ne faut pas que la moyenne s’écarte de plus de \\(10^{-6} \\times mad\\) en valeur absolue.Si l’écart est supérieur, comme c’est le cas ici, répète l’opération, mais va comparer la prochaine moyenne estimée à la première moyenne estimée. Concrètement,calcule une nouvelle moyenne :Et vérifie si l’écart entre la première et la seconde estimation de la moyenne est inférieure à la tolérance :Ce n’est pas le cas, recommence l’opérationCe n’est toujours pas le cas, va donc encore répéter l’opération jusqu’à ce notre test devienne vrai :la 8e itération, le critère de tolérance est accepté, nous avons donc obtenu notre M-estimateur qui vaut :obtient le M-estimateur de Huber à l’aide de la fonction huber du package MASS.","code":"\na<-c(1:10, 100, 1000)\na##  [1]    1    2    3    4    5    6    7    8    9   10  100 1000\nmedian(a) ## [1] 6.5\nmad(a) ## [1] 4.4478\n  median(a) + 1.5*mad(a)## [1] 13.1717\n  yy <- pmin(pmax(median(a) - 1.5 * mad(a), a), median(a) + 1.5 * mad(a))\n  yy##  [1]  1.0000  2.0000  3.0000  4.0000  5.0000  6.0000  7.0000  8.0000  9.0000\n## [10] 10.0000 13.1717 13.1717\nm<-mean(yy)\nm## [1] 6.778617\nabs(m-median(a)) < 10^-6 *mad(a)## [1] FALSE\n  yy <- pmin(pmax(m - 1.5 * mad(a), a), m + 1.5 * mad(a))\n  yy##  [1]  1.00000  2.00000  3.00000  4.00000  5.00000  6.00000  7.00000  8.00000\n##  [9]  9.00000 10.00000 13.45032 13.45032\n  m2 <- mean(yy)\n  m2## [1] 6.825053\nabs(m-m2) < 10^-6 *mad(a)## [1] FALSE\n  yy <- pmin(pmax(m2 - 1.5 * mad(a), a), m2 + 1.5 * mad(a))\n  yy##  [1]  1.00000  2.00000  3.00000  4.00000  5.00000  6.00000  7.00000  8.00000\n##  [9]  9.00000 10.00000 13.49675 13.49675\n  m3 <- mean(yy)\n  m3## [1] 6.832792\nabs(m2-m3) < 10^-6 *mad(a)## [1] FALSE\n# 4e itération   \n  yy <- pmin(pmax(m3 - 1.5 * mad(a), a), m3 + 1.5 * mad(a))\n  m4<-mean(yy)\n  abs(m4-m3) < 10^-6 *mad(a)## [1] FALSE\n# 5e itération   \n  yy <- pmin(pmax(m4 - 1.5 * mad(a), a), m4 + 1.5 * mad(a))\n  m5<-mean(yy)\n  abs(m4-m5) < 10^-6 *mad(a)## [1] FALSE\n# 6e itération   \n  yy <- pmin(pmax(m5 - 1.5 * mad(a), a), m5 + 1.5 * mad(a))\n  m6<-mean(yy)\n  abs(m6-m5) < 10^-6 *mad(a)## [1] FALSE\n# 7e itération  \n  yy <- pmin(pmax(m6 - 1.5 * mad(a), a), m6 + 1.5 * mad(a))\n  m7<-mean(yy)\n  abs(m6-m7) < 10^-6 *mad(a)## [1] FALSE\n# 8e itération  \n  yy <- pmin(pmax(m7 - 1.5 * mad(a), a), m7+ 1.5 * mad(a))\n  m8<-mean(yy)\n  abs(m8-m7) < 10^-6 *mad(a)## [1] TRUE\nm8## [1] 6.83434\nlibrary(MASS)\nhuber(a)## $mu\n## [1] 6.834339\n## \n## $s\n## [1] 4.4478"},{"path":"les-statistiques-descriptives.html","id":"les-indices-de-dispersion","chapter":"8 Les statistiques descriptives","heading":"8.3.2 Les indices de dispersion","text":"","code":""},{"path":"les-statistiques-descriptives.html","id":"minimum-maximum-et-étendue","chapter":"8 Les statistiques descriptives","heading":"8.3.2.1 Minimum, maximum et étendue","text":"","code":""},{"path":"les-statistiques-descriptives.html","id":"quartile","chapter":"8 Les statistiques descriptives","heading":"8.3.2.2 Quartile","text":"","code":""},{"path":"les-statistiques-descriptives.html","id":"variance-et-écart-type","chapter":"8 Les statistiques descriptives","heading":"8.3.2.3 Variance et écart-type","text":"","code":""},{"path":"les-statistiques-descriptives.html","id":"écart-moyen-absolu-à-la-médiane-mad","chapter":"8 Les statistiques descriptives","heading":"8.3.2.4 Écart moyen absolu à la médiane (mad)","text":"","code":""},{"path":"les-statistiques-descriptives.html","id":"les-statistiques-descriptives-avec-r","chapter":"8 Les statistiques descriptives","heading":"8.4 Les statistiques descriptives avec R","text":"Ainsi, pour rappel, il est toujours bon d’avoir installé et chargé les packages au début de votre script. Nous aurons besoin du package psych (Revelle, 2025), readxl (Wickham & Bryan, 2022), ggplot2 (Wickham, 2016b) et éventuellement Hmisc (Harrell Jr, 2025) et summarytools (Comtois, 2025)Les packages que vous n’avez pas encore installés peuvent être installés avec les lignes de commande suivantes :Pour charger ces packages, il faut utiliser les lignes de commande suivantes :","code":"\ninstall.packages(\"ggplot2\")\ninstall.packages(\"psych\")\ninstall.packages(\"readxl\")\ninstall.packages(\"Hmisc\")\ninstall.packages(\"summarytools\")\ninstall.packages(\"grid\")\ninstall.packages(\"titanic\")\nlibrary(\"ggplot2\")\nlibrary(\"psych\")\nlibrary(\"readxl\")\nlibrary(\"Hmisc\")\nlibrary(\"summarytools\")\nlibrary(\"grid\")\nlibrary(\"titanic\")"},{"path":"les-statistiques-descriptives.html","id":"traiter-des-données-quantitatives","chapter":"8 Les statistiques descriptives","heading":"8.5 Traiter des données quantitatives","text":"Lorsque l’cherche à analyser des données quantitatives, il est important de fournir des statistiques descriptives telles que la moyenne, la médiane, l’écart-type, le minimum, le maximum ou encore les quartiles. Ces statistiques permettent d’obtenir les caractéristiques principales d’une variable quantitative. La façon la plus simple et la plus rapide d’y parvenir est d’utiliser la fonction summary.Afin d’illustrer cette fonction (et les suivantes), nous utiliserons le jeu de données mtcars qui est intégré dans R.\nLa base de données mtcars contient des informations sur 32 voitures et 11 variables, telles que la consommation de carburant par miles (mpg), le nombre de cylindres (cyl), la puissance en chevaux (hp), et le poids (wt). Pour ceux qui sont intéressés, vous pouvez obtenir le détail complet du jeu de données en utilisant la fonction ?mtcars. Les premières lignes du jeu de données sont présentées ci-dessous :mpgcyldisphpdratwtqsecvsamgearcarb21.061601103.902.62016.46014421.061601103.902.87517.02014422.84108933.852.32018.61114121.462581103.083.21519.44103118.783601753.153.44017.02003218.162251052.763.46020.221031On obtient donc les statistiques descriptives en utilisant la fonction summary de ce jeu de données de la manière suivante :Cependant, il existe plusieurs packages R permettant d’obtenir des statistiques descriptives plus précises ou plus directement, notamment la fonction describe du package Hmisc (Harrell Jr, 2025) ou l’intégralité du package summarytools (Comtois, 2025) qui est spécifiquement dédié aux statistiques descriptives. Dans ce document, nous utiliserons principalement le package psych (Revelle, 2025) et plus particulièrement les fonctions describe et describeby qui permettent de fournir rapidement les statistiques descriptives souhaitées. Nous laissons le soin au lecteur intéressé d’explorer les autres fonctions et packages si les informations fournies par le package psych ne sont pas suffisantes.Attention : Si vous avez chargés à la fois les packages psych et hmisc, vous pourriez rencontrer un conflit car ces deux packages possèdent une fonction appelée describe. Cela peut entraîner des erreurs lors de l’exécution de votre code. Pour éviter ce problème, veillez à préciser le package dont vous souhaitez utiliser la fonction en écrivant par exemple : psych::describe ou Hmisc::describe. Cette syntaxe permet de spécifier clairement le package à utiliser dans votre code en faisant préceder la fonction describe par le nom du package suivant de deux fois deux points.Table 8.1: Statistiques descriptives des caractéristiques des voitures (mtcars)varsnmeansdmediantrimmedmadminmaxrangeskewkurtosisse132.0020.096.0319.2019.705.4110.4033.9023.500.61-0.371.07232.006.191.796.006.232.974.008.004.00-0.17-1.760.32332.00230.72123.94196.30222.52140.4871.10472.00400.900.38-1.2121.91432.00146.6968.56123.00141.1977.1052.00335.00283.000.73-0.1412.12532.003.600.533.703.580.702.764.932.170.27-0.710.09632.003.220.983.333.150.771.515.423.910.42-0.020.17732.0017.851.7917.7117.831.4214.5022.908.400.370.340.32832.000.440.500.000.420.000.001.001.000.24-2.000.09932.000.410.500.000.380.000.001.001.000.36-1.920.091032.003.690.744.003.621.483.005.002.000.53-1.070.131132.002.811.622.002.651.481.008.007.001.051.260.29Lorsque l’execute le code dans R, peut constater que la fonction describe du package psych fournit des statistiques descriptives pour chaque variable de la base de données mtcars.En regardant les résultats nous savons par exemple qu’il y 32 individus et que le nombre moyen de cylindre par voiture est de 6.19.peut également faire apparaître simplement les quantiles de nos variables dans le tableau des résultats si veut avoir une idée de la distribution de nos données.L’argument quant permet de spécifier les qunatiles à calculer pour chaque variable.Table 8.2: Statistiques descriptives de mtcars avec les quantilesvarsnmeansdmediantrimmedmadminmaxrangeskewkurtosisseQ0.1Q0.25Q0.5Q0.75Q0.9132.0020.096.0319.2019.705.4110.4033.9023.500.61-0.371.0714.3415.4319.2022.8030.09232.006.191.796.006.232.974.008.004.00-0.17-1.760.324.004.006.008.008.00332.00230.72123.94196.30222.52140.4871.10472.00400.900.38-1.2121.9180.61120.83196.30326.00396.00432.00146.6968.56123.00141.1977.1052.00335.00283.000.73-0.1412.1266.0096.50123.00180.00243.50532.003.600.533.703.580.702.764.932.170.27-0.710.093.013.083.703.924.21632.003.220.983.333.150.771.515.423.910.42-0.020.171.962.583.333.614.05732.0017.851.7917.7117.831.4214.5022.908.400.370.340.3215.5316.8917.7118.9019.99832.000.440.500.000.420.000.001.001.000.24-2.000.090.000.000.001.001.00932.000.410.500.000.380.000.001.001.000.36-1.920.090.000.000.001.001.001032.003.690.744.003.621.483.005.002.000.53-1.070.133.003.004.004.005.001132.002.811.622.002.651.481.008.007.001.051.260.291.002.002.004.004.00La variance de nos variables n’apparait pas dans le tableau et doit donc ajouter une colonne que l’va inclure dans le tableau si désire accéder à cette information. Il suffit d’élever l’écart-type (correspondant à la colonne sd) au carré pour obtenir la variance.Table 8.3: Statistiques descriptives des caractéristiques des voitures (mtcars) - avec variancevarsnmeansdmediantrimmedmadminmaxrangeskewkurtosisseQ0.1Q0.25Q0.5Q0.75Q0.9variance132.0020.096.0319.2019.705.4110.4033.9023.500.61-0.371.0714.3415.4319.2022.8030.0936.32232.006.191.796.006.232.974.008.004.00-0.17-1.760.324.004.006.008.008.003.19332.00230.72123.94196.30222.52140.4871.10472.00400.900.38-1.2121.9180.61120.83196.30326.00396.0015,360.80432.00146.6968.56123.00141.1977.1052.00335.00283.000.73-0.1412.1266.0096.50123.00180.00243.504,700.87532.003.600.533.703.580.702.764.932.170.27-0.710.093.013.083.703.924.210.29632.003.220.983.333.150.771.515.423.910.42-0.020.171.962.583.333.614.050.96732.0017.851.7917.7117.831.4214.5022.908.400.370.340.3215.5316.8917.7118.9019.993.19832.000.440.500.000.420.000.001.001.000.24-2.000.090.000.000.001.001.000.25932.000.410.500.000.380.000.001.001.000.36-1.920.090.000.000.001.001.000.251032.003.690.744.003.621.483.005.002.000.53-1.070.133.003.004.004.005.000.541132.002.811.622.002.651.481.008.007.001.051.260.291.002.002.004.004.002.61On obtient donc une colonne supplémentaire dans la table avec la variance de chacune de nos variables.","code":"\ndata(mtcars)\nsummary(mtcars) ##       mpg             cyl             disp             hp       \n##  Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n##  1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n##  Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n##  Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n##  3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n##  Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n##       drat             wt             qsec             vs        \n##  Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n##  1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n##  Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n##  Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n##  3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n##  Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n##        am              gear            carb      \n##  Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n##  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n##  Median :0.0000   Median :4.000   Median :2.000  \n##  Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n##  3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n##  Max.   :1.0000   Max.   :5.000   Max.   :8.000\ndesc<-psych::describe(mtcars)\ndesc.data<-psych::describe(mtcars,quant=c(.1,.25,.5,.75,.90)) \n#Ajout de la colonne variance \n\ndesc.data$variance<-desc.data$sd^2"},{"path":"les-statistiques-descriptives.html","id":"analyse-en-sous-groupe","chapter":"8 Les statistiques descriptives","heading":"8.5.1 Analyse en sous-groupe","text":"L’analyse en sous-groupes permet de mieux comprendre les variations au sein d’un jeu de données notamment en fonction de critères spécifiques. Par exemple, des scientifiques font souvent des hypothèses concernant des différences de moyennes entre des conditions. Analyser les données en sous-groupes permet de savoir si les résultats vont dans le sens des hypothèses, et quand ils sont significatifs, ces statistiques descriptives permettent d’interpréter le sens de l’inférence à devoir réaliser.\nEn résumé, diviser les données en groupes permet d’identifier des tendances, des différences ou des comportements particuliers qui seraient invisibles dans une analyse globale. Cette approche permet ainsi d’obtenir des informations plus détaillées et pertinentes pour chaque groupe.La fonction describeby de psych permet à ce titre d’analyser un jeu de données en créant des sous-groupes basés sur une variable catégorielle.Cela nous donne des statistiques descriptives détaillées pour les voitures à 4, 6 et 8 cylindres. Cependant, l’analyse des résultats peut être complexe comme le voit avec la sortie du code, car ces sous-groupes génèrent beaucoup de valeurs à interpréter. Il peut donc être difficile de tirer des conclusions rapidement si l’regarde toutes les statistiques en même temps et les tableaux peuvent être difficile à lire simultanément.À ce titre, nous allons nous intéresser spécifiquement aux voitures à 4 cylindres, ce qui nous permettra de simplifier l’interprétation des données en nous concentrant sur un sous-groupe particulier.Les résultats sont présentés dans le Tableau @(tab:cylinde)Ce code extrait uniquement les statistiques descriptives pour les voitures à 4 cylindres à partir des résultats complets obtenus précédemment avec la fonction describeBy.Table 8.4: Statistiques descriptives des voitures à 4 cylindres.varsnmeansdmediantrimmedmadminmaxrangeskewkurtosisse111.0026.664.5126.0026.446.5221.4033.9012.500.26-1.651.36211.004.000.004.004.000.004.004.000.000.00311.00105.1426.87108.00104.3043.0071.10146.7075.600.12-1.648.10411.0082.6420.9391.0082.6732.6252.00113.0061.000.01-1.716.31511.004.070.374.084.020.343.694.931.241.000.120.11611.002.290.572.202.270.541.513.191.680.30-1.360.17711.0019.141.6818.9018.991.4816.7022.906.200.55-0.020.51811.000.910.301.001.000.000.001.001.00-2.474.520.09911.000.730.471.000.780.000.001.001.00-0.88-1.310.141011.004.090.544.004.110.003.005.002.000.11-0.010.161111.001.550.522.001.560.001.002.001.00-0.16-2.150.16","code":"\n# La fonction describeBy permet de calculer des statistiques descriptives pour chaque sous-groupe\n# basé sur une variable catégorielle, ici le nombre de cylindres des voitures.\n\ndesc_by_cyl <- psych::describeBy(mtcars, group = mtcars$cyl)\n\ndesc_by_cyl## \n##  Descriptive statistics by group \n## group: 4\n##      vars  n   mean    sd median trimmed   mad   min    max range  skew\n## mpg     1 11  26.66  4.51  26.00   26.44  6.52 21.40  33.90 12.50  0.26\n## cyl     2 11   4.00  0.00   4.00    4.00  0.00  4.00   4.00  0.00   NaN\n## disp    3 11 105.14 26.87 108.00  104.30 43.00 71.10 146.70 75.60  0.12\n## hp      4 11  82.64 20.93  91.00   82.67 32.62 52.00 113.00 61.00  0.01\n## drat    5 11   4.07  0.37   4.08    4.02  0.34  3.69   4.93  1.24  1.00\n## wt      6 11   2.29  0.57   2.20    2.27  0.54  1.51   3.19  1.68  0.30\n## qsec    7 11  19.14  1.68  18.90   18.99  1.48 16.70  22.90  6.20  0.55\n## vs      8 11   0.91  0.30   1.00    1.00  0.00  0.00   1.00  1.00 -2.47\n## am      9 11   0.73  0.47   1.00    0.78  0.00  0.00   1.00  1.00 -0.88\n## gear   10 11   4.09  0.54   4.00    4.11  0.00  3.00   5.00  2.00  0.11\n## carb   11 11   1.55  0.52   2.00    1.56  0.00  1.00   2.00  1.00 -0.16\n##      kurtosis   se\n## mpg     -1.65 1.36\n## cyl       NaN 0.00\n## disp    -1.64 8.10\n## hp      -1.71 6.31\n## drat     0.12 0.11\n## wt      -1.36 0.17\n## qsec    -0.02 0.51\n## vs       4.52 0.09\n## am      -1.31 0.14\n## gear    -0.01 0.16\n## carb    -2.15 0.16\n## ------------------------------------------------------------ \n## group: 6\n##      vars n   mean    sd median trimmed   mad    min    max  range  skew\n## mpg     1 7  19.74  1.45  19.70   19.74  1.93  17.80  21.40   3.60 -0.16\n## cyl     2 7   6.00  0.00   6.00    6.00  0.00   6.00   6.00   0.00   NaN\n## disp    3 7 183.31 41.56 167.60  183.31 11.27 145.00 258.00 113.00  0.80\n## hp      4 7 122.29 24.26 110.00  122.29  7.41 105.00 175.00  70.00  1.36\n## drat    5 7   3.59  0.48   3.90    3.59  0.03   2.76   3.92   1.16 -0.74\n## wt      6 7   3.12  0.36   3.21    3.12  0.36   2.62   3.46   0.84 -0.22\n## qsec    7 7  17.98  1.71  18.30   17.98  1.90  15.50  20.22   4.72 -0.12\n## vs      8 7   0.57  0.53   1.00    0.57  0.00   0.00   1.00   1.00 -0.23\n## am      9 7   0.43  0.53   0.00    0.43  0.00   0.00   1.00   1.00  0.23\n## gear   10 7   3.86  0.69   4.00    3.86  0.00   3.00   5.00   2.00  0.11\n## carb   11 7   3.43  1.81   4.00    3.43  0.00   1.00   6.00   5.00 -0.26\n##      kurtosis    se\n## mpg     -1.91  0.55\n## cyl       NaN  0.00\n## disp    -1.23 15.71\n## hp       0.25  9.17\n## drat    -1.40  0.18\n## wt      -1.98  0.13\n## qsec    -1.75  0.65\n## vs      -2.20  0.20\n## am      -2.20  0.20\n## gear    -1.24  0.26\n## carb    -1.50  0.69\n## ------------------------------------------------------------ \n## group: 8\n##      vars  n   mean    sd median trimmed   mad    min    max  range  skew\n## mpg     1 14  15.10  2.56  15.20   15.15  1.56  10.40  19.20   8.80 -0.36\n## cyl     2 14   8.00  0.00   8.00    8.00  0.00   8.00   8.00   0.00   NaN\n## disp    3 14 353.10 67.77 350.50  349.63 73.39 275.80 472.00 196.20  0.45\n## hp      4 14 209.21 50.98 192.50  203.67 44.48 150.00 335.00 185.00  0.91\n## drat    5 14   3.23  0.37   3.12    3.19  0.16   2.76   4.22   1.46  1.34\n## wt      6 14   4.00  0.76   3.76    3.95  0.41   3.17   5.42   2.25  0.99\n## qsec    7 14  16.77  1.20  17.18   16.86  0.79  14.50  18.00   3.50 -0.80\n## vs      8 14   0.00  0.00   0.00    0.00  0.00   0.00   0.00   0.00   NaN\n## am      9 14   0.14  0.36   0.00    0.08  0.00   0.00   1.00   1.00  1.83\n## gear   10 14   3.29  0.73   3.00    3.17  0.00   3.00   5.00   2.00  1.83\n## carb   11 14   3.50  1.56   3.50    3.25  0.74   2.00   8.00   6.00  1.48\n##      kurtosis    se\n## mpg     -0.57  0.68\n## cyl       NaN  0.00\n## disp    -1.26 18.11\n## hp       0.09 13.62\n## drat     1.08  0.10\n## wt      -0.71  0.20\n## qsec    -0.92  0.32\n## vs        NaN  0.00\n## am       1.45  0.10\n## gear     1.45  0.19\n## carb     2.24  0.42\n# Ici, nous sélectionnons les statistiques spécifiques aux voitures à 4 cylindres \n# parmi les résultats obtenus pour tous les sous-groupes de cylindres.\n\ndesc_4_cyl <- desc_by_cyl[[\"4\"]]"},{"path":"les-statistiques-descriptives.html","id":"faire-une-représentation-graphique-des-données","chapter":"8 Les statistiques descriptives","heading":"8.6 Faire une représentation graphique des données","text":"Lorsque l’mène une analyse descriptive de données sur R, peut s’aider à l’aide de graphiques, ce qui permet de mieux visualiser leur distribution, leur tendance et les relations entre les variables. Pour cela, le package ggplot2 (Wickham, 2016b) est largement utilisé. Ce package permet de créer une variété de graphiques personnalisés, comme des histogrammes, des graphiques en violon ou des diagrammes de dispersion, facilitant ainsi l’analyse et l’interprétation des données.","code":""},{"path":"les-statistiques-descriptives.html","id":"graphique-violon","chapter":"8 Les statistiques descriptives","heading":"8.6.1 Graphique violon","text":"Le graphique en violon (violin plot) est une excellente optioin pour visualiser des données quantitatives. Il permet de représenter la distribution des valeurs d’une variable en montrant à la fois la densité et les principales statistiques descriptives.Le package ggplot2 (Wickham, 2016b), avec sa fonction ggplot permet de crée assez aisément un graphique en violon. Pour faciliter la lisibilité, les lignes de commande seront annotées au sein du code.Dans l’exemple suivant, va représente la consommation de carburant en miles par gallon (mpg). L’axe des ordonnées représente la consommation et l’axe des abscisses est fixé à une valeur constante pour afficher une seule distribution. Le violon montre la densité de la variable, et un point rouge indique la moyenne de la consommation, accompagné d’une barre bleue représentant l’intervalle de confiance autour de cette moyenne.\nFigure 8.1: Distribution de la consommation de carburant par miles (mpg)\npeut également utiliser ce type de graphique pour comparer la distribution d’une variable continue en fonction d’une variable catégorielle.\nPar exemple, dans le code suivant, la variable catégorielle cyl (le nombre de cylindres) est utilisée pour créer des violons séparés pour chaque groupe de cylindres, permettant de comparer les distributions de la consommation pour chaque catégorie de cylindres.\nFigure 8.2: Distribution de la consommation de carburant par miles (mpg) en fonction du nombre de cylindres (cyl)\nobserve que chaque violon correspond à une catégorie de cylindre (4,6 ou 8 cylindres) et montre la densité des valeurs de consommation à l’intérieur de cette catégorie.Avec cet exemple peut dire que les véhicules à 4 cylindres ont une consommation de carburant généralement moins élevée,cela montre qu’elles sont plus économes en carburant tandis que les véhicules à 8 cylindres ont une consommation plus élevée. Les véhicules à 6 cylindres se situent entre ces deux extrêmes.\nDans cet exemple, nous allons expliquer comment lire un graphique en violon en utilisant un exemple de la densité de la loi normale. Le graphique ci-dessous montre la densité de la distribution normale générée avec une moyenne de 50 et un écart-type de 10.Un graphique en violon représente la distribution des données en combinant des éléments d’un diagramme en boîte et la visualisation de la densité des données. Cette visualisation permet de montrer non seulement la tendance centrale (moyenne ou médiane), mais aussi la forme de la distribution et sa dispersion. Cela ressemble à la courbe de densité d’une loi normale, comme celle présentée ici par la courbe rouge.Les graphiques en violon sont souvent utilisés pour visualiser la distribution des données. Cependant, ces graphiques peuvent être difficiles à interpréter sans repères. C’est pourquoi nous utilisons la loi normale (ou distribution gaussienne) comme référence. La loi normale est l’une des distributions les plus courantes en statistiques et possède une forme bien définie et symétrique. En comparant la forme du graphique en violon à celle d’une loi normale, nous pouvons mieux comprendre comment les données se répartissent, identifier les éventuels écarts et repérer les asymétries ou anomalies dans la distribution.En d’autres termes, la courbe de densité de la loi normale nous aide à avoir un repère visuel pour interpréter les graphiques en violon.Dans le graphique en violon, cette densité est simplement représentée dans un axe à 90° par rapport à cette distribution.\nFigure 8.3: Représentation de la densité d’une distribution normale telle qu’peut la rencontrer dans un graphe violon.\nLes graphiques en violon sont souvent présentés avec une orientation horizontale, comme c’est le cas avec le graphique ci-dessus. Cela permet de mieux visualiser la densité des données sur un axe horizontal, ce qui correspond à l’orientation habituelle des graphiques en violon.ExerciceDansla base de données mickey2, trouve les caractéristiques de souris d’ordinateur. Veuillez répondre aux questions proposées.Utilisez la fonction describe ou bien la fonction describeBy du package psych (Revelle, 2025) pour obtenir des statistiques descriptives sur la variable Prix de notre jeu de données, tout en calculant également la variance. De plus, nous voudrions les quantiles .10, .25, .75, .90 de cette variable.Par ailleurs, voudrait également les statistiques descriptives de la variable prix en fonction des dpi des soucis.réaliser une analyse descriptive des variables, réaliser une analyse en sous-groupe avec la variable de votre choix dans la base de données mickey2 et réalisez au moins un graphique avec ggplot2.Nous devons dans un premier temps importer le jeu de données :À présent, nous allons définir une fonction appelée Prix, qui extraira spécifiquement la variable Prix du jeu de données Mickey. Cette fonction nous permettra de simplifier l’analyse en nous concentrant uniquement sur cette variable, afin d’explorer ses statistiques descriptives et d’identifier ses principales caractéristiques (Tableau 8.5).Table 8.5: Presentation du jeu de données mickeysourismickeyPoidsPrixDPIhz1.000.0185.0045.99800.00125.002.000.01120.0089.501,200.00500.003.000.0195.0019.991,600.001,000.004.000.0175.00129.00800.00125.005.000.01100.0059.992,400.00500.006.000.02110.0075.001,600.001,000.007.000.0190.00105.991,000.00125.008.000.01105.0089.901,200.00500.009.000.0180.0029.991,600.001,000.0010.000.01130.0049.002,000.00500.0011.000.01115.0085.00800.001,000.0012.000.0195.00120.002,400.00125.0013.000.01125.0095.503,200.00500.0014.000.0185.0060.00800.00125.0015.000.01110.00119.991,800.001,000.0016.000.00100.0050.001,200.00500.0017.000.01120.00110.001,000.00125.0018.000.02105.0069.992,400.001,000.0019.000.01115.0079.993,200.00500.0020.000.0195.0085.991,600.00125.0021.000.0180.00109.001,000.001,000.0022.000.01110.0059.99800.00125.0023.000.01130.0099.001,600.00500.0024.000.01120.0055.504,000.001,000.0025.000.01100.0089.995,000.00500.0026.000.0190.00120.001,000.001,000.0027.000.01125.0079.001,200.00125.0028.000.01105.00135.002,000.00500.0029.000.01115.0065.99800.00125.0030.000.01110.0079.991,600.001,000.0031.000.0195.0049.992,400.00125.0032.000.0085.0089.003,200.001,000.0033.000.01100.00120.501,800.00500.0034.000.01110.00100.002,400.00500.0035.000.01120.0069.004,000.00125.0036.000.0190.00110.991,000.001,000.0037.000.01105.0045.001,800.00125.0038.000.01115.0080.991,600.00500.0039.000.01100.00125.001,200.001,000.0040.000.01110.0090.002,000.00125.0041.000.01130.0075.993,200.00500.0042.000.01115.00120.001,600.001,000.0043.000.0185.0060.002,400.00125.0044.000.01100.0069.993,200.00500.0045.000.01125.00115.001,600.00125.0046.000.0195.0099.991,800.001,000.0047.000.01120.00130.002,400.00125.0048.000.01105.0060.991,600.001,000.0049.000.01110.0060.00800.00500.0050.000.0290.0045.004,000.00500.00Pour cela, nous allons utiliser le package psych et sa fonction describe, en appliquant ces outils directement à la fonction Prix, qui extrait cette variable du jeu de données Mickey.obtient le résultat suivant :Table 8.6: Statistiques descriptives de la variable prixvarsnmeansdmediantrimmedmadminmaxrangeskewkurtosisseQ0.1Q0.25Q0.75Q0.9variance1.0050.0083.6328.5283.0083.9134.0919.99135.00115.01-0.05-0.934.0348.7060.00108.25120.05813.40On constate que la moyenne des prix est de 83.63, ce qui reflète le prix moyen dans le jeu de données. L’écart-type vaut 28.52, avec des prix allant de 19.99 euros à 135.00 euros.Si veut effectuer une analyse en sous-groupes, il suffit d’utiliser la même logique que celle que nous avons employée ici. Imaginons que l’veuille des statistiques descriptives concernant la variable DPI , alors nous pouvons utiliser la fonction describeby de psych.obtient nos statistiques descriptives pour chaque sous-groupe de DPI.Tableau . Statistiques descriptives des variables Poids,Prix et hzcolumn nameitemgroup1varsnmeansdmediantrimmedmadminmaxrangeskewkurtosissePrix118001.007.0072.2827.5860.0072.288.8845.99129.0083.011.09-0.3010.42Prix2210001.005.00111.205.27110.00111.201.48105.99120.0014.010.72-1.242.35Prix3312001.005.0086.6826.8889.5086.6815.5750.00125.0075.000.07-1.4812.02Prix4416001.0010.0076.6932.6680.4978.3728.1819.99120.00100.01-0.40-1.1510.33Prix5518001.004.0096.3735.55109.9996.3715.2045.00120.5075.50-0.60-1.8117.78Prix6620001.003.0091.3343.0290.0091.3360.7949.00135.0086.000.03-2.3324.84Prix7724001.007.0084.2832.0869.9984.2829.6549.99130.0080.010.32-1.9012.12Prix8832001.005.0082.0910.1979.9982.0913.3669.9995.5025.510.14-1.944.56Prix9940001.003.0056.5012.0355.5056.5015.5745.0069.0024.000.08-2.336.95Prix101050001.001.0089.9989.9989.990.0089.9989.990.00Comme vu précedemment, peut également représenter nos données avec un graphique afin d’obtenir un rendu plus clair et visible.\nFigure 8.4: Distribution des prix en fonction des DPI de la souris souris\nvoit que les prix ne semblent pas vraiment dépendre du DPI de la souris.","code":"\nggplot(data = mtcars, aes(x = 1, y = mpg)) + \n  # data = le jeu de données où on va trouver les variables d'intérêt, mpg en l'occurrence\n  # x = 1 : L'axe des x est constant, ce qui fait que tous les violons seront positionnés sur la même ligne\n  # y = mpg : L'axe des y représente la consommation (miles per gallon)\n  \n  geom_violin(fill = \"lightblue\", trim = FALSE) + \n  # fill = \"lightblue\" : Remplissage uniforme pour le violon avec la couleur 'lightblue'\n  \n  stat_summary(fun = mean, geom = 'point', color = \"red\") + \n  # Affiche la moyenne de la consommation comme un point rouge\n  \n  stat_summary(fun.data = 'mean_sdl', geom = \"errorbar\", color = \"blue\") + \n  # Affiche l'intervalle de confiance autour de la moyenne en bleu\n  \n  xlab(\" \") + \n  # Aucun titre pour l'axe des x, car il n'y a qu'une seule catégorie\n  \n  ylab(\"Consommation (mpg)\") + \n  # Titre de l'axe des y, indiquant la consommation en miles par gallon\n  \n  theme_light() + \n  # Thème léger pour le fond et le style du graphique\n  \n  theme(axis.title.x = element_blank(), \n        axis.text.x = element_blank(), \n        axis.ticks.x = element_blank())\n  # Supprime l'axe des x, car il n'est pas nécessaire ici\n  # la fonction ggplot permet de faire le graphique\n  # data = mtcars : On utilise le jeu de données mtcars\n  # aes(x = factor(cyl), y = mpg, fill = factor(cyl)) : \n  # - x = factor(cyl) : L'axe des x représente le nombre de cylindres (converti en facteur pour chaque groupe)\n  # - y = mpg : L'axe des y représente la consommation (miles per gallon)\n  # - fill = factor(cyl) : On colore les violons en fonction du nombre de cylindres\nggplot(data = mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) + \n  # geom_violin() : Crée un graphique en violon pour montrer la distribution des données\n  # trim = FALSE : On ne découpe pas les bords des violons, laissant toute la distribution visible\n  geom_violin(trim = FALSE) + \n  # stat_summary() : Ajoute une statistique résumée au graphique\n  # fun = mean : On veut afficher la moyenne de chaque groupe (cylindres) comme un point sur les violons.\n  # geom = 'point' : Le résumé est représenté par un point\n  stat_summary(fun = mean, geom = 'point') + \n  # stat_summary(fun.data = 'mean_sdl') : Affiche une statistique avec l'intervalle de confiance autour de la moyenne\n  # fun.data = 'mean_sdl' : Affiche la moyenne et l'écart-type comme une barre d'erreur autour du point moyen\n  stat_summary(fun.data = 'mean_sdl') + \n  # xlab(\"Cylindres\") : Ajoute un titre à l'axe des x, indiquant que l'on mesure les cylindres\n  xlab(\"Cylindres\") + \n  # ylab(\"Consommation\") : Ajoute un titre à l'axe des y, indiquant que l'on mesure la consommation de carburant par miles (mpg)\n  ylab(\"Consommation (mpg)\") + \n  # theme_light() : Applique un thème clair pour le fond et le style du graphique\n  theme_light() \nMickey<-readxl::read_xlsx(\"./statdesc/mickey2.xlsx\")\nMickeymouse <- psych::describe(Mickey$Prix, quant = c(.1, .25,  .75, .90))\n\nMickeymouse$variance <- Mickeymouse$sd^2\n\n\nMickeymouse_df <- as.data.frame(Mickeymouse)\ndesc_by_dpi <- psych::describeBy(Mickey[,c( \"Prix\")], \n                                 group = Mickey$DPI, mat=T)"},{"path":"les-statistiques-descriptives.html","id":"pourquoi-utiliser-la-loi-normale-pour-comprendre-les-graphiques-en-violon","chapter":"8 Les statistiques descriptives","heading":"8.7 Pourquoi utiliser la loi normale pour comprendre les graphiques en violon ?","text":"Les graphiques en violon sont souvent utilisés pour visualiser la distribution des données. Cependant, ces graphiques peuvent être difficiles à interpréter sans repères. C’est pourquoi nous utilisons la loi normale (ou distribution gaussienne) comme référence. La loi normale est l’une des distributions les plus courantes en statistiques et possède une forme bien définie et symétrique. En comparant la forme du graphique en violon à celle d’une loi normale, nous pouvons mieux comprendre comment les données se répartissent, identifier les éventuels écarts et repérer les asymétries ou anomalies dans la distribution.En d’autres termes, la courbe de densité de la loi normale nous aide à avoir un repère visuel pour interpréter les graphiques en violon.Dans le graphique en violon, cette densité est simplement représentée dans un axe à 90° par rapport à cette distribution.\nFigure 8.3: Représentation de la densité d’une distribution normale telle qu’peut la rencontrer dans un graphe violon.\nLes graphiques en violon sont souvent présentés avec une orientation horizontale, comme c’est le cas avec le graphique ci-dessus. Cela permet de mieux visualiser la densité des données sur un axe horizontal, ce qui correspond à l’orientation habituelle des graphiques en violon.","code":""},{"path":"les-statistiques-descriptives.html","id":"traiter-des-données-qualitatives","chapter":"8 Les statistiques descriptives","heading":"8.8 Traiter des données qualitatives","text":"Nous avons vu que la manière la plus efficace pour décrire les données qualitatives est de fournir la table des effectifs. Il s’agit du nombre d’observations pour chacune des modalités.Si la fonction summary permet de le faire, il semble plus pertinent d’utiliser la fonction table ou ftable en fonction du nombre de variables à prendre en considérationPour illustrer la manière dont fonctionnent ces fonctions, nous pouvons aussi utiliser un jeu de données intégré à R : le jeu de données titanic_train dans le package titanic (titanic?).Voici les premières lignes du jeu de données :Tableau . Statistiques descriptives du jeu de données titanic PassengerIdSurvivedPclassNameSexAgeSibSpParchTicketFareCabinEmbarked103Braund, Mr. Owen Harrismale22.0010A/5 211717.25S211Cumings, Mrs. John Bradley (Florence Briggs Thayer)female38.0010PC 1759971.28C85C313Heikkinen, Miss. Lainafemale26.0000STON/O2. 31012827.92S411Futrelle, Mrs. Jacques Heath (Lily May Peel)female35.001011380353.10C123S503Allen, Mr. William Henrymale35.00003734508.05S603Moran, Mr. Jamesmale003308778.46QOn peut commencer par se demander combien de personnes étaient dans chacune des classes de passagers :voit qu’il y 216 personnes en première classe, 184 en deuxième et 491 personnes en troisième classe.Si se demande dans quelle mesure, le fait d’être sauvé dépend de la classe, nous pouvons croiser les deux variables et faire un tableau à doubles entrées.Nous nous rendons compte de deux phénomènes :l’absence de nom aux dimensions de la table la rend difficile à lire ;il est difficile d’apprécier la survieNous pouvons corriger ces deux défauts en utilisant l’argument dnn et la fonction addmargins. L’argument dnn permet de donner un nom aux dimensions. La première information est le nom de la variable pour les lignes et la seconde est le nom des variables pour les colonnes.Pour rendre les choses encore plus claires, connaitre la fréquence pour chaque modalité serait un plus. peut calculer les fréquences sur les effectifs totaux en divisant la table par la somme des effectifs :Dans certains cas, il est souhaitable d’obtenir ces informations par ligne ou par colonne. nouveau, R permet d’obtenir ces informations mais l’obtention de ces informations est techniquement un peu plus compliqué. Pour traduire la fonction ci-dessous : va retourner une nouvelle table grâce à la fonction sweep. Cette nouvelle table contiendra de nouvelles marges grâce à la fonction addmargins. va ajouter ces marges à l’objet “tab”, sur la première dimension de la table (donc ajoute des lignes) et dans ses marges, nous souhaitons les totaux de chaque colonne (en ajoutant des lignes), qui sera intitulé “N”. fera également, grâce à la fonction sum, une somme des différentes lignes (aboutissant donc à 100% dans chaque colonne). Enfin, pour les pourcentages, va utiliser la fonction apply sur la seconde dimension (le 2) et en divisant les totaux divisés par 100.Si cette fonction vous semble compliquée, vous pouvez simplement vous contenter de comprendre que vous devez donner le nom de la table, ‘tab’, dans mon exemple et garder le reste sans changement.Avec cette même logique, peut obtenir les pourcentages par ligne.Dans ce tableau, constate beaucoup plus aisément qu’avait plus de chance de survivre si avait un ticket en \\(1^e\\) qu’en \\(3^e\\) classe.Lorsqu’doit faire des tables plus complexes, impliquant plus de deux variables, peut utiliser la fonction ftable. L’utilisation de la fonction est assez simple : il suffit de donner à R un ensemble de variables qu’il peut considérer comme des facteurs. Le plus simple est d’utiliser un data.frame et de préciser le nom des variables à inclure.Ainsi, si veut regarder si le croisement entre le pont d’embarquement et le sexe eu un impact sur la survie en fonction de la classe, peut faire :Notez qu’il est possible de modifier l’apparence de la table en choisissant les variables qui doivent être présentées sur les lignes et celles qu’il faut présenter sur les colonnes en précisant les arguments row.vars et col.vars.ExerciceCet exercice vous permettra de mettre en pratique les outils de statistiques descriptives sur des données qualitatives et d’analyse de tableaux croisés en R. Vous explorerez les relations entre les variables qualitatives du jeu de données Titanic en manipulant des tableaux de fréquences avec la fonction table et en ajoutant des totaux grâce à addmargins. Vous calculerez également des pourcentages à l’aide des fonctions sweep et apply, avant d’utiliser ftable pour analyser les interactions entre plusieurs variables, notamment le sexe, l’âge catégorisé, et la survie , vous êtes libres de chosir les variables à analyser. Enfin, vous serez amené à interpréter les résultats obtenus pour tirer des conclusions sur les tendances observées.Questions :Comptez le nombre de passagers en fonction du sexe.Comptez le nombre de passagers en fonction du sexe.la survie est-elle la même entre les hommes et les femmes ?la survie est-elle la même entre les hommes et les femmes ?Ajoutez des totaux pour chaque catégorie dans le tableau croisé entre sexe et survie.Ajoutez des totaux pour chaque catégorie dans le tableau croisé entre sexe et survie.Transformez les comptages en pourcentages pour visualiser les proportions de survie.Transformez les comptages en pourcentages pour visualiser les proportions de survie.Créez des groupes d’âge (enfants, jeunes adultes, adultes, seniors) et analysez la survie au sein de chaque groupe.Créez des groupes d’âge (enfants, jeunes adultes, adultes, seniors) et analysez la survie au sein de chaque groupe.Pour débuter, il est indispensable d’importer la base de données titanic_train nécessaire à la réalisation de l’exercice. Il est probable que le jeu de données soit déjà installé sur votre session mais si toutefois ça n’est pas le cas , vous pouvez utiliser la fonction data pour l’importer directement dans votre environnement de travail.Si le lecteur désire plus d’informations concernant le jeu de données titanic_train , il peut utiliser la commande ?titanic_train pour avoir le détail des informations concernant cette dernière.Commençons par examiner combien de passagers étaient des hommes et combien étaient des femmes dans le jeu de données. Pour ça utilise la fonction table sur la variable Sex.voit que parmis les passagers, il y 314 femmes et 577 hommes.Ensuite, analysons comment la survie varie selon le sexe. Nous croisons les variables Sex et Survived à l’aide d’un tableau à doubles entrées pour savoir si la survie est similaire entre les sexes.Le tableau croisé montre le nombre de survivants (1) et de non-survivants (0) selon le sexe. Pour les femmes, 233 ont survécu et 81 n’ont pas survécu. Pour les hommes, 109 ont survécu et 468 n’ont pas survécu. Il semble donc, priori, qu’il y ait plus de femmes à avoir survécu que d’hommes.Néanmoins, le tableau précédent n’est pas très lisible. Donnons des noms aux dimensions (colonnes et lignes) et ajoutons des marges pour avoir les totaux. Cela nous permettra d’interpréter plus facilement les résultats.Pour mieux comprendre les proportions, transformons le tableau précédent en fréquences en divisant chaque cellule par le total des effectifs.Maintenant, voyons comment les pourcentages de survie varient par ligne (par sexe). Cette étape est utile pour analyser les chances de survie selon chaque groupe.Nous allons maintenant créer une nouvelle variable, AgeGroup, pour catégoriser l’âge en groupes : enfants, jeunes adultes, adultes et seniors. Une fois la variable créée, examinons si l’âge eu un impact sur la survie des passagers.\nLa fonction cut permet de diviser une variable continue en intervalles définis. Dans cet exemple, elle est utilisée pour créer une nouvelle variable AgeGroup en catégorisant les âges des passagers en quatre groupes : “Enfant” pour les passagers de 0 à 18 ans, “Jeune adulte” de 18 à 35 ans, “Adulte” de 35 à 50 ans, et “Senior” de 50 à 100 ans. Les intervalles sont définis avec breaks et les catégories sont nommées avec labels. Cette fonction transforme ainsi une variable numérique en une variable catégorielle et vous pouvez l’utiliser dans vos lignes de code.Enfin, peut combiner les variables AgeGroup, Sex et Survived pour examiner leur interaction. Pour ça peut utiliser la fonction ftable pour obtenir un tableau lisible et multidimensionnel.Le tableau montre que les femmes, surtout les enfants et les seniors, avaient des chances de survie nettement plus élevées que les hommes. Les enfants, indépendamment du sexe, avaient de meilleures chances, confirmant les priorités “femmes et enfants d’abord”. En revanche, les hommes jeunes et adultes ont eu les taux de survie les plus faibles, reflétant une vulnérabilité accrue dans ces groupes.","code":"\nlibrary(titanic)\ndata(titanic_train)\ntable(titanic_train$Pclass)## \n##   1   2   3 \n## 216 184 491\ntable(titanic_train$Pclass, titanic_train$Survived)##    \n##       0   1\n##   1  80 136\n##   2  97  87\n##   3 372 119\ntab<-table(titanic_train$Pclass, titanic_train$Survived, dnn=list(\"classe\", \"Survie\"))\ntab<-addmargins(tab)\ntab##       Survie\n## classe   0   1 Sum\n##    1    80 136 216\n##    2    97  87 184\n##    3   372 119 491\n##    Sum 549 342 891\ntab<-round(tab/sum(tab), 3) \ntab##       Survie\n## classe     0     1   Sum\n##    1   0.022 0.038 0.061\n##    2   0.027 0.024 0.052\n##    3   0.104 0.033 0.138\n##    Sum 0.154 0.096 0.250\nround(\n  sweep(\n    addmargins(tab, 1, list(list(All = sum, \n                                 N = function(x) sum(x)^2/100))), 2,apply(tab, 2, sum)/100, \"/\"),\n    1)##       Survie\n## classe     0     1   Sum\n##    1     7.2  19.9  12.2\n##    2     8.8  12.6  10.4\n##    3    33.9  17.3  27.5\n##    Sum  50.2  50.3  49.9\n##    All 100.0 100.0 100.0\n##    N     0.3   0.2   0.5\nround(\n   sweep(\n     addmargins(\n       tab, 2, list(list(All = sum, N = \n                           function(x) sum(x)^2/100))), 1,apply(tab, 1, sum)/100, \"/\"),\n       1)##       Survie\n## classe     0     1   Sum   All     N\n##    1    18.2  31.4  50.4 100.0   0.1\n##    2    26.2  23.3  50.5 100.0   0.1\n##    3    37.8  12.0  50.2 100.0   0.3\n##    Sum  30.8  19.2  50.0 100.0   0.5\nftable(titanic_train[,c(\"Survived\", \"Pclass\", \"Sex\",\"Embarked\")])##                        Embarked       C   Q   S\n## Survived Pclass Sex                            \n## 0        1      female            0   1   0   2\n##                 male              0  25   1  51\n##          2      female            0   0   0   6\n##                 male              0   8   1  82\n##          3      female            0   8   9  55\n##                 male              0  33  36 231\n## 1        1      female            2  42   1  46\n##                 male              0  17   0  28\n##          2      female            0   7   2  61\n##                 male              0   2   0  15\n##          3      female            0  15  24  33\n##                 male              0  10   3  34\ntab<-ftable(titanic_train[,c(\"Survived\", \"Pclass\", \"Sex\",\"Embarked\")], \n       row.vars =c(\"Pclass\", \"Embarked\") , \n       col.vars =c(\"Survived\", \"Sex\"))\ntab##                 Survived      0           1     \n##                 Sex      female male female male\n## Pclass Embarked                                 \n## 1                             0    0      2    0\n##        C                      1   25     42   17\n##        Q                      0    1      1    0\n##        S                      2   51     46   28\n## 2                             0    0      0    0\n##        C                      0    8      7    2\n##        Q                      0    1      2    0\n##        S                      6   82     61   15\n## 3                             0    0      0    0\n##        C                      8   33     15   10\n##        Q                      9   36     24    3\n##        S                     55  231     33   34\ndata(\"titanic_train\")\ntable(titanic_train$Sex)## \n## female   male \n##    314    577\ntable(titanic_train$Sex, titanic_train$Survived)##         \n##            0   1\n##   female  81 233\n##   male   468 109\ntab <- table(titanic_train$Sex, titanic_train$Survived, dnn = list(\"Sexe\", \"Survie\"))\ntab <- addmargins(tab)\ntab##         Survie\n## Sexe       0   1 Sum\n##   female  81 233 314\n##   male   468 109 577\n##   Sum    549 342 891\ntab <- round(tab / sum(tab), 3)\ntab##         Survie\n## Sexe         0     1   Sum\n##   female 0.023 0.065 0.088\n##   male   0.131 0.031 0.162\n##   Sum    0.154 0.096 0.250\nround(sweep(addmargins(tab, 2, list(list(All = sum, N = function(x) sum(x)^2 / 100))), \n            1, apply(tab, 1, sum) / 100, \"/\"), 1)##         Survie\n## Sexe         0     1   Sum   All     N\n##   female  13.1  36.9  50.0 100.0   0.2\n##   male    40.4   9.6  50.0 100.0   0.3\n##   Sum     30.8  19.2  50.0 100.0   0.5\ntitanic_train$AgeGroup <- cut(titanic_train$Age, breaks = c(0, 18, 35, 50, 100), \n                              labels = c(\"Enfant\", \"Jeune adulte\", \"Adulte\", \"Senior\"))\ntable(titanic_train$AgeGroup, titanic_train$Survived)##               \n##                  0   1\n##   Enfant        69  70\n##   Jeune adulte 221 137\n##   Adulte        92  61\n##   Senior        42  22\nftable(titanic_train[, c(\"Survived\", \"AgeGroup\", \"Sex\")])##                       Sex female male\n## Survived AgeGroup                    \n## 0        Enfant               22   47\n##          Jeune adulte         26  195\n##          Adulte               15   77\n##          Senior                1   41\n## 1        Enfant               46   24\n##          Jeune adulte         94   43\n##          Adulte               41   20\n##          Senior               16    6"},{"path":"les-statistiques-descriptives.html","id":"statistiques-descriptives-avec-easier","chapter":"8 Les statistiques descriptives","heading":"8.9 Statistiques descriptives avec easieR","text":"Pour réaliser des statistiques descriptives avec easieR (Stefaniak, 2018), il faut avoir au préalable avoir chargé le package easieR et lancé easieR avec la fonction easieR().La seconde étape consiste à importer les données.","code":""},{"path":"les-différentes-méthodes-dinférences-statistiques.html","id":"les-différentes-méthodes-dinférences-statistiques","chapter":"9 Les différentes méthodes d’inférences statistiques","heading":"9 Les différentes méthodes d’inférences statistiques","text":"","code":""},{"path":"comment-choisir-la-bonne-analyse.html","id":"comment-choisir-la-bonne-analyse","chapter":"10 Comment choisir la bonne analyse","heading":"10 Comment choisir la bonne analyse","text":"“Si les statistiques vous ennnuient, c’est sans doute parce que vous n’avez pas les bons chiffres.”","code":""}]
