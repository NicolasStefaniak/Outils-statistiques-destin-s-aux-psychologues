[["index.html", "Outils statistiques destinés aux psychologues ", " Outils statistiques destinés aux psychologues Nicolas Stefaniak 2026-01-27 "],["licence.html", "Licence", " Licence Ce document est mis à disposition selon les termes de la licence : Creative Commons Attribution – Pas d’Utilisation Commerciale 4.0 Internationale (CC BY-NC 4.0). Vous êtes autorisé·e à : partager — copier, distribuer et communiquer le matériel adapter — remixer, transformer et créer à partir du matériel Sous les conditions suivantes : Attribution — vous devez créditer l’auteur Pas d’utilisation commerciale Texte complet de la licence : https://creativecommons.org/licenses/by-nc/4.0/ "],["pourquoi-les-statistiques.html", "Chapter 1 Pourquoi les statistiques 1.1 Á quoi servent les statistiques ? 1.2 Pourquoi les statistiques sont indispensables pour de futurs psychologues ? 1.3 Exercice", " Chapter 1 Pourquoi les statistiques “Statistical thinking will one day be as necessary for efficient citizenship as the ability to read or write. (Un jour, la pensée statistiques sera aussi indispensable pour pouvoir être un citoyen engagé qu’être capable de lire et d’écrire)” — H. G. Wells 1.1 Á quoi servent les statistiques ? Vous êtes-vous déjà demandé pourquoi vous aviez des cours de statistiques ? Avez-vous déjà pensé ou entendu des camarades dire « je ne vois pas pourquoi on a de tels cours, moi je veux être psy (ou biologiste, chimiste, médecin…) » Vous avez sans doute déjà pensé ou entendu « de toute façon, j’ai jamais rien compris aux maths, je veux juste arriver à compenser », voire affirmé avec force « les statistiques sont là pour nous éliminer et pour nous démotiver ». Au mieux, certain·es voulaient faire preuve de lucidité « on en aura besoin pour notre TER », mais leur voix furent rapidement étouffée par « de toute façon, on n’en aura jamais besoin dans notre vie pro ». Cependant, la véritable raison des cours de statistiques est bien plus inavouable que vous ne l’imaginez. Pour la comprendre, il faut se poser une question : à quoi sevent les statistiques ? Prenez le temps d’y répondre par vous-même avant de continuer votre lecture. Beaucoup de personnes considèrent que les statistiques permettent de rendre objectif, scientifique, indiscutable des faits qui sont présentés. Cependant, cette vision des statistiques est très étriquées et ne correspond, la plupart du temps, pas à la réalité. L’objectif premier des statistiques de faire passer un message, de convaincre et d’influencer. Comparez ces différentes phrases et questionnez-vous sur leur caractère persuasif : Il y a de plus en plus de personnes au chômage ; Depuis 2023, le taux de chômage a doublé ; Depuis 2023, le taux de chômage est passé de 4% à 8% ; Depuis 2023, le taux de chômage est passé de 4% à 8%, ce qui représente un niveau historiquement haut qui n’a plus été atteint depuis 1975 ; En deux ans, le taux de chômage a doublé, passant de 4% à 8%, cette augmentation est la plus forte enregistrée dans des délais aussi court depuis les années 1970. On constate que les deux dernières propositions ont un pouvoir persuasif bien plus important et finalement, qui ira vérifier ? L’important est d’être convaincant, pas que ce soit vrai. Cet usage des statistiques est, qu’on le veuille ou non, extrêmement fréquente. Il est donc nécessaire de pouvoir avoir un regard critique sur ce que les chiffres signifient pour pouvoir se forger se propre opinion. En l’absence de cette capacité, on peut se trouver face à deux sortes d’écueils tout aussi graves l’un que l’autre : accepter sans condition le message transmis, sans que notre esprit critique n’ait quoique ce soit à y redire ; raisonner sur la base d’une incompréhension des statistiques et aboutir à des conclusions qui sont fausses. Au travers de quelques exemples, nous allons explorer comment les statistiques peuvent être utilisées pour duper les esprits qui y sont hermétiques. 1.1.1 L’efficacité du vaccin contre la covid Durant la crise covid qui a eu lieu entre 2020 et 2022, la recherche d’un traitement efficace s’est avéré être d’une importance capitale pour pouvoir reprendre un mode de fonctionnement identique à celui qui précédait le confinement. Parmi ces traitements, le vaccin à ARN s’est vite imposé comme étant une solution fiable et robuste pour lutter contre les dégâts de la maladie. Cependant, à cette époque, de nombreuses critiques issues des antivax se faisaient entendre. Parmi les raisonnements les plus fréquents, on pouvait avoir celui-ci : la preuve que le vaccin ne fonctionne pas est qu’il y a autant de personnes qui sont hospitalisées à cause de la covid en étant vaccinée que de personnes qui ne le sont pas. Si on analye les chiffres bruts, les antivax ont raison : il y a autant de personnes vaccinées que de personnes vaccinées, comme le montre la Figure 1.1. Figure 1.1: Nombre de personnes hospitalisées en fonction du statut de vaccination Cependant, un élève de CM1 sait ce qu’est la notion de proportion. Ainsi, on ne peut comparer le nombre de personnes vaccinées de personnes non vaccinées qu’à la condition d’avoir autant de personnes vaccinées et non vaccinées, ce qui n’était pas le cas. En effet, il y avait environ 10 fois plus de personnes vaccinées que de personnes non vaccinées. Ainsi, dans la Figure 1.2, le cadre indique les personnes qui ont été hospitalisées et on se rend compte que proportionnellement au nombre de personnes non vaccinées, les personnes vaccinées étaient moins souvent hospitalisées, ou pour le formuler autrement le rapport entre les verts en dehors du cadre sur ceux qui sont dans le cadre est plus important que le rapport des rouges en dehors du cadre par rapport à ceux qui sont à l’intérieur du cadre. Figure 1.2: Nombre de personnes hospitalisées vs non hospitalisation en fonction du statut de vaccination 1.1.2 La Suède et le paradis des violeurs 1.1.2.1 Version standard En 2015, pendant la campagne présidentielle aux USA, un site d’information d’extrême droite appelé Breitbart a publié un article qui incitait les citoyens américains à regarder ce qui se passait en Suède de la manière suivante. La particularité de cet article était qu’il s’appuyait sur une utilisation trompeuse des statistiques pour nourrir la peur, la haine et l’exclusion. Concrètement, ils effectuaient une comparaison brute de chiffres qui ne renvoyaient pas aux mêmes réalités. Concrètement l’article utilisait les statistiques de viol pour dénoncer des sociétés ouvertes à l’immigration, en particulier la Suède en tenant le raisonnement suivant : ce pays présente l’un des taux de viol les plus élevés du monde occidental ; cette situation serait la conséquence directe de l’accueil d’immigrés issus de cultures supposément plus violentes. Une telle conclusion peut sembler convaincante à première vue, mais elle repose sur une incompréhension profonde de ce que mesurent réellement ces chiffres. Comparer les taux de viol entre pays n’a de sens que si l’on compare des indicateurs construits de manière identique. Or ce n’est pas le cas. Comme l’explique von Hofer (Hofer, 2000), les statistiques de viol sont des constructions sociales et juridiques avant d’être des reflets directs de la réalité criminelle. Ainsi, en Suède, toute plainte pour viol est comptabilisée comme telle, y compris lorsque l’affaire est ultérieurement classée sans suite ou requalifiée. Dans de nombreux autres pays, seules les condamnations judiciaires sont prises en compte. Ces deux indicateurs ne mesurent donc pas la même chose et cela ne fait dès lors pas sens de comparer les taux de viols entre des pays qui n’utilisent pas la même définition pour les comptabiliser. En outre, s’ajoutent des différences majeures dans les définitions légales. La notion de viol est plus large en Suède que dans beaucoup d’autres pays : des faits qui seraient qualifiés d’« abus sexuel » ailleurs, notamment en France, entrent dans la catégorie du viol en Suède. De plus, lorsqu’une même victime subit des viols répétés par le même agresseur, comme c’est le cas lorsqu’un mari viole sa femme, chaque acte est comptabilisé séparément dans les statistiques suédoises, alors qu’ils sont souvent regroupés en un seul événement dans d’autres pays. Les facteurs culturels jouent également un rôle déterminant. La propension à déclarer un viol dépend fortement du contexte social, du regard porté sur les victimes et des risques encourus après une plainte. Dans certaines sociétés, dénoncer un viol expose les victimes à la stigmatisation, à des représailles, voire à des violences graves. Dans ces conditions, des taux officiellement faibles ne traduisent pas une moindre prévalence des violences sexuelles, mais un sous-signalement massif. À titre d’illustration, une enquête montre qu’environ un quart des Français considèrent que l’auteur d’un viol est moins responsable si la victime portait une tenue jugée sexy (Le Breton, 2016), ce qui donne la mesure des freins culturels à la dénonciation, y compris dans des pays occidentaux. Les écarts entre statistiques officielles et données issues de la recherche scientifique renforcent ce constat. Koss et al. (1987), dans une enquête menée auprès d’étudiantes américaines, montrent que plus de la moitié rapportent avoir subi des abus sexuels au sens large, et qu’environ 15 % déclarent avoir été violées. Ces chiffres sont sans commune mesure avec ceux issus des statistiques policières ou judiciaires, ce qui illustre à quel point les données administratives sous-estiment la réalité des violences sexuelles. Ainsi, affirmer qu’un pays serait plus dangereux qu’un autre en se fondant uniquement sur des taux de viol publiés sans analyse méthodologique revient à tirer des conclusions infondées. Les chiffres ne sont pas des faits bruts : ils sont le produit de définitions juridiques, de pratiques institutionnelles et de contextes culturels. Sans cette compréhension, les comparaisons internationales deviennent non seulement invalides, mais dangereuses, car elles peuvent servir de support à des discours idéologiques qui instrumentalisent la peur. Comprendre les statistiques, c’est apprendre à se demander ce qui est réellement mesuré, comment cela l’est, et ce que les chiffres ne disent pas. C’est à cette condition seulement que les données peuvent éclairer le débat public au lieu de l’obscurcir. 1.1.2.2 Version conte de fées Il était une fois une forêt, appelée la forêt de Breitbart. Dans la forêt de Breitbart vivait une femme. On dit d’elle qu’elle avait des pouvoirs magiques. On l’appelait la sorcière de Breitbart. Cette sorcière n’avait pas plus de baguette magique qu’elle ne préparait de potion à base de griffes de dragons. Son pouvoir était bien plus sournois. Le pouvoir de cette sorcière était l’obscurantisme. Elle se complaisait dans la haine et l’exclusion. Elle avait ce pouvoir d’influencer et de manipuler l’esprit des plus fragiles. Elle se montrait patiente et efficace en attendant de pouvoir semer la graine de la haine dans le coeur des malheureux qui osaient s’aventurer dans cette ténébreuse forêt. Il arrivait que des promeneurs venus du monde entier passaient par la forêt de Breitbart et en ressortaient profondément changés : des agneaux transformés en loups. Quand on traversait cette forêt, il y avait comme un souffle, un murmure qui caressait les oreilles des passants. Il était si facile à écouter. C’était tellement apaisant et agréable d’entendre « vous êtes les meilleurs, la race supérieure, vous devez vous défendre » ou encore « montrer la supériorité des hommes sur les femmes » (l’auteur de ces lignes ayant des nausées en écrivant la manière de penser de cette sorcière s’est vu contraint de ne pouvoir donner d’autres exemples au risque d’être atteint de vomissements durables). La perfidie de cette sorcière était sans limite. Elle éprouvait une haine sans fin pour un pays appelé Ouvertuland. Dans ce pays, les hommes considéraient les femmes comme leurs égales, les habitants étaient convaincus que l’endroit de notre naissance n’était pas un choix, mais un état de fait et que chacun avait le droit d’aspirer à un monde meilleur lorsque la guerre et les persécutions frappaient, que la souffrance n’était pas tolérable, qu’importe l’endroit où on était né. La sorcière de Breitbart étalait cette haine au grand jour. Elle criait à qui voulait l’entendre : « Ecoutez-les se plaindre, regardez ce qui se passe là-bas, ils accueillent les étrangers, leurs femmes et leurs filles se font violer ; ne laissons pas des immigrés faire, protégeons-nous ; nous sommes supérieurs, ils ne méritent pas notre plus petite considération ». Regardez ce qu’il s’y passe, leur taux de viol est le plus élevé du monde occidental (et si vous faites une petite recherche sur le taux de viol en Suède notre statistique d’intérêt, vous trouverez que, en 2010, les statistiques de viol montrent que la Suède arrive en \\(3^{ème}\\) position derrière l’Afrique du Sud et le Botswana et si vous vous dites tout cela pour cela, vous auriez raison), tout cela parce qu’ils ont laissé des immigrés ayant une culture du viol entrer dans leur pays, voyez ce qui s’y passe et ne surtout ne faisons pas comme eux ». Et les esprits fragiles (dont vous ne faites pas partie… ou dont vous ne ferez bientôt plus partie) se laisseraient évidemment envahir par ces pensées, et le coeur des hommes vaillants deviendrait alors sombre et froid, intolérant et xénophobe. Comment était-il possible que ce pays tant admiré pour ses avancées sociales soit l’objet d’une situation aussi abjecte ? Et les passants qui entraient dans la forêt de Breitbart en sortaient avec la certitude que les immigrés étaient la cause de tous les problèmes de ces peuples trop ouverts, trop tolérants, trop naïfs. Un jour, un homme qui était connu sous le nom de Gandwells traversa la forêt de Breitbart. La sorcière de Breitbart exerça son pouvoir maléfique contre Gandwells. Le coeur de Gandwells vacilla, la haine commençait à le submerger, mais Gandwells avait l’esprit vif et incisif. Il concentra ses pensées pour tenter de combattre les pouvoirs de la sorcière. Il lui dit : « Donalda (c’était le prénom de la sorcière de Breitbart, et à nouveau toute ressemblance avec des personnes existant ou ayant existé est purement … volontaire), peux-tu m’expliquer comment les pays dont sont issus ces immigrés ont des taux de viols bien plus faibles que les pays dont proviennent ces immigrés ? » (Vous pouvez constater que des pays comme le Pakistan, l’Albanie ou l’Azerbaïdjan ont des taux de viols particulièrement faibles, alors que ce sont des pays qui correspondent aux critères dénoncés par Breitbart). La sorcière Breitbart argumenta : « C’est parce que, dans ces pays, le viol est tellement commun, qu’ils ne les comptabilisent même pas. » Gandwells demanda alors : « Et dans les autres pays, comment le taux de viol est-il calculé ? ». La sorcière venait d’être touchée, elle tenta de bredouiller mal à l’aise « Ben, euh, comme partout. -Je ne suis pas sûr de comprendre, reprit Gandwells. Vous avez dit que certains pays ne comptabilisent pas les viols, car c’est dans leur nature d’en perpétrer, et puis vous dites que tout le monde comptabilise les viols de la même manière… » La sorcière se tut un instant, ne sachant que répondre. Elle tenta : « Ça n’a strictement rien à avoir. Ce n’est pas pareil.» Gandwells ignora la sorcière et continua sa démonstration : « En réalité, je crois que le peuple d’Ouvertuland est particulièrement attentif au respect des êtres-humains, et du corps des femmes (Je me focalise sur les femmes car la très grande majorité des viols concerne les femmes). On peut penser qu’ils ne comptabilisent pas du tout les viols de la même manière » [Von Hofer (2000) explique en détail pourquoi les statistiques de viol ne peuvent pas être comparées d’un pays à l’autre. En effet, en Suède, est comptabilisé comme viol, toute plainte pour viol, même si par la suite l’affaire est classée ou que les charges sont réduites. Dans la plupart des autres pays, les statistiques de viols portent sur les condamnations uniquement. De même, en Suède, une personne qui va être violée 30 fois par son mari va compter comme 30 viols dans les statistiques, alors que, dans la plupart des autres pays, ce ne sera comptabilisé qu’une seule fois étant donné que ce sont les mêmes personnes qui sont impliquées dans chacun des viols. Par ailleurs, la définition de viol est plus large en Suède que dans d’autres pays. Ainsi, ce qui serait classé comme un abus sexuel dans un autre pays, serait considéré comme viol en Suède. On peut rajouter à cela que la culpabilisation des femmes dans la dénonciation des viols est très dépendante de la culture, avec dans certains pays, un réel risque d’être persécutée, voire tuée. Pour comprendre cet effet de la culture, 1/4 des Français estiment que l’auteur d’un viol est moins responsable si la victime portait une tenue sexy (Le Breton, 2016). Il n’est dès lors pas étonnant de ne pas oser dénoncer un viol dans ces conditions. La Suède étant très en avance sur la considération de l’être humain, les femmes reportent plus facilement les abus sexuels dont elles ont été victimes. Pour illustrer encore plus ces différences, sans entrer dans le débat sur la manière dont on doit comptabiliser les taux de viols, on peut regarder les différences qui existent entre les statistiques officielles et celles issues des articles scientifiques, comme le montre l’article de Koss et al. (1987) qui montre que 53% des étudiantes américaines ont été victimes d’abus sexuels au sens large, et 15% rapportent avoir été violées, ce qui ne correspond absolument pas aux chiffres très inférieurs publiés par les autorités des pays]. C’est ainsi que Gandwells mena son combat contre l’obscurantisme en illustrant avec force et conviction que les nombres ne renvoyaient pas aux mêmes réalités et que vouloir comparer les statistiques criminelles entre les pays et Ouvertuland n’avait pas de sens ! La sorcière de Breitbart se réfugia dans une grotte cachée au fin fond de sa forêt, humiliée par Gandwells, mais le combat n’était pas gagné pour autant, le plus dur restait à venir. Il fallait que le monde sache la perfidie de Donalda, il fallait que le monde ne se laisse pas manipuler par cette sorcière, il fallait apprendre aux hommes à avoir l’esprit vif et incisif, à se poser les bonnes questions. C’était la seule issue. 1.2 Pourquoi les statistiques sont indispensables pour de futurs psychologues ? En tant que futur3es psychologues, vous serez amené·e à utiliser des statistiques de manière récurrente dans plusieurs contextes : Identifier les propriétés psychométriques des outils que vous utilisez. En l’absence de cette analyse, vous risquez d’utiliser des outils dont les fondements scientifiques sont largement insuffisants (Lilienfeld et al., 2000; Strauss et al., 2006) ; Être capable de réaliser des inférences relatives au niveau de fonctionnement des usagers auxquels vous serez confronté·e, c’est-à-dire déterminer si l’usage présente un déficit, un trouble, un risque pour la société … selon l’environnement de travail dans lequel vous travaillerez ; Vous former en lisant des articles scientifiques, des livres ou des rapports, ou en suivant des formations. qu’importe le moyen vous devrez pouvoir vous demander “comment est-ce que cela a été montré ?” et “est-ce que cette manière de le montrer est convaincante ?”. Ainsi, la nature même d’un cours de statistiques n’est pas de vous apprendre une suite interminable d’outils dont vous devrez maîtriser les tenants et les aboutissants mais d’identifier les fondements qui sont communs à l’ensemble des outils statistiques afin de pouvoir donner du sens aux nombres. 1.3 Exercice En 2020, la covid19 fait rage, la population est confinée et aucun traitement ne semble efficace. Des milliers de personnes meurent tous les jours dans le monde à cause de cette infection. Dans cette course effrénée visant à rechercher un traitement, Raoult et son équipe ont publié une étude qui vantait l’efficacité de l’hydroxychloroquine associée à l’azythromycine pendant au moins trois jours pour lutter efficacement contre la covid-19 (Lagier et al., 2020). Imaginez que vous, ou une personne qui vous est chère, développe une forme grave de la covid19. Allez-vous prendre ce remède miracle ? Afin d’éclairer votre réflexion, vous trouverez dans la Figure 1.3 et dans le Tableau 1.4 et 1.5 les informations nécessaires pour prendre votre décision. Il est à noter que pour analyser ces résultats, nul besoin d’avoir des compétences en médecine. Les notions qui sont enseignées dans un cours d’introduction à la méthodologie, quelques connaissances d’ordre général et un peu de logique suffisent amplement à se faire une opinion. Figure 1.3: Diagramme de flux des participants permettant de représenter les allocations entre les différents groupes. Figure 1.4: Tableau des données démographiques. Figure 1.5: Tableau des résultats. Solution alternative Normalement, vous devriez avoir remarqué des problèmes qui remettent considérablement en cause les conclusions de Raoult et de ses collaborateurs. Une des remarques qui est souvent faite est que l’effectif n’est pas le même dans les différents groupes. En réalité, ce point ne pose pas réellement souci car il existe des outils statistiques en mesure de prendre cet aspect en compte. Par ailleurs, si on a de bonnes raisons de penser qu’un traitement est efficace, il serait éthiquement douteux de ne pas le proposer à une plus grande proportion de personnes que celles qui ne le reçoit pas. En revanche, ce qu’on observe : la proportion de personnes hospitalisées qui reçoivent le traitement (13.8%) est largement inférieure à la proportion de personnes hospitalisées qui ne le reçoivent pas (39.3%) ; l’indicateur NEWS-2 est un indicateur est un indicateur permettant d’évaluer l’état du patient au moment où il est examiné. Plus cette valeur est élevée et plus son état est dégradé. Dès lors que le score est supérieur à 5, les risques d’une réponse urgente deviennent important. Pour cet indicatuer, il apparaît que les risques sont beaucoup plus élevés (19.9%) pour les personnes qui ne reçoivent pas le traitement que pour les personnes qui le reçoivent (6.3%) un troisième problème qui saute d’emblée aux yeux est que, parmi les personnes qui font partie de la condition ‘autre traitement’, certains ont reçu l’association hydoxychloroquine (HCQ) et azymothricine (AZ) pendant moins de 3 jours, d’autres ont reçu uniquement de l’hydoxychloroquine, d’autres ont reçu uniquement de l’azymothricine et enfin, certaines personnes n’ont rien reçu. Cette répartition est étonnante car : on s’attend normalement à ce que le groupe de contrôle ne reçoivent pas le même traitement que le groupe traité (pendant une durée moindre). En effet, on peut raisonnablement penser que les personnes pour lesquelles le traitement n’a pas durée 3 jours ou plus sont celles dont l’état s’est dégradé et qu’elles ont basculé du bras “traité” au bras “contrôle”. Cette manière de faire est méthodologique douteuse. On ne change pas les membres d’une équipe en cours de match. Cela reviendrait à ce que, à un moment du match, un joueur de l’OM se mette à jouer pour le PSG (ou inversément). Il est possible de comparer HCQ+AZ &gt;3 jours avec HCQ &gt; 3 jours, AZ &gt; 3 jours et aucun traitement. Cependant, dans ce cas, on n’est pas dans un plan avec deux bras parallèles mais dans un plan factoriel. Il est donc anormal de regrouper ces trois groupes ensemble. Par ailleurs, le Tableau 1.4 montre que les personnes dans “autres groupes” sont plus âgés que dans le groupê HCQ+AZ. Concrètement, on a 3.6% de personnes de plus de 74 ans dans le groupe HCQ+AZ, alors que cette proportion est de 16% dans “autres traitements”. Or, on sait que les personnes les plus à risque de complication dus à la covid19 sont les personnes âgées. Remarquez que ce phénomène est valable pour chacun des sous-groupe, et en particulier le sous-groupe (à l’exception de l’hydroxychloroquine seule) qui a reçu le traitement pendant moins de trois jours. Ce tableau nous indique également que les scores au NEWS supérieur ou égal à 6 (donc requérant des soins urgents) est de 2.6% dans le groupe HCQ+AZ, alors que cette proportion est de 10.5% dans dans les autres groupes, avec une proportion de 14.2% dans le groupe qui a reçu le traitement pendant moins de 3 jours. Cette proportion particulièrement importante est interpellante car cela amène à penser que les personnes dont l’état se serait dégradé ou qui serait décédées en ayant reçu le traitement efficace ont été basculées dans le groupe de contrôle. Enfin, on observe également que le scanner pulmonaire est proportionnellement plus dégradé chez les personnes appartenant à “autre traitement” qu’au traitement HCQ+AZ. Ceci amène donc à se questionner sur la manière de choisir les personnes qui allaient recevoir le traitement : il semble que les personnes à risque de dégradation avaient moins de chance de recevoir le traitement, ce qui entrainement artificiellement un meilleur résultat pour les personnes traitées. Examinons à présent les personnes qui ont été hospitalisées en soins intensifs (ICU) et celles qui sont décédées dans le Tableau 1.5. Ils sont de 1.1% dans le groupe HCQ+AZ et de 9.4% dans le groupe “autre traitement”, avec un taux de 17% dans le groupe qui reçoit le traitement pendant moins de 3 jours, tandis que les personnes qui ne reçoivent rien ont une évolution vers les soins intensifs et le décès de 3.1%. Pour le formuler autrement, les personnes qui ne reçoivent rien ont un taux d’évolution négative proche du groupe traité, et le groupe pour lequel l’évolution est la plus négative est le groupe ayant reçu le traitement durant moins de trois jours. Donc, même si on laissait le bénéfice du doute concernant le fait d’avoir basculé les personnes recevant le traitement miracle dans le groupe de contrôle pour faire apparaître des résultats flatteurs, on aurait du mal à trouver une explication rationnelle pour rendre compte du fait que, si l’HCQ+AZ est efficace, les patients qui le reçoivent durant moins de 3 jours voient leur état se dégrader bien plus souvent que n’importe quel autre groupe. On devrait s’attendre à ce que cela les aide tout de même un peu (ou a minima que cela n’entraîne pas un risque supplémentaire) plutôt que de présenter un risque d’évolution négative plus important que les autres groupes Références Hofer, H. von. (2000). Crime statistics as constructs: The case of swedish rape statistics. European Journal on Criminal Policy and Research, 8(1), 77–89. https://doi.org/10.1023/A:1008713631586 Koss, M. P., Gidycz, C. A., &amp; Wisniewski, N. (1987). The scope of rape: Incidence and prevalence of sexual aggression and victimization in a national sample of higher education students. Journal of Consulting and Clinical Psychology, 55(2), 162–170. https://doi.org/10.1037/0022-006X.55.2.162 Lagier, J.-C., Million, M., Gautret, P., Colson, P., Cortaredona, S., Giraud-Gatineau, A., Honoré, S., Gaubert, J.-Y., Fournier, P.-E., Tissot-Dupont, H., Chabrière, E., Stein, A., Deharo, J.-C., Fenollar, F., Rolain, J.-M., Obadia, Y., Jacquier, A., La Scola, B., Brouqui, P., … Zandotti, C. (2020). Outcomes of 3, 737 COVID-19 patients treated with hydroxychloroquine/azithromycin and other regimens in marseille, france: A retrospective analysis. Travel Medicine and Infectious Disease, 36, 101791. https://doi.org/10.1016/j.tmaid.2020.101791 Le Breton, M. (2016). Pour 27% des français, l’auteur d’un viol est moins responsable si la victime portait une tenue sexy. Huffington Post France. http://www.huffingtonpost.fr/2016/02/29/francais-victime-viol-etude_n_9346702.html Lilienfeld, S. O., Wood, J. M., &amp; Garb, H. N. (2000). The scientific status of projective techniques. Psychological Science in the Public Interest, 1(2), 27–66. https://doi.org/10.1111/1529-1006.002 Strauss, E. M. S., Sherman, E. M. S., &amp; Spreen, O. (2006). Psychometrics in neuropsychological assessment. In E. Strauss, E. M. S. Sherman, &amp; O. Spreen (Eds.), A compendium of neuropsychological tests: Administration, norms, and commentary (3rd ed., pp. 3–43). Oxford University Press. "],["quel-logiciel-utiliser-et-pourquoi.html", "Chapter 2 Quel logiciel utiliser et pourquoi ? 2.1 Introduction 2.2 Qu’est-ce R et pourquoi l’utiliser ? 2.3 Qu’est-ce que easieR et pourquoi l’utiliser", " Chapter 2 Quel logiciel utiliser et pourquoi ? “Science advances by questioning results, not by hiding them. (La science avance en questionnant les résultats, non pas en les cachant” — Karl Popper 2.1 Introduction Aborder les statistiques dans la perspective d’une mise en application requiert non seulement de fournir les fondements théoriques pour choisir et comprendre les analyses statistiques mais doit également fournir les bases nécessaires pour utiliser de manière autonome un logiciel de statistiques. Bien qu’il existe une multitude de logiciels statistiques, nous nous consacrerons au logiciel R (R Core Team, 2025) et au package que nous avons développé, easieR (Stefaniak, 2018) Nous commencerons par expliquer pourquoi l’utilisation de R est bon pour vous. Ensuite, nous expliquerons comment l’installer et la logique sous-jacente à R. Nous continuerons avec les règles de bonnes pratiques qui vous seront utiles tout au long de cet ouvrage mais également au-delà si vous étiez amené.e à devoir réaliser des analyses qui ne sont pas couvertes par le contenu de ce livre. Enfin, nous terminerons en présentant quelques outils vous permettant de préparer vos données. 2.2 Qu’est-ce R et pourquoi l’utiliser ? Sorti pour la première fois en 1995, R est un langage de programmation ouvert, issu de langage S, spécialisé dans le traitement des données. Á l’heure actuelle, il s’agit d’un des logiciels les plus utilisés dans le monde académique. D’après Choueiry (Choueiry, 2021), non seulement R arrive en seconde position pour traiter les données dans le domaine de la recherche en santé, mais arrive quasiment à égalité de la première place lorsqu’on s’intéresse aux articles publiés dans les meilleures revues (si on considère que le facteur d’impact d’une revue informe d’une manière ou d’une autre sur la qualité des articles qui y sont publiés). De plus, R fait partie des langages de programmation les plus demandés dans les offres d’emploi de l’IEEE (https://www.facebook.com/48576411181), Á l’heure actuelle, de nombreux mastodontes de l’informatique, en particulier celles qui utilisent l’intelligence artificielle, comme google et Facebook, l’utilisent. Etant donné que R a pour vocation première le traitement des données statistiques, il est légitime de se demander ce qui le rend aussi populaire et pourquoi vous devriez utiliser R. Utiliser R, c’est avant tout s’engager vis-à-vis d’un système de valeurs dans lequel on considère que l’accès au savoir et à la science doit être ouvert et transparent, que l’évolution des connaissances doit être un processus collaboratif dont le but est de faire progresser l’Humanité dans son ensemble. Peut-être pensez-vous que ce projet est utopique, mais de plus en plus de personnes, et en particuliers de plus en plus de scientifiques ont la conviction qu’une science ouverte et transparente est l’avenir de la science. La place que prend R dans la publication dans les revues les plus prestigieuses, tel que nous venons de l’évoquer, n’est pas un phénomène irrationnel, mais est le fruit d’un double processus qui interagit : d’une part, les lignes éditoriales de ces revues imposent aux auteurs plus de transparence à la fois sur les données, mais également sur la manière dont elles ont été traitées et R est un outil parfaitement adapté pour répondre à ce critère ; d’autre part, les chercheurs qui adoptent les pratiques de la science ouverte vont avoir plus de citations, plus d’attention des médias, plus de collaborateurs potentiels, plus d’opportunités d’emploi et plus de facilités pour obtenir des financements (McKiernan et al., 2016). Pour la seconde raison, la question de la poule et de l’oeuf se pose : est-ce les meilleurs chercheurs qui favorisent la science ouverte ou est-ce le fait de favoriser la science ouverte qui rend un chercheur meilleur? Sans doute un peu les deux. Utiliser R est un pas vers cet objectif de science ouverte car R est un logiciel ouvert. Cela signifie qu’il est gratuit (ce qui est une bonne nouvelle pour vous), que vous pouvez analyser le code utilisé pour programmer les fonctions que vous allez utiliser, et vous pourrez également, si vous le souhaitez, contribuer à son développement. En d’autres termes, R est collaboratif, et ce processus collaboratif n’est pas restreint aux contributions que chacun peut faire. Le côté collaboratif de R se manifeste également par la communauté d’entraide qui existe sur les forums en ligne et par le fait que chaque utilisateur a la possibilité d’informer un des contributeurs s’il y a une erreur ou un bug dans son code. Pour comprendre en quoi cette manière de penser est avantageuse, prenons le temps de nous créer une représentation mentale de la manière dont vos logiciels commerciaux préférés fonctionnent et comparons ce fonctionnement avec celui de R. Pour le logiciel que vous avez à l’esprit, connaissez-vous les auteurs du logiciel et leurs compétences ? Probablement que non. Dans le cas de R, il y a des milliers de contributeurs, certains étant des anonymes qui veulent apporter leur pierre à l’édifice, mais d’autres sont d’éminents spécialistes, de domaines parfois très spécialisés. On conçoit donc aisément que, si la qualité des contributeurs est plus inégale lorsqu’on utilise R, vous pourrez cependant bénéficier des outils les plus performants pour traiter une problématique et ce en étant bien guidé et en cherchant les fonctions les plus recommandées. Cela se fait notamment grâce aux communautés actives sur le web et à la liste des packages que le noyau dur des programmeurs de R met en avant. Rappelons que ce seront des spécialistes du domaine qui auront développé la boîte à outil permettant de répondre spécifiquement à votre question. Utiliser R en ligne de commande offre donc une flexibilité qu’aucun logiciel sous la forme d’interface graphique ne peut proposer. Ainsi, si on reprend notre comparaison avec un logiciel commercial, on imagine aisément qu’une équipe restreinte d’une entreprise commerciale, aussi bonne que soit cette équipe, ne peut se tenir à la pointe de tous les domaines pour pouvoir rivaliser avec une communauté de scientifiques spécialistes de leur domaine. Imaginons à présent que le logiciel commercial que vous utilisez a un bug, que faites-vous ? Probablement, rien. Au mieux, vous enverrez un courriel au SAV de la société qui commercialise le logiciel qui, avec un peu de chance, prendra vos remarques en compte pour améliorer la version suivante du logiciel, version suivante que vous serez amenés à acheter et qui sortira plus ou moins tardivement par rapport au moment où le bug a été identifié. R fonctionne sur un autre modèle : vous pouvez contacter directement le contributeur de la fonction pour laquelle vous avez identifié un souci. Ce dernier identifiera l’origine du problème et le corrigera en quelques jours. Croyez-en mon expérience, les auteurs des packages fiables sont extrêmement réactifs. Les avantages de R ne se limitent pas à être gratuit. En effet, R vous permet également d’être transparent et reproductible, car vous pouvez aisément partager le code qui a servi à analyser les données. Cela a deux avantages. Le premier, et pas le moindre, est que cela permet à d’autres personnes d’avoir accès et donc de réutiliser la méthode d’analyse qui a été utilisée dans un article donné. Ces personnes pourront alors reproduire cette méthode pour l’appliquer à un autre article. Dans cette section, on va vous présenter des lignes de commandes que vous réutiliserez pour vos propres données. Ce chapitre a été écrit en utilisant directement R. Ainsi, je suis sûr que les lignes de commandes que je vais vous présenter vont fonctionner en l’utilisant exactement de la manière dont elle est présentée. Les erreurs de copier/coller sont donc évitées. Le second avantage est que tout le monde peut vérifier qu’il n’y a pas d’erreur dans le traitement des données qui ont été publiées. Ces erreurs peuvent se manifester à la fois dans la manière de mener les analyses, mais également dans le report des indices. En effet, pour les articles publiés entre 1985 et 2013 dans 8 grosses revues de psychologie, Nuijten et ses collaborateurs (Nuijten et al., 2016) ont identifié que la moitié des articles présentaient des inconsistances entre la valeur de la probabilité reportée au regard de la valeur de la statistique et des degrés de libertés 1 qui leur étaient associés.Les erreurs peuvent arriver. Il est tout en fait acceptable d’en commettre mais il est moins acceptable de ne pas permettre aux autres de vérifier qu’il n’y en ait pas. Il faut se rappeler que la recherche en France, mais également dans beaucoup de pays du monde, est financée, au moins en partie, par des fonds publics. Á partir du moment où de l’argent public est dépensé, il y a une nécessité de transparence à la fois sur la manière dont cet argent a été dépensé, mais également sur le fruit de ces dépenses. Ainsi, la valorisation des recherches, soit les financeurs (les contribuables lorsque la recherche est financée sur les fonds publics), doivent non seulement pouvoir accéder aux résultats de la recherche, mais également à toutes les étapes qui ont permis d’atteindre ces résultats, y compris le traitement des données. R permet cette transparence puisqu’on peut partager les données ainsi que le code utilisé et qu’il n’est pas nécessaire d’avoir une licence d’un logiciel payant pour vérifier que tout est correct. Au-delà de ces avantages, en termes de fonctionnalités, R est le champion incontestable en offrant bien plus de fonctionnalités que les logiciels payants, avec plus de 150 fois plus de fonctions que SAS (Muenchen, 2015), en faisant ainsi un outil extrêmement puissant et polyvalent pour l’analyse de données. On peut ajouter aux qualités de R qu’il est compatible avec tous les systèmes d’exploitation (Windows, MacOS, Linux) et qu’il existe des serveurs en ligne qui vous permettent d’utiliser R sans devoir l’installer sur votre ordinateur. Cela peut être utile lorsqu’on a peu de place sur le disque dur ou qu’on veut travailler de manière collaborative. Certains aficionados vous diront même que R va vous aider à mieux comprendre les statistiques. Je ne partage pas cet avis : un logiciel en soi n’aide pas à comprendre ou à ne pas comprendre. En revanche, la personne qui vous explique les statistiques peut utiliser R pour faire des simulations ou encore l’utiliser comme outil pédagogique pour l’enseignement des statistiques, ce qui ne peut pas être fait dans d’autres logiciels statistiques où on ne peut faire que du click. Par exemple, Snow (2024) a développé toute une série d’outils permettant d’expliquer des concepts fondamentaux des statistiques. Par ailleurs, utiliser R requiert de prendre des décisions statistiques et de comprendre ce que l’on fait sinon c’est le message d’erreur assuré. Ainsi, si vous ne comprenez pas les statistiques, R n’a pas le pouvoir magique de vous faire comprendre, mais si vous comprenez les statistiques, l’utilisation de R vous permettra d’aller encore plus loin. Cependant, que vous compreniez ou non les statistiques, utiliser R de manière efficiente nécessite du temps, car il faudra apprendre à utiliser des lignes de commandes, lignes de commandes qui ont des arguments. Cet apprentissage est assez ténu dans un premier temps mais l’équipe qui développe R a fait un effort important pour harmoniser la manière d’utiliser les fonctions. Et si l’apprentissage est lent dans un premier temps, par la suite, vous serez beaucoup plus rapide pour toute une série de tâches, telles que les prétraitements de vos données. Par ailleurs, les outils d’intelligence artificiel ont largement progressé pour vous aider à utiliser les lignes de commande. Il n’est pas possible de ne s’appuyer que ces outils sans comprendre ce qu’on fait, mais cela fait gagner un temps considérable quand on a les bons réflexes et les bonnes bases. Enfin, utiliser R a un intérêt indirect : l’utilisation des lignes de commande est un enseignement sans compromis à la rigueur. Comme évoqué juste avant, nous commettons tous des erreurs, quand on en commet en utilisant des lignes de commande, le résultat est sans appel : un message d’erreur. Le fait d’être exposé à ce message d’erreur n’est pas anodin, il souligne le nombre de fois que nous sommes susceptibles de nous tromper, indépendamment des statistiques, et de prendre conscience de la nécessité d’être exigeant avec nous-mêmes, avec nos productions lorsque nous rédigeons un document afin d’éviter au maximum ces erreurs car la science, au travers des articles scientifiques, a pour vocation d’être exempte autant que peu se faire d’erreurs. Si vous vous dites que vous ne voulez pas utiliser R car vous recevez constamment des messages d’erreur, est-ce que vous êtes disposés à être critiqués, comme c’est le cas lorsque nous sommes relus par des coauteurs ou dans le processus d’expertise, et à élever votre niveau pour éviter de commettre les mêmes erreurs à l’avenir. La vertu pédagogique de cet apprentissage est sans doute inégalable pour quiconque envisage une carrière dans la recherche, et pour les autres, cela apprend l’humilité face à nos certitudes, et notre niveau de compétence. R, dans sa forme brute, n’est pas très conviviale. C’est pourquoi nous utiliserons Rstudio, également gratuit, qui est une interface graphique de R et un éditeur de code. 2.3 Qu’est-ce que easieR et pourquoi l’utiliser 2.3.1 Rationnel d’easieR D’un point de vue pédagogique, utiliser R en ligne de commande requiert de faire le deuil concernant l’autonomie des étudiant·es dans le traitement des données. En effet, il n’est pas pédagogiquement possible, quand le volume horaire est restreint pour enseigner les statistiques, de les aborder de manière suffisamment approfondie pour assurer l’autonomie dans le traitement des données tout en assurant l’accompagnement indispensable aux étudiant·es qui se trouvent dans la situation où ils et elles utilisent pour la première fois des lignes de commande, à savoir faire face aux messages d’erreurs. Ces messages d’erreurs pouvaient également être source de frustration et de découragement pour les étudiant·es, en particulier celles et ceux qui ont une vision de l’intelligence en tant qu’entité (Dweck, 2006) C’est dans ce contexte que easieR (Stefaniak, 2018) a commencé être développé, à cette époque où JASP (2025) n’était qu’à ses prémices et Jamovi (2025) n’existait pas encore. Si ces deux derniers logiciels se sont imposés dans de nombreux cours de statistiques des programmes de psychologie en France et à l’étranger, easieR se démarque dans sa conception à plusieurs égards expliquant pourquoi il continue à se justifier par rapport à ces alternatives. 2.3.2 Philosophie sous-tendant easieR Le package easieR (Stefaniak, 2018) s’articule autour de 3 piliers philosphiques (voir Figure 2.1 : une démarche science ouverte ; un pipeline complet et cohérent dans la présentation des analyses ; des outils permettant de répondre aux besoins les plus fréquents des étudiant·es et des chercheur·euses en psychologie ; une structure pédagogique inhérente à sa conception. Figure 2.1: Piliers sur lesquels easieR s’appuie. 2.3.2.1 Une démarche science ouverte Le package easieR s’inscrit dans une démarche sciences ouvertes de deux manières principales: Permettre d’adopter une approche multiverse de l’analyse des données (Steegen et al., 2016) ; Permettre d’utiliser des scripts reproductibles. En ce qui concerne l’approche multiverse de l’analyse des données, il s’agit d’analyser les données de différentes manières afin de s’assurer de la robustesse des résultats. Si l’interprétation des résultats diffère selon la manière dont l’analyse a été réalisée, il semble indispensable de s’interroger sur ce que veulent vraiment dire les données. Cette approche est le contre-pied du p-hacking puisque les auteurs sont invités à présenter les résultats (par exemple en annexe) des différentes méthodes d’analyses qui peuvent être envisagées pour tester l’hypothèse et de montrer la cohérence entre ces différentes manières d’analyser les données et les résultats présentés. Quant au fait d’être reproductible et de faciliter la transition entre les outils statistiques à base de boîtes de dialogues vers ceux à base de les lignes de commande, cela est possible car easieR (Stefaniak, 2018) peut être utilisé tant en boîte de dialogue (ce qui est fait avec les étudiant·es) qu’en ligne de commande. Cette fonctionnalité représente l’originalité de easieR en tout qu’outil statistiques. En effet, si de plus en plus de revues scientifiques demandent de partager les codes des analyses, s’approprier un langage informatique est souvent compliqué. C’est le cas pour R (R Core Team, 2025). Ainsi, confrontés aux erreurs répétées que l’on peut commettre en débutant à utiliser R (R Core Team, 2025) en ligne de commande, les apprenants peuvent considérer leur courbe d’apprentissage particulièrement lente, ce qui affectera de manière négative leur motivation. Dans le package easieR (Stefaniak, 2018), les erreurs ne sont pas sanctionnés par un message d’erreur mais par une boîte de dialogue qui prend le relais. Ainsi, même en cas d’erreur, l’analyse peut être réalisée, et surtout en comparant la ligne de commande générée automatiquement avec celle initialement utilisée afin d’identifier les causes qui ont empêché les lignes de commande d’être exécutée correctement. Par ailleurs, comme easieR est directement utilisé dans l’interface R et permet dès lors sensibiliser les utilisateur·ices à des notions comme le chargement d’une bibliothèque ou l’utilisation d’une fonction, tout en se familiarisant avec l’environnement. Enfin, il est possible de générer un rapport des résultats en html ou en word, ce qui facilite les copiés collers et éviter les erreurs de reports. 2.3.2.2 Une conception orientée vers la pédagogie Souvent les outils statistiques proposent une multitude d’options, mais les utilisateurs ne porte que très peu leur intérêt sur ces options car ces options ne font pas partie de leurs connaissances statistiques. La conception de easieR est fondamentalement différente à bien des égards : easieR est une suite de boîte de dialogue qui force à prendre une décision. Par défaut, la meilleure option (une des options parmi les meilleurs options possibles) statistiques est choisie. Néanmoins, il est possible d’envisager une alternative. Cette décision requiert alors une décision éclairée ; afin d’accompagner les utilisateurs à prendre leurs décisions statistiques, des informations sont affichées dans la console en parallèle de l’apparition des boîtes de dialogue de sorte à aiguiller la décision des néophytes ; lorsque la meilleure option statistique est peu connue par la plupart des personnes, les résultats plus habituels sont présentés à côté ou au-dessus de la solution optimale, ce qui facilite la comparaison et permet d’être initié à ces options alternatives, meilleures mais moins connues. Par exemple, pour une analyse factorielle, les corrélations polychoriques devraient être préférées pour des données ordinales et les corrélations tétrachoriques pour des données dichotomiques. Ce choix est celui par défaut lorsque le nombre de modalités est inférieur à 8 (pour les corrélations polychoriques) ou sont égales à 2 (pour les corrélations tétrachoriques). 2.3.2.3 Un pipeline cohérent dans la sortie des résultats La philosophie inhérente à easieR (Stefaniak, 2018) est également quelque peu différente. En effet, contrairement à la plupart des packages de statistiques, la vérification des conditions d’application n’est pas une option et il n’est pas possible de l’éviter, tout comme il n’est pas possible d’ignorer les statistiques descriptives. Ces deux prérequis semblent être la base d’une analyse statistique critique puisqu’il n’est pas possible de donner sens aux résultats si les conditions d’applications des analyses ne sont pas respectées (vu que l’estimation des probabilités de dépassement sont inhérentes au respect de ces conditions d’application). De même, si on ne regarde pas les statistiques descriptives, il n’est pas possible de s’assurer que les données correspondent aux données censées être analysées : est-ce que l’effectif est correct ? Est-ce que les valeurs minimales et maximales pour les variables quantitatives sont compatibles avec la mesure ? Ainsi, l’utilisateur d’easieR (Stefaniak, 2018) est guidé dans une démarche systématique d’analyse des données qui ne se limite pas à vérifier si la probabilité de dépassement est inférieure à 0.05. Afin de favoriser l’adopter de cette démarche systématique, toutes les analyses sont structurées selon un même schéma (voir Figure 2.2). Figure 2.2: Structure de l’analyse des données avec easieR. L’analyse commence systématiquement par la présentation des statistiques descriptives avec pour objectif : - d’identifier si on travaille sur le bon nombre d’observations ; - s’assurer que les minimums et maximums sont compatiables avec les échelles qui ont été utilisées (par ex., on ne peut pas avoir un QI de 245) ; - identifier les tendances d’un point de vue descriptif ; Ensuite, les conditions d’application sont systématiquement testées de sorte à ce que, en cas de violation de ces conditions d’application, l’utilisateur·trice soit amené·e à utiliser une analyse alternative ou à corriger ces violations quand c’est possible. L’objectif est également de représenter un signal d’alarme sur le fait que les conclusions perdent en robustesse dès lors que ces conditions ne sont pas respectées. La présentation de l’analyse principale avec les tailles d’effet et les intervalles de confiance afin de comprendre que l’essence des statistiques n’est pas d’avoir une valeur p inférieur à 0.05, mais d’avoir une taille d’effet qui fasse sens. La présentation des alternatives non paramétriques et/ou robustes, à savoir des bootsrap, des M-estimator ou encore des anovas sur les médianes, afin d’adopter une démarche multiverse (Steegen et al., 2016), consistant à s’assurer que les conclusions qui sont tirées ne dépendent pas de la manière dont les données ont été analysées mais aboutissent à la même conclusion, qu’importe l’outil statistique utilisé. L’identification de valeurs influentes qui pourraient avoir un impact sur les résultats et, le échéant, une réanalyse sans ces valeurs influentes afin de rester dans cette continuité de démarche multiverse d’analyse des données. Á ma connaissance, easieR est le seul outil utilisant des boîtes de dialogue qui intègre directement cette possibilité. 2.3.2.4 Répondre aux besoins les plus courants des utilisateurs Les fonctions principales de easieR sont résumées dans le tableau 2.1 .cl-1a8507ac{}.cl-1a724afe{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-1a78a6ba{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1a78e850{width:2.247in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e85a{width:3.267in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e864{width:8.31in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e865{width:6.281in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e86e{width:2.247in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e86f{width:3.267in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e878{width:8.31in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e879{width:6.281in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e882{width:2.247in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e883{width:3.267in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e884{width:8.31in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e88c{width:6.281in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e88d{width:2.247in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e896{width:3.267in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e8a0{width:8.31in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e8aa{width:6.281in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e8ab{width:2.247in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e8ac{width:3.267in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e8b4{width:8.31in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e8be{width:6.281in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e8bf{width:2.247in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e8c0{width:3.267in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e8d2{width:8.31in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e8d3{width:6.281in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e8dc{width:2.247in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e8dd{width:3.267in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e8e6{width:8.31in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e8e7{width:6.281in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e8f0{width:2.247in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e8f1{width:3.267in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e8f2{width:8.31in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e8fa{width:6.281in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e8fb{width:2.247in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e8fc{width:3.267in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e904{width:8.31in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e90e{width:6.281in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e90f{width:2.247in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e918{width:3.267in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e919{width:8.31in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e922{width:6.281in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e923{width:2.247in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e924{width:3.267in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e92c{width:8.31in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e92d{width:6.281in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e936{width:2.247in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e937{width:3.267in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e940{width:8.31in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e941{width:6.281in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e94a{width:2.247in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e94b{width:3.267in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e954{width:8.31in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1a78e955{width:6.281in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 2.1: Tableau synthétique reprenant les principales fonctions d'easieR Choix dans le menu principalFonction principaleAnalyse alternativeL'option utileDonnéesImporter (en différents formats)Rxporter (en différents formats)VisualiserGénérer un rapport de résultatsPréparation des donnéesFilrer des observationsSélectionner des variablesTransformer en rangsImputation de valeurs manquantesCentrer réduireTrierRéaliser des opérations mathématiquesPasser d'un format large au format longGraphiquesBoxplotNuage des pointsDensitéViolin plotHistogrammesAnalysesChi carré d'ajustementsimulation de Monte CarloChi carré d'indépendancesimulation de Monte Carlo, correction de continuitéContrastes pour identifier les cases qui s'écartent significativementTest de McNemarCorrection de continuitét de Student comparaison à une normetest de Wilcoxon, M-estimator,bootstrapt de Student pour échantillons appariésTest de Wilcoxon, M estimator, bootstrapGraphique selon les recommandations de Loftust de Student pour échantillons indépendantsMann-Whitney, correction de Satterthwaite, t sur les médianes, t sur les moyennes tronquées (avec ou sans bootstrap)ANOVA simple à groupes indépendantsKruskall-Wallis, correction de Welch, anova sur les médianes ou moyennes tronquéesTous les contrastes sont possiblesANOVA à mesure répétéesANOVA de Friedman, correction de Greenhouse et Geisser, anova sur les médianesTous les contrastes sont possiblesANOVA factoriellesanova sur les médianesTous les contrastes sont possiblesANOVA mixtesanova sur les médianesTous les contrastes sont possiblesAnalyse de covarianceTous les contrastes sont possiblesRégressionsBoostrap,M-estimatorEffets de modérations et effets non linéairesCoefficient de concordanceAlpha de CronbachOmega de McDonaldCoefficient de corrélation intraclasseCoefficient de corrélations de PearsonRho de Spearman, tau de Kendall, boostrapAnalyse par groupe analyse complète ou matrice de corrélation (rectangulaire ou carrée)Analyse factorielle exploratoireEstimateurs robustesUtilisation des corrélations tétrachoriques/polychoriquesAnalyse factorielle confirmatoireEstimateurs robustesUtilisation des corrélations tétrachoriques/polychoriquesAnalyse en composantes principalesEstimateurs robustesUtilisation des corrélations tétrachoriques/polychoriques Références Choueiry. (2021). Statistical Software Popularity in 40,582 Research Papers &amp;#x2013; QUANTIFYING HEALTH — quantifyinghealth.com. https://quantifyinghealth.com/statistical-software-popularity-in-research/. Dweck, C. S. (2006). Mindset. Random House. https://www.facebook.com/48576411181. The 2016 Top Programming Languages — spectrum.ieee.org. https://spectrum.ieee.org/the-2016-top-programming-languages. JASP Team. (2025). Jeffrey’s Amazing Statistics Program, JASP (Version 0.19.3)[Computer software]. https://jasp-stats.org/ McKiernan, E. C., Bourne, P. E., Brown, C. T., Buck, S., Kenall, A., Lin, J., McDougall, D., Nosek, B. A., Ram, K., Soderberg, C. K., Spies, J. R., Thaney, K., Updegrove, A., Woo, K. H., &amp; Yarkoni, T. (2016). How open science helps researchers succeed. Elife, 5. Muenchen, B. (2015). R now contains 150 times as many commands as SAS. R-bloggers. https://www.r-bloggers.com/2015/05/r-now-contains-150-times-as-many-commands-as-sas/ Nuijten, M. B., Hartgerink, C. H. J., Assen, M. A. L. M. van, Epskamp, S., &amp; Wicherts, J. M. (2016). The prevalence of statistical reporting errors in psychology (1985–2013). Behavioral Research Methods, 48(4), 1205–1226. R Core Team. (2025). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/ Snow, G. (2024). TeachingDemos: Demonstrations for teaching and learning. https://doi.org/10.32614/CRAN.package.TeachingDemos Steegen, S., Tuerlinckx, F., Gelman, A., &amp; Vanpaemel, W. (2016). Increasing transparency through a multiverse analysis. Perspectives on Psychological Science, 11(5), 702–712. https://doi.org/10.1177/1745691616658637 Stefaniak, N. (2018). easieR: easieR: A GUI r metapackage. https://github.com/NicolasStefaniak/easieR The jamovi project. (2025). Jamovi (version 2.4). https://www.jamovi.org. si ces notions vous sont inconnues, elles sont expliquées dans les sections consacrées aux principes fondamentaux des statistiques↩︎ "],["installer-le-logiciel-de-traitement-de-données.html", "Chapter 3 Installer le logiciel de traitement de données 3.1 Installer R 3.2 Je suis un utilisateur windows 3.3 Je suis un utilisateur apple 3.4 Je suis un utilisateur Linux ou Chromebook 3.5 Installation de RStudio 3.6 Installation de easieR", " Chapter 3 Installer le logiciel de traitement de données 3.1 Installer R La première étape consiste en l’installation de R. Le logiciel est en libre téléchargement à l’adresse suivante : https://cran.r-project.org/ où vous trouverez le lien de téléchargement pour les différents systèmes d’exploitation. 3.2 Je suis un utilisateur windows Pour les utilisateurs windows, il faut se rendre sur http://cran.r-project.org/bin/windows/base/ Une fois sur cette page, il faut cliquer « download R » dont le numéro de version évolue constamment. Comme le fichier téléchargé est un fichier « exe », il faut double-cliquer sur le fichier. Il suffit ensuite de cliquer constamment sur suivant jusqu’à ce que le logiciel soit installé. 3.3 Je suis un utilisateur apple Pour les utilisateurs MacOS, il faut cliquer sur ce lien Hyper important : vérifiez le type de puce sur votre ordinateur en cliquant sur la pomme en haut à gauche et puis “à propos de ce mac”. Si, à côté de puce, vous avez un nom du type M1, M2 ou M3, alors, vous devez télécharger et installé la version arm de R. Si, à côté de puce, vous avez un nom qui contient intel, vous devez télécharger, la version x86 de R. Important : si votre système d’exploitation (OS) est inférieur à Big Sur, vous ne pouvez pas installer la dernière version de R. Vous devez trouver une version de R compatible avec votre système d’exploitation à l’adresse suivante (voir https://cran.r-project.org/). Néanmoins, vous vous exposez à ce que une ancienne version de R ne soit pas compatible avec des packages que vous utiliserez. L’alternative moins risquée est d’utiliser la même procédure que celle préconisée pour les utilisateurs Linux et Chromebook Une fois téléchargé, il faut double-cliquer sur le fichier « pkg », Il suffit ensuite de cliquer sur suivant jusqu’à ce que le logiciel soit installé. Pour finaliser l’installation, vous devez encore installer XQuartz disponible à l’adresse suivante : https://www.xquartz.org/ 3.4 Je suis un utilisateur Linux ou Chromebook Si, théoriquement, il est possible de faire l’installation (du moins sur les ordinateurs ayant un Linux comme système d’exploitation), n’étant pas expert de ces systèmes d’exploitation, La solution la plus simple est de créer un compte (gratuit) sur Rstudio cloud (https://login.rstudio.cloud). Cela évite toutes les difficultés d’installation. Dans votre cas, aucune installation n’est requise. Notez que ce document est écrit à partir d’une version bureau (donc sur un ordinateur personnel) de RStudio et qu’il peut y avoir des petites divergences entre la version bureau et la version cloud. 3.5 Installation de RStudio Pour aucun utilisateur, il n’est indispensable d’installer Rstudio. Néanmoins, pour les utilisateurs de R en ligne de commande, l’installation de Rstudio peut représenter un environnement plus simple d’utiliation. RStudio n’est rien d’autre qu’une interface graphique qui va rendre l’utilisation de R un peu plus agréable. R studio propose également quelques fonctionnalités d’importation, de navigation entre les graphiques et d’accès aux objets stockés dans la mémoire de R. Son utilisation n’est pas obligatoire mais est très vivement recommandée dès lors qu’on veut travailler avec des scripts reproductibles. Rstudio est disponible gratuitement en téléchargement à l’adresse suivante : https://rstudio.com/products/rstudio/download/ Qu’importe votre système d’exploitation, la procédure est la même : on clique sur le lien de téléchargement. Une fois téléchargé, on double-clique sur le fichier et sur suivant jusqu’à ce que Rstudio soit installé. 3.6 Installation de easieR eaieR est une interface graphique qui peut être directement utilisée dans R et il n’est pas indispensable d’avoir installé Rstudio. La procédure pour installer easieR est la suivante : vous avez téléchargé et installé R2 en fonction de votre système d’exploitation. Pour les utilisateur·ices macOS, n’oubliez pas d’installer XQuartz. Télécharger et installer Pandoc vous ouvrez R et dans la fenêtre, vous copier coller le code ci-dessous : install.packages(&quot;devtools&quot;) devtools::install_github(&quot;nicolasstefaniak/easieR&quot;, type=&quot;binary&quot;) Si vous recevez un message indiquant que certains packages peuvent être mis à jour, choisissez l’option 2 (qu’il faut taper dans la console), CRAN only. Il est important de noter que, quand trop d’utilisateurs se connectent en même temps à github, github peut empêcher l’installation car il y a trop de requêtes en même temps. Il faut alors utiliser la solution alternative, ci-dessous. Solution alternative # 1. Télécharger le zip depuis GitHub url &lt;- &quot;https://github.com/NicolasStefaniak/easieR/archive/refs/heads/master.zip&quot; temp_zip &lt;- tempfile(fileext = &quot;.zip&quot;) download.file(url, destfile = temp_zip, mode = &quot;wb&quot;) # 2. Dézipper dans un dossier temporaire temp_dir &lt;- tempdir() unzip(temp_zip, exdir = temp_dir) # 3. Identifier le dossier du package package_dir &lt;- file.path(temp_dir, &quot;easieR-master&quot;) # 4. Installer le package devtools::install(package_dir, subdir = subdir) # 5. Nettoyer (optionnel) file.remove(temp_zip) 3.6.1 easieR ne s’installe pas correctement Si vous êtes sur windows, le problème le plus fréquent est que votre compte windows est associé à des caractères spéciaux, tels que “à”,“ç”,“é”,“è”,“ê”… Par exemple, votre compte s’appelle élève. La solution la plus simple est de créer une nouvelle session sur votre ordinateur en passant par : - panneau de configuration - comptes d’utilisateurs - créer un nouveau compte Pensez à accorder les droits administrateurs à ce compte pour pouvoir installer tout correctement. Un autre problème que vous pourriez avoir est de manquer d’espace sur votre disque dur. La solution à ce problème est plus technique et nécessite d’avoir un support externe pour stocker les packages. Voici la procédure : créer un dossier sur le support externe où les packages seront stockés (imaginons que le chemin d’accès est D:/packages); créer un script R que vous enregistrerez dans document avec le nom .Rprofile (exactement tel que je l’ai écrit) dans demandez à R où sont stockés les packages avec la fonction .libPaths() .libPaths() Dans le document .Rprofile créé, ajouter les deux chemins d’accès suivants (le premier vous est fourni par .libPaths() et le second est celui que vous avez créé) ; .libPaths(c(&quot;C:/Program Files/R/R-4.5.2/library&quot;, #chemin existant préalablement &quot;D:/packages&quot;))# nouveau chemin pour les packages Reprendre l’installation de easieR à install_github Enfin, pour tous les utilisateur·ices, un des problèmes les plus fréquents est que tous les logiciels n’ont pas été installé. Vous pouvez vérifier que pandoc a été installé correctement par : rmarkdown::pandoc_version() ## [1] &#39;3.6.3&#39; Vous avez que pandoc est installé si vous avez une autre valeur que ‘0’. Remarque : il est possible que vous ayez installé pandoc mais que R ne le prenne pas en compte parce que vous avez installé pandoc après avoir ouvert R. Il suffit de fermer R, de le rouvrir et de revérifier. Pour les utiliteurs macOS, il faut aussi s’assurer que XQuartz est correctement installé. Pour cela, allez dans votre répertoire des applications et dans le dossier Utilitaires (le nom peut potentiellement changer en fonction de la version de l’OS). Si vous ne vous XQuartz ni dans vos applications, ni dans aucun des sous-dossiers, c’est que ce n’est pas installé. pour les utilisateurs apple, vous pouvez vérifier que c’est installé en cliquant sur finder et puis Applications↩︎ "],["débuter-avec-son-logiciel-de-statistiques.html", "Chapter 4 Débuter avec son logiciel de statistiques 4.1 Débuter avec R 4.2 Débuter avec easieR", " Chapter 4 Débuter avec son logiciel de statistiques 4.1 Débuter avec R 4.1.1 L’environnement R dans RStudio La Figure 4.2 représente la fenêtre qui s’ouvre lorsque vous lancez RStudio, Figure 4.1: Fenêtre d’accueil dans RStudio. Trois parties apparaissent : la console (grosse flèche noire) ; l’environnement (l’encadré jaune) ; et les graphiques (l’encadré vert). Il est possible de taper des lignes de commandes directement dans la console. Ainsi, vous pourriez par exemple utiliser la console comme calculatrice. Il va de soi que R offre bien plus de possibilités que de l’utiliser comme calculatrice. Se servir de R en l’utilisant comme une calculatrice reviendrait approximativement à acheter le dernier smartphone haut de gamme pour s’en servir comme réveil. Néanmoins, quand on débute, il est pédagogiquement nécessaire de comprendre comment R fonctionne et ces exemples simplistes permettent d’y contribuer. Ainsi, si vous tapez dans la console 5+3 et que vous appuyez sur la touche ‘entrée’ de votre clavier, voici ce que vous allez obtenir : 5+3 ## [1] 8 4.1.2 Le script Remarquez que nous avons utilisé R de manière triviale en demandant de réaliser une opération arithmétique simple. Cependant, quand le traitement des données va commencer à se complexifier et qu’il faudra utiliser plusieurs fonctions (la notion de fonction sera explicitée un peu plus loin) pour obtenir les résultats souhaités, vous conviendrez que cette pratique sera loin d’être optimale. Tout d’abord, ce ne sera pas pratique de devoir retrouver, chaque fois que vous en aurez besoin, les fonctions qui vous seront utiles et les réécrire. En d’autres termes, la console ne permet pas de garder aisément une trace des opérations qui ont été faites. Or, il semble évident qu’il est utile d’avoir une trace de ce que vous faites, avec éventuellement la possibilité d’y faire vos propres annotations pour vous aider à vous rappeler la manière dont vous devez utiliser la fonction ou à quoi elle sert. Par ailleurs,pour aller retrouver un résultat que vous avez obtenu précédemment, naviguer dans la console sera fastidieux. Enfin, si vous vous trompez et qu’un message d’erreur apparaît, il n’est pas commode d’utiliser la console pour la corriger. C’est pour cette raison qu’utiliser R directement dans la console est à proscrire. Votre courbe d’apprentissage serait bien plus lente et la démotivation vous gagnerait rapidement. Une manière bien plus adaptée d’utiliser R dans RStudio est d’utiliser un script. Pour cela, il faut cliquer sur la feuille blanche entourée d’un cercle rouge sur la Figure 4.2, et choisir dans le menu déroulant ‘R script’. Vous obtenez alors une nouvelle fenêtre (voir Figure 4.3) : Figure 4.2: Procédure de création d’un script dans R. Figure 4.3: Procédure de création d’un script dans R. Nous allons donc commencer par donner un titre à notre script. Toute ligne ou tout information précédée par un dièse (#) est interprété comme du commentaire. Nous pourrions donc donner comme nom au script “Introduction à R”. En l’entourant de dièse, il ressortira et permettra d’identifier rapidement de quoi traite le script. ################################### ### Introduction à R #### ################################### Alternativement, vous pouvez créer des chapitres dans Rstudio en cliquant sur Code et ensuite sur Rstudio. En donnant un nom à la section, vous pourrez par la suite naviguer aisément entre les sections de votre script en cliquant sur code et ensuite sur jump to. Vous l’aurez compris, vous devriez faire un script pour chaque chapitre ou faire un seul script. Sachant que les scripts peuvent être aussi long que vous le souhaitez, il est théoriquement possible de ne faire qu’un seul et unique script avec toutes les thématiques. Néanmoins, il vous sera plus facile de faire et d’utiliser des scripts plus courts qui ne traitent que d’une thématique. Á présent que nous avons une section, nous allons pouvoir découvrir les opérations de base qu’on fait habituellement avec une calculatrice. 5+3 # Addition 5-3 # Soustraction 5*3 # multiplication 5/3 # division 5^3 # exposant. Donc, ici, 5 exposant 3 Dans le script, contrairement à ce qui se passe avec la console, rien ne se passe lorsque vous écrivez une ligne de commande. La raison est que la console interprète directement la ligne de commande quand on appuie sur entrée alors que le script ne le fait pas. La ligne de commande sera exécutée uniquement quand vous en ferez la demande explicite. Pour cela, il y a deux possibilités : soit vous sélectionnez une ligne (ou un ensemble de lignes à devoir exécuter), et vous cliquez sur ‘Run’ en haut à droite de la fenêtre du script ; soit vous sélectionnez une ligne (ou un ensemble de lignes à devoir exécuter), et vous appuyez sur les touches ctrl et entrée (pomme et entrée pour les mac users) de votre clavier. Dans les deux cas, les résultats vont s’afficher dans la console et vous obtiendrez ceci : ## [1] 8 ## [1] 2 ## [1] 15 ## [1] 1.666667 ## [1] 125 Avant de continuer, pensez à enregisgtrer votre script. Quand vous cliquez sur la petite disquette, RStudio vous propose de choisir un type d’encodage. Vous pouvez laisser la valeur par défaut, ou éventuellement choisir “UTF-8” (voir Encadré 1 pour plus de détails). Le type d’encodage des scripts Sur MacOS et sur Linux, “UTF-8” est la valeur par défaut. Pour les utilisateurs windows, la valeur par défaut est ‘ISO-8859-1’. Concrètement, cela n’a pas de réel impact tant que vous n’utilisez pas de caractères spéciaux, c’est-à-dire et sans être exhaustif l’un des caractères suivants : é,è, ê, à, ç. En revanche, dès qu’il y a des caractères spéciaux, les choses vont se compliquer. Donc si vous commentez en anglais, pas de souci d’utiliser l’un ou l’autre ; en français, cela peut poser souci. Tant que vous êtes le seul utilisateur du fichier, il sera lisible. En revanche, si vous souhaitez partager votre script, les caractères spéciaux ne seront pas lisibles si l’encodage n’est pas bon. Bref, si un utilisateur MacOS travaille avec un utilisateur Windows, il est préférable d’utiliser “UTF-8” qui est considéré comme un format universel. Si vous ouvrez un script où certains caractères ne sont pas lisibles, vous pouvez le rendre lisible en demandant à Rstudio de rouvrir le script avec le bon encodage. Pour cela, on clique sur ‘file’, ensuite sur ‘reopen with encoding’ et on choisit l’encodage. Á présent que vous avez choisi l’encodage, vous pouvez donner un nom à votre script. Pour pouvoir le réutiliser par la suite, il faut que l’extension du fichier, c’est-à-dire ce qui vient après le dernier point soit un R majuscule. Ainsi, j’ai donné le nom suivant à mon fichier : Introduction_R.R. 4.1.3 Créer des sections et annoter votre script Peut-être avez-vous remarqué que j’ai mis à côté de chaque opération un dièse avec une information textuelle. Cela m’a permis d’expliquer à quoi correspondait chacune des lignes de commande. Cette opération peut se faire en annotant à côté de la ligne de commande ou au-dessus de celle-ci (il est théoriquement possible d’annoter en-dessous mais ce n’est pas une procédure conventionnelle). # Addition 5+3 # Soustraction 5-3 # multiplication 5*3 # division 5/3 # exposant. Donc, ici, 5 exposant 3 5^3 Même si l’utilité d’une fonction, ou son utilisation vous semble triviale, prenez le réflexe d’annoter votre script. Les annotations servent à vous rappeler à quoi sert une fonction, à expliquer à d’autres personnes (y compris à vous-même plus tard) ce que vous avez fait et à indiquer les difficultés que vous avez rencontrées avec la fonction et la manière d’y faire face. Á présent que nous avons succinctement abordé l’utilisation de R comme s’il s’agissait d’une calculatrice, nous pourrions vouloir commencer à utiliser R de manière un peu plus élaborée. On va donc aborder une nouvelle thématique. Vous pourrez (et c’est recommandé) créer des délimitations dans votre script afin d’indiquer ces changements de thématique. Ces délimitations s’appellent des sections. Vous pourriez ainsi utiliser des sections pour identifier chacun des sous-chapitres. Pour créer une section, le plus simple est de cliquer sur ‘Code’ en haut de la fenêtre, et ensuite sur ‘insert section’. L’alternative est de créer la section en appuyant simultanément sur les touches ‘Ctrl’, majuscule et ‘R’. Ainsi, en étant au-dessus des lignes de commandes relatives aux opérations mathématiques, vous pourriez créer une première section intitulée ’utiliser R comme une calculatrice et vous verriez cette ligne apparaître dans votre script (voir Figure 4.4). Il faut noter que vous pouvez directement recopier cette ligne et adapter le titre pour créer de nouvelles sections. # Utiliser R comme une calculatrice --------------------------------------- Figure 4.4: Créer une section intitulée : utiliser R comme calculatrice Naviguer entre les sections de votre script. Astuce : si vous cliquez à présent sur ‘Code’ en haut de votre écran et ensuite sur ‘Jump to’, vous allez voir les différentes sections de votre script et vous pourrez naviguer aisément entre ces différentes parties. 4.1.4 Les fonctions Les fonctions représentent des lignes de commande qui permettent de réaliser des tâches particulières. Par exemple, il y a une fonction pour calculer une moyenne, il y a une fonction aussi pour compter le nombre d’observations. En réalité, il y a des milliers de fonctions dans R. Dans cette section, nous allons décrire quelques fonctions de manière succinctes, dont certaines feront l’objet d’un approfondissement dans les chapitres suivants. D’un point de vue formel, une fonction a un nom. Par exemple, la fonction pour calculer une moyenne est mean. Le nom de la fonction est suivi de parenthèses. Á l’intérieur de ces parenthèses, vous allez spécifier ce qu’on appelle les arguments. Si je reviens à la fonction qui permet de calculer la moyenne, un des arguments de la fonction consiste à fournir les valeurs numériques sur lesquelles la moyenne doit être calculée. Commençons avec deux fonctions assez simples car elles n’ont pas d’argument, la fonction getwd permet de savoir dans quel répertoire on est en train de travailler, et la fonction dir qui permet d’obtenir la liste des fichiers disponibles dans ce répertoire. Pour aborder ces fonctions dans le script, nous allons commencer par créer une nouvelle section que nous allons intituler Les fonctions de base. # Les fonctions de base --------------------------------------- Nous pouvons à présent utiliser ces deux fonctions sans argument : getwd() # permet d&#39;obtenir le répertoire de travail dir() # permet d&#39;obtenir la liste des fichiers disponibles dans le répertoire Par défaut, R va utiliser le dossier ‘documents’ de votre ordinateur comme répertoire de travail, dont le chemin d’accès prendra approximativement la forme de C:/Users/votre_nom/Documents. Ce répertoire de travail n’est sans doute pas le répertoire dans lequel sont localisées vos données, ni celui où vous souhaiteriez travailler. Avoir comme répertoire de travail le répertoire dans lequel se situe l’ensemble des documents est une règle de bonne pratique. Vous pourriez, par exemple, créer dans vos documents un dossier intitulé Statistiques3 et à l’intérieur de ce dossier, un sous-dossier appelé “Introduction” qui contiendra le script pour ce chapitre et les données qui seront utilisées ultérieurement. Il faut à présent préciser à R que votre répertoire de travail ce situe par exemple dans C:/Users/votre_nom/Documents/Statistiques/Introduction. Pour cela, nous allons utiliser la fonction setwd. Cette fonction a un argument. Cet argument (code&gt;dir pour directory en anglais) est le répertoire dans lequel vous souhaitez travailler # Ligne de commande permettant de préciser le répertoire de travail setwd(dir=&quot;C:/Users/XXX/Documents/livre de statistiques/Introduction&quot;) # Attention que &quot;XXX&quot; renvoie à votre nom et que vous devez adapter ce chemin d&#39;accès à votre ordinateur Vous venez d’essayer de changer le répertoire de travail et vous avez reçu un message d’erreur qui prend la forme suivante : Error in setwd(&quot;C:/Users/XXX/Documents/Statistiques/Introduction&quot;) : cannot change working directory Vous avez probablement commis une ou plusieurs des erreurs suivantes : Le dossier n’existe pas. Vous avez oublié de créer le dossier dans lequel vous voulez travailler. Le dossier doit déjà exister sur votre ordinateur ; Le chemin n’est pas correct. Vous avez fait une faute de frappe, ou oublié un sous-répertoire dans le chemin d’accès ; Vous n’avez pas respecté la casse. Comme pour les mots de passe, R est sensible à la casse, c’est-à-dire au respect des majuscules et des minuscules ; Vous êtes sur Windows et vous avez copier/coller le chemin d’accès depuis la fenêtre de navigation. Cependant, R souhaite le symbole slash pour accéder à un dossier enfant, c’est-à-dire un dossier imbriqué dans un autre dossier alors que windows utilise par défaut le backslash. Il faut donc inverser tous les backslash en slash. Travailler directement dans le bon environnement de travail. Il est possible de se trouver directement dans le bon environnement de travail sans à avoir à le préciser. Pour cela, il faut créer un projet. Dans Rstudio, il suffit de clquer sur File et ensuite New Project. Après l’avoir sauvegardé à l’endroit pertinent, il suffira d’ouvrir sur le fichier créer pour que le répertoire de travail soit automatiquement celui dans lequel le projet a été enregitré. Une partie de ces fonctions sont disponibles directement lorsqu’on installe R, mais la grande majorité de ces fonctions ont été programmées par les contributeurs qui ont été évoqués à la section expliquant pourquoi il est pertinent d’utiliser R. Ces contributeurs n’ajoutent pas directement les nouvelles fonctionnalités au logiciel mais vont empaqueter les fonctionnalités qu’ils ont programmées dans ce qu’on appelle un package. Ces packages devront être installés en plus pour pouvoir être utilisés. Si vous avez du mal à comprendre ce qu’est un package, pensez à votre smartphone : quand vous l’allumez pour la première fois, il n’y a que les fonctionnalités de base et vous allez installer les applications selon vos besoins. R fonctionne sur un principe un peu similaire, à la différence qu’on ajoute ces nouvelles fonctionnalités à l’intérieur d’un logiciel. Dans un premier temps, nous allons utiliser le package readxl (Wickham &amp; Bryan, 2022) pour importer des fichiers excel, et le package dplyr (Wickham et al., 2022) pour avoir des fonctionnalités de manipulation des données. Nous allons donc installer et charger ces packages. Pour installer un package, on utilise la fonction install.packages. Cette fonction a pas mal d’arguments mais il ne semble pas utile de les expliciter tous en détail. Nous nous limiterons à installer un package de manière classique en invoquant simplement le nom du package à installer. install.packages(pkgs = &quot;readxl&quot;) # installation du package readxl - en utilisant l&#39;argument pkgs install.packages(&quot;dplyr&quot;) # installation du package dplyr - en omettant l&#39;argument pkgs Vous savez que les packages sont installés correctement par le fait que R vous indique que le package a été décompressé avec succès et que la somme des MD5 a été vérifiée (package ‘readxl’ successfully unpacked and MD5 sums checked). Il arrive néanmoins que certains packages ne s’installent pas aussi aisément. Nous renvoyons le lecteur vers la section “installer les packages” pour une description plus détaillée de cette fonction et les difficultés qui peuvent être associées à l’installation de certains packages. Remarquez qu’il est possible d’utiliser une fonction avec des arguments sans invoquer l’argument de manière explicite. Dans la fonction install.packages, le premier argument s’appelle ‘pkgs’. Il s’agit du nom du package à installer. Si vous respectez l’ordre des arguments, il n’est pas indispensable de les appeler explicitement (comme je l’ai fait pour l’installation du package dplyr) mais si vous ne respectez pas l’ordre ou si vous omettez des arguments intermédiaires pour lesquels vous souhaitez garder les valeurs par défaut, alors il faut les appeler explicitement. En l’occurrence, l’argument pkgs est le premier argument et je peux donc me permettre d’omettre de le spécifier quand j’utilise la fonction. En revanche, c’est une pratique qu’il est préférable d’utiliser uniquement avec les fonctions qu’on connait parfaitement car cela pourrait entraîner des messages d’erreur ou des résultats qui ne correspondent pas à ce qui était souhaité. Á présent que nos deux packages sont bien installés, nous pouvons les charger pour pouvoir utiliser leurs fonctionnalités ultérieurement. On les charge avec la fonction library en indiquant le nom du package sans les guillemets. library(readxl) # chargement du package readxl library(dplyr) # chargement du package dplyr Charger les packages dont on va avoir besoin en début de script représente également une règle de bonne pratique. Trouver de l’aide sur une fonction. Comme évoqué précédemment, il existe une large communauté d’utilisateurs de R qui s’entraident en ligne. Néanmoins, si vous connaissez la fonction que vous devez utiliser, la stratégie de première intention que vous devriez utiliser pour comprendre comment utiliser une fonction est de consulter l’aide qui lui est associée. En effet, cela fait partie du cahier de charge de R de fournir une documentation détaillée quant à l’utilisation des fonctions, avec une explication de chacun des arguments et des exemples sur la manière d’utiliser la fonction. Pour accéder à cette aide, il existe deux manières : utiliser le point d’interrogation suivi du nom de la fonction ou utiliser la fonction help avec le nom de la fonction entre parenthèse. ?setwd # recherche de l&#39;aide pour la fonction setwd en utilisant le point d&#39;interrogation help(install.packages) # recherche de l&#39;aide pour la fonction setwd en utilisant la fonction help Cette manière de trouver de l’aide sur l’utilisation d’une fonction est une stratégie indispensable car, pour l’utilisateur débutant, l’utilisation des fonctions n’est pas suffisamment automatique et l’aide permet de fournir les indices indispensables pour savoir comment utiliser une fonction et, pour l’utilisateur expert, il se rappellera du nom d’une fonction qu’il n’utilise pas souvent mais pas forcément du nom des arguments ou de la manière de les utiliser. En revanche, cette stratégie ne fonctionne que si la fonction est accessible dans R. Vous ne pourriez pas trouver de l’aide de cette manière pour une fonction qui est dans un package qui n’est pas installé ou qui n’est pas chargé. Pour trouver de l’aide sur une fonction d’un package installé mais pas chargé, on peut utiliser deux points d’interrogation successifs avant le nom de la fonction. R va alors chercher dans tous les packages qui sont installés s’il trouve une fonction qui contient le nom que vous rechercher. Á des fins d’illustrations, installez le package psych (Revelle, 2025) sans le charger et vous pourrez trouver de l’aide sur la fonction describe en utilisant les deux points d’interrogation de la manière suivante : ??describe # cherche l&#39;aide d&#39;une fonction appelée describe dans n&#39;importe quel package installé sur votre ordinateur, même s&#39;il n&#39;est pas chargé. 4.1.5 Les objets dans R Lorsque vous utilisez une des lignes de commande décrite ci-dessus, une fois le résultat obtenu, ce résultat est devenu inutilisable car les informations n’ont pas été stockées dans la mémoire de R. Or, il pourrait être utile de pouvoir réutiliser le résultat produit. Par exemple, lorsque vous utilisez la fonction dir, vous avez la liste des fichiers disponibles dans votre dossier. Imaginons que ces fichiers soient les résultats individuels de chacun de vos participants et que vous avez 200 participants. Cela pourrait être plus que fastidieux de devoir importer les données de chaque participant les uns après les autres. On voudrait pouvoir les importer en une seule fois. Cependant, si la liste des fichiers n’est plus utilisable, on ne peut pas le faire. Il est donc nécessaire de stocker ces informations dans la mémoire de R. On fait cela en créant des objets. Il existe trois manières de stocker un objet dans la mémoire de R : le symbole ‘&lt;’ et ‘-’ pour créer une flèche orientée vers la gauche ; le symbole ‘&gt;’ et ‘-’ pour créer une flèche orientée vers la droite ; le symbole ‘=’. Nous allons illustrer la création d’objets avec ces trois méthodes. Dans un premier temps, nous allons attribuer la valeur 5 à un objet appelé ‘a’. Je vais attribuer la valeur 3 à un objet que je vais appeler ‘b’ et enfin, l’objet ‘c’ sera l’addition entre a et b. a&lt;-5 # créer un objet appelé a et ayant comme valeur 5 3 -&gt;b # créer un objet appelé b et ayant comme valeur 3 c=a+b # créer un objet appelé c et ayant comme valeur la somme de a et b De ces 3 méthodes que nous avons évoquées pour créer des objets, l’utilisation du symbole ‘=’ est à déconseiller car il est préférable d’utiliser exclusivement cette procédure pour l’attribution des valeurs à un argument d’une fonction. Par exemple, lorsque nous avons utilisé la fonction setwd, il fallait préciser le répertoire de travail, qui est l’argument ‘dir’, et nous l’avons fait avec le symbole ‘=’. # Identifier ici l&#39;argument &#39;dir&#39; à l&#39;intérieur de la fonction et que le symbole utilisé pour lui attribuer une valeur est &#39;=&#39; setwd(dir=&quot;C:/Users/XXX/Documents/Statistiques/Introduction&quot;) L’utilisation des deux autres méthodes se valent mais préférez faire vos flèches toujours dans le même sens pour vous éviter des écueils. Je vous encourage à les faire toujours vers la gauche (comme pour le ‘a’ dans le code ci-dessus) afin d’avoir le nom des nouveaux objets en début de ligne à chaque fois. Quand vous ferez plusieurs opérations à la suite, cela vous évitera de passer du temps à chercher le nom que vous avez attribué à un objet 10 lignes de commandes plus haut. Si on exécute les trois lignes de commandes ci-dessus, apparemment rien ne se passe. Il n’y a rien qui s’affiche dans la console. Pour voir ce que contiennent les objets, il faut les appeler. Ainsi, en tapant leur nom et en cliquant sur ‘run’, vous allez voir ; a # appeler l&#39;objet a, renvoie 5 ## [1] 5 b # appeler l&#39;objet b, renvoie 3 ## [1] 3 c # appeler l&#39;objet c, renvoie 8 ## [1] 8 Le nom de ces objets est pratiquement complètement arbitraire et vous pourriez l’appeler ‘ficus’, ‘Xmen’, ‘HarryPotter’ si vous le souhaitiez. En revanche, si vous l’appelez ‘HarryPotter’, pour pouvoir l’utiliser, il faudra écrire ‘HarryPotter’. Il est donc préférable d’utiliser un nom court pour éviter de devoir retaper des noms interminables. # Illustration du côté arbitraire des noms ficus&lt;-5 Xmen&lt;-5 HarryPotter&lt;-5 ficus ## [1] 5 Xmen ## [1] 5 HarryPotter ## [1] 5 Et si vous faites une faute de frappe (y compris sur la casse4), alors R va renvoyer un message d’erreur indiquant qu’il ne trouve pas l’objet appelé (Error: object ‘Harrypotter’ not found). Harrypotter # Harrypotter avec un &quot;p&quot; minuscule n&#39;existe pas et renvoie un message d&#39;erreur Même si on peut utiliser pratiquement n’importe quel nom pour les objets, certaines règles doivent impérativement être respectées. D’abord, vous ne pouvez pas utiliser un nom qui contient des espaces. Dans ce cas, R va renvoyer un message d’erreur indiquant qu’il y a un symbole inattendu. Ce symbole est l’espace. En revanche, vous pouvez utiliser un underscore ou un point (mais il est en revanche prohibé d’utiliser une virgule, un point-virgule ou un deux points) pour faire vos séparations. Ma valeur cinq&lt;-5 # renvoie un message d&#39;erreur en raison des espaces Mavaleurcinq&lt;-5 # ne renvoie pas de message d&#39;erreur Ma_valeur_cinq&lt;-5 # ne renvoie pas de message d&#39;erreur quand on utilise un underscore Ma.valeur.cinq&lt;-5 # ne renvoie pas de message d&#39;erreur quand on utilise un point Ensuite, il faut éviter tous les caractères spéciaux. On va commencer par la situation la plus pernicieuse : les accents, les ‘ç’, et autres trémats. Je vais attribuer la valeur “Jean” à un objet appelé “élève” élève&lt;-&quot;Jean&quot; élève ## [1] &quot;Jean&quot; Là, tout va bien. Cependant, certaines fonctions ne vont pas reconnaître les accents et vous ne comprendrez pas pourquoi la fonction renvoie un message d’erreur alors que votre objet ne présente aucun problème en apparence (ici le souci est lié au type d’encodage des caractères spéciaux, dont nous avons parlé précédemment). Vous pourriez passer des heures à vous demander quel est le problème et vous finiriez par vous décourager pour un problème aussi trivial. La règle à appliquer est simple : n’utilisez jamais d’accent ou de caractères spéciaux. La dernière règle à appliquer est qu’il ne faut aucun symbole : pas de parenthèse, R considérerait ce qui précède comme une fonction ; pas d’apostrophe car ce serait interprété comme une chaîne de caractères ; pas de symbole mathématique car il voudrait réaliser une opération mathématique. Voici quelques exemples de ce qu’il ne faut pas faire : # Quelques exemples de ce qu&#39;il ne faut pas faire interprete(ma.fonction)&lt;- 5 # ne fonctionne pas et renvoie le message d&#39;erreur indiquant que cette fonction ne peut être trouvée c&#39;est_pas_terrible&lt;-5 # l&#39;apostrophe pose souci ceci+cela+non+plus&lt;-5 # le &#39;+&#39; est interprété comme une opération mathématique, de même s&#39;il y avait &#39;-&#39;,&#39;/&#39;,&#39;*&#39;,&#39;^&#39;. pas_d_egal=erreur&lt;-5 # mettre un &quot;=&quot; pose souci car on crée deux objets différents : pas_d_egal et erreur Remarquez également que, dans votre environnement global (l’encadré jaune de la Figure 4.2), vous avez à présent plusieurs objets que vous pouvez réutiliser pour des traitements ultérieurs. Si pour des opérations aussi triviales que celle qu’on vient de voir, cela a peu d’intérêt, n’oubliez pas que l’objectif sera in fine de pouvoir utiliser des modèles statistiques sur des jeux de données réels et qu’il sera important de pouvoir passer par des étapes intermédiaires. En l’occurrence, il y a peu d’objets dans la mémoire de R. Cependant, il peut y en avoir beaucoup plus et vous pourriez vouloir en obtenir la liste. Pour cela, on utilise la fonction ls sans devoir préciser d’argument. ls() # connaître la liste des objets dans la mémoire de R. ## [1] &quot;a&quot; &quot;arrows&quot; &quot;b&quot; &quot;boxes&quot; ## [5] &quot;c&quot; &quot;cards&quot; &quot;condition_label&quot; &quot;easier&quot; ## [9] &quot;élève&quot; &quot;ficus&quot; &quot;ft&quot; &quot;HarryPotter&quot; ## [13] &quot;lines&quot; &quot;tot&quot; &quot;tot2&quot; &quot;Xmen&quot; 4.1.6 Les erreurs les plus courantes et les règles de bonnes pratiques Le plus difficile quand on débute avec R est d’être confronté à un message d’erreur et de ne pouvoir lui attribuer du sens. Il va de soi qu’il n’est pas possible de faire une liste exhaustive des erreurs possibles mais voici les plus fréquentes, et leurs solutions : .cl-1b56bcde{}.cl-1b4a29ce{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-1b4e74e8{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1b4ea47c{width:4.901in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1b4ea486{width:18.838in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1b4ea490{width:4.901in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1b4ea491{width:18.838in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1b4ea49a{width:4.901in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1b4ea49b{width:18.838in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 4.1: Liste des erreurs les plus fréquentes et leur cause/solution ErreurSignification et solutionunexpected symbolLe nom de l'objet ou de la variable est incorrect. Vérifiez les règles énoncées précédemmentobject XXX not foundL'objet n'a pas été créé ou est mal orthographie. Attention à la casse - càd. aux majuscules et minusculescould not find function XXXLa fonction est mal orthographiée ou est dans un package non chargé, voire non installé. Cette erreur est courante quand on utilise une fonction qu'on trouve sur le internet sans vérifier le package auquel elle appartient. Chargez le package contenant la fonction si elle existe.Error in cor.test.default(c(\"a\", \"b\")) : x doit être un vecteur numériqueLes arguments doivent avoir certaines caractéristiques. Ici, ils doivent être numériques mais ce n'est pas le cas. Vérifiez avec la fonction class ou str si les objets sur lesquels vous souhaitez faire l'analyse ont le format correctargument is of length zeroUn argument indispensable a pour valeur NULL alors qu'il doit être spécifié. Vérifiez l'aide de la fonctionno default for argumentUn argument nécessaire n'a pas été spécifié. Vérifiez l'aide de la fonctionaucun package nommé XXX n'est trouvéLe package n'est pas installé ou est mal installé. Si vous avez tenté d'installer le package mais que le souci persiste, voir la section installer les packages récalcitrants ens section 6. Néanmoins, en appliquant les règles de bonnes pratiques suivantes, vous diminuerez considérablement les erreurs : Utiliser un script pour toutes les fonctions que vous utilisez. Avant de commencer les analyses, pensez à charger les packages et vérifier qu’il n’y a pas de problème concernant leur chargement5. Annotez votre script pour savoir ce que vous faites, mais également pour expliquer les erreurs auxquelles vous avez été confrontées et comment cela a été résolu. Faites attention à la casse6. Utilisez des noms simples et pour les noms complexes, favorisez les copier coller en utilisant la fonction ls pour avoir le nom des objets en mémoire et la fonction names pour avoir le nom des variables. Par exemple : data(mtcars) names(mtcars) ## [1] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; &quot;gear&quot; ## [11] &quot;carb&quot; Enfin, les outils d’intelligence artificiel représente des aides précieuses pour vous aider à identifier pourquoi une erreur est survenue et pour vous aider à corriger une ligne de commande. Le danger est de trop se reposer sur l’IA sans comprendre ce qu’on fait. Dans ce genre de situation, il peut arriver que l’IA ne vous fournisse pas une solution adaptée, même avec un bon prompt, et vous ne pourrez pas régler la difficulté à laquelle vous êtes confronté·e. Il est donc indispensable de comprendre en premier et de profiter de l’aide de l’IA dans un second temps. 4.1.7 Installer des packages récalcitrants Précédemment, nous avons décrit comment installer des packages. Néanmoins, certains peuvent être plus difficiles à installer que d’autres pour différentes raisons. Voici les solutions à envisager pour installer ces packages. Pour l’illustration, nous utiliserons le package ‘devtools’ (Wickham et al., 2025). Ainsi, quand un package ne veux pas se charger, c’est qu’il n’est pas correctement installé. La procédure ci-dessus explique comment forcer cette installation lorsque les choses ne se passent pas correctement. Chaque étape peut se suffire à elle-même. Néanmoins, si l’installation continue à poser problème, alors, il faut passer à l’étape suivante. Pour le formuler autrement : je tente l’étape 1, ça marche, je m’arrête ; ou, je tente l’étape 1, ça ne marche pas, je passe à l’étape 2. Forcer l’installation des dépendances, le site miroir et les packages compilés. Quand nous avons décrit la fonction install.packages, elle a été présentée de sorte à pouvoir l’utiliser sans argument. On peut néanmoins préciser des arguments supplémentaires. Le premier argument est l’argument dependencies qui permet de forcer l’installation des dépendances en indiquant l’opérateur logique TRUE. Le second argument qu’on va utiliser est l’argument type pour forcer à installer les packages sous une forme compilée (binaire) plutôt que sous une forme où le package devra être compilé (source). Ainsi, cela évite de devoir avoir un logiciel de compilation, comme Rtools. Enfin, on va préciser le site miroir sur lequel on veut télécharger les packages qu’on souhaite installer. Dans cet exemple, il s’agit du CRAN de Lyon. install.packages(&quot;devtools&quot;, # nom du package dependencies = TRUE, # force l&#39;installation des dépendances type= &quot;binary&quot;, # on installe uniquement les packages sous un format compilé repos = &quot;https://mirror.ibcp.fr/pub/CRAN&quot;) # le site miroir CRAN est celui de Lyon Fermer R et refaire l’étape 1. Il arrive que certaines dépendances soient chargées, empêchant dès lors leur mise à jour et l’installation du nouveau package. Le fait de fermer R permet de régler le souci dans la plupart des situations. Vérifier l’espace sur le disque dur. Il arrive parfois que les packages ne s’installent pas parce que l’espace sur le disque dur est insuffisant. S’il est suffisant, passez à l’étape 4. S’il est insuffisant, il existe deux solutions, une assez technique qui consiste à choisir comme répertoire d’installation des packages un support externe. Cela implique de forcer R à aller chercher les packages dans ce dossier. L’autre solution est de travailler en ligne en créant un compte Rstudio cloud. Le nom d’accès du dossier d’installation des packages contient des caractères spéciaux. Á nouveau, il existe deux solutions, forcer l’installation des packages dans un autre dossier (mais cela implique de préciser le chemin de ce dossier) ou créer un compte en ligne. 4.1.8 Pour conclure L’objectif de ce chapitre était de fournir les bases indispensables à l’utilisation de R, comprendre ce qu’est un object, pouvoir utiliser une fonction et adopter les bonnes pratiques pour éviter les erreurs. Á présent qu’on s’est dégagé des contraintes relatives à l’utilisation du logiciel, il est possible à présent de s’intéresser au raisonnement statistique et de pouvoir réaliser les analyses adaptées. 4.2 Débuter avec easieR Comme évoqué dans la section consacrée au choix des logiciels, le package easieR (Stefaniak, 2018) peut être utilisé de différentes manières. En particulier, on peut l’utiliser en boîte de dialogue uniquement, en ligne de commande uniquement, ou un mixte entre les deux. Dans cette section, nous fournissons les informations pour l’utiliser en boîte de dialogue uniquement dans sa forme la plus générale. Ainsi, pour utiliser easieR, il faut charger la package avec la fonction library et on peut accéder à toutes les fonctions de easieR avec la fonction easieR(). library(easieR) #charge le package # ouvre la boîte de dialogue permettant d&#39;accéder à toutes les options easieR() Références Revelle, W. (2025). Psych: Procedures for psychological, psychometric, and personality research. Northwestern University. https://CRAN.R-project.org/package=psych Stefaniak, N. (2018). easieR: easieR: A GUI r metapackage. https://github.com/NicolasStefaniak/easieR Wickham, H., &amp; Bryan, J. (2022). Readxl: Read excel files. https://CRAN.R-project.org/package=readxl Wickham, H., François, R., Henry, L., &amp; Müller, K. (2022). Dplyr: A grammar of data manipulation. https://CRAN.R-project.org/package=dplyr Wickham, H., Hester, J., Chang, W., &amp; Bryan, J. (2025). Devtools: Tools to make developing r packages easier. https://doi.org/10.32614/CRAN.package.devtools ou si vous êtes plus enthousiaste, Vive les statistiques↩︎ c’est-à-dire le respect des majuscules et des minuscules↩︎ s’il y a un problème, un message d’erreur s’affichera.↩︎ les majuscules et les minuscules↩︎ "],["les-données.html", "Chapter 5 Les données 5.1 Les variables 5.2 Les objets et leur nature dans R 5.3 La nature des variables avec easieR", " Chapter 5 Les données Les statistiques n’ont d’intérêt qu’à deux conditions principales. Premièrement, il faut que ce qui est mesuré ait une issue incertaine. Cela implique qu’il faut mesurer quelque chose ayant un caractère aléatoire (par exemple, on ne connait pas à l’avance la couleur des cheveux de la prochaine personne qu’on va croiser dans la rue) afin de tester une hypothèse dont l’issue est incertaine (par exemple, on peut faire l’hypothèse qu’il y a plus de personnes aux cheveux clairs dans le nord de l’Europe que dans le sud de l’Europe, mais san l’avoir mesuré, on ne peut pas en avoir la certitude). Cette mesure aléatoire est ce qu’on appelle une variable. La seconde condition est de réaliser les analyses sur des données de qualité. Dès lors qu’il y a des erreurs dans les données, les analyses statistiques perdent toute leur utilité. Il est donc nécessaire de préparer les données avec soin, tant lorsqu’on les encodage dans le jeu de données, que quand on les manipule. Cependant, même si on applique le plus grand soin dans la création du jeu de données, des erreurs peuvent s’immiscer. Il est bon de mettre en place des procédure de vérification pour éviter ces erreurs. Par exemple, si on utilise une échelle dont les valeurs vont de 45 à 155, alors, on ne peut pas avoir une valeur à 255. On peut donc vérifier que les données sont compatibles avec ce qui a été mesuré. Cette procédure peut sembler longue et fastidieuse alors qu’on voudrait savoir si nos résultats étayent ou non les hypothèses. Cependant, si on considère le temps qui a été investi dans la lecture de la littérature pour émettre une hypothèse, le temps pour construire le protocole expérimental, le temps pour recueillir les données, il semble flagrant que réaliser des analyses sur des données qui n’ont pas été vérifiées est faire bien mauvais usage de tout ce temps investi. 5.1 Les variables Quand on veut réaliser des statistiques, il est nécessaire de manipuler et/ou de mesurer quelque chose de variable. Si nous mesurons à moult reprises la même valeur, on n’a pas besoin de statistiques car nous sommes face à des certitudes. Par exemple, il n’est pas nécessaire de faire des statistiques sur le fait de savoir si un·e étudiant·e peut réussir sans venir en cours, sans ouvrir ses cours, sans se renseigner sur ses cours, le tout en allant à l’ensemble de ses examens sans avoir de quoi écrire. Ainsi, puisque pour faire des statistiques, il faut manipuler et/ou mesurer quelque chose qui varie, ce qu’on appelle une variable, il s’ensuit que, pour comprendre les statistiques, il est indispensable de pouvoir distinguer toute une série d’informations concernant ces variables. Plus précisément, il faut pouvoir distinguer les variables dépendantes des variables indépendantes et pouvoir distinguer la nature des variables. Pour rappel, une variable indépendante est tout ce qu’un·e expérimentateur·rice manipule est une variable indépendante, tandis que tout ce qu’un·e expérimentateur·rice enregistre est une variable dépendante. Maîtriser cette distinction est un prérequis nécessaire pas suffisant. On peut rajouter à cette distinction toutes les variables dont on veut s’assurer qu’elles ne peuvent être une explication plausible aux résultats qui ont été obtenus. Nous appellerons ces variables les variables de contrôle (ou controlées). Enfin, il existe une catégorie de variables qui sont loin d’être négligeables dans le cadre des statistiques mais dont la compréhension est plus subtile : il s’agit des variables aléatoires. Quand un chercheur·se recrute des participant·es pour prendre part à leur étude, iel n’a pas volontairement choisi ces participant·es. Les personnes ont été recrutées parce qu’elles étaient là à ce moment, mais cela aurait pu être n’importe qui d’autre si l’expérience avait été réalisée deux semaines plus tard ou 50 km plus loin. De la même manière, si on s’intéresse à la vitesse de lecture pour des textes écrits après 1900 et avant 1900, on va choisir un ou plusieurs textes avant 1900 et un ou plusieurs textes après 1900, mais les textes choisis en tant que tels ont été choisis au hasard. Á l’inverse, si ce qui nous intéresse est la vitesse de lecture pour le premier chapitre des Misérables de Victor Hugo par rapport au premier chapitre du Seigneur des Anneaux de Tolkien, les textes ne sont plus choisis au hasard et ne sont donc pas des variables aléatoires. Si cette catégorie de variables vous semble difficile à saisir, vous pouvez, pour le moment, l’ignorer car, dans la plupart des cas, ces variables ne sont pas modélisées de manière explicite. Néanmoins, si elles sont abordées ici, c’est parce que, pour certaines analyses, comme les modèles linéaires mixtes, il est nécessaire de les formaliser. Il est également nécessaire de pouvoir distinguer une variable quantitative (appelée aussi numérique ou métrique), d’une variable ordinale, ou qualitative (appelée aussi nominale ou catégorielle). Ne pas confondre variables et modalités Il apparaît que beaucoup d’étudiant·es confondent la variable avec les modalités avec ses modalités. Il est également indispensable de correctement distinguer les deux. La variable a un nom qui est un résumé abstrait de l’ensemble des modalités que la variable recouvre. Par exemple, le genre est une variable qui a comme modalités : femme, homme, non binaire, autre. Ainsi, le nom de la variable doit englober l’ensemble des modalités. 5.1.1 Les variables qualitatives Les variables qualitatives (ou catégorielles) sont caractérisée par des étiquettes, telle que le genre, la couleur des cheveux ou l’appartenance à un groupe… Elles ne sont pas dotées d’une structure particulière. Les différentes étiquettes permettent simplement de distinguer les individus les uns pdes autres sur une caractéristique précise. Elles sont caractérisées par la propriété d’exclusivité, c’est-à-dire qu’appartenir à la catégorie des hommes implique qu’on n’appartient pas à la catégorie des femmes, non binaire ou autre, et par la propriété d’exhaustivité, c’est-à-dire que les modalités doivent pouvoir prendre en compte l’ensemble des cas de figures possibles. Dans la réalité, ces notions d’exhaustivité et d’exclusivité sont parfois compromises. Par exemple, vous voulez ranger des personnes en fonction de leur couleur de cheveux, certains ayant des cheveux bruns, d’autres des cheveux gris. Imaginons à présent une personne dont les cheveux sont bruns mais avec de nombreuses mèches grises (le fameux poivre et sel). Allez-vous le ranger dans la catégorie des cheveux bruns ou dans celle des cheveux gris ? 5.1.2 Les variables ordinales Les variables ordinales sont des variables pour lesquelles il est possible d’attribuer un ordre entre les différentes modalités. Classiquement, les échelles ordinales sont les échelles de Likert, qui peuvent prendre 3, 5, 7 ou parfois 9 modalités différentes. La particularité de ces échelles est que la différence observée entre deux modalités n’est pas représentative d’une différence de même ampleur entre deux autres modalités. Par exemple, si on vous pose la question « faites-vous du sport ? » et que vous devez répondre sur une échelle du type « moins d’une fois par mois – une fois par semaine – deux à trois fois par semaine – plus de trois fois par semaine ». Une personne qui répond « moins d’une fois par mois » ne fera pas trois fois moins de sport qu’une personne qui répond « deux à trois fois par semaine ». Il est à noter que les statistiques réalisées sur des variables ordinales se réalisent systématiquement sur le rang des différentes observations. Généralement, le nombre de modalités est limité, et dépasse rarement 9 modalités différentes . Il est aussi à noter que, quand les variables ont peu de modalités, en particulier quand il n’y en a que 2 ou 3, on a tendance à utiliser les propriétés qualitatives de la variables plutôt qu’ordinale. 5.1.3 Les échelles quantitatives Les échelles quantitatives (ou métrique ou continue ou numérique) sont des variables qui présentent, si pas une infinité, un très grand nombre de modalités. Il est théoriquement possible d’entre dans une distinction supplémentaire : les échelles de rapport et les échelles d’intervalle. Sans entrer dans les détails entre les échelles de rapport et les échelles d’intervalle, une des propriétés des échelles de rapport est que la différence et le rapport entre les différentes modalités ont du sens. Par exemple, si je mets deux poids de 5 kilos sur une extrémité d’une balance et un poids de 10 kg sur l’autre extrémité, la balance sera en équilibre7. 5.1.4 Type de variable et leurs propriétés Il est à noter que les échelles métriques ont toutes les propriétés des échelles ordinales et des échelles qualitatives et que les échelles ordinales possèdent toutes les propriétés des échelles qualitatives. Par contre, les variables qualitatives n’ont pas les propriétés des variables ordinales et les variables ordinales n’ont pas les propriétés des variables qunatitatives. Ces caractéristiques sont particulièrement importantes à partir du moment où, lorsqu’on réalise des statistiques, des opérations sur les variables peuvent avoir été réalisée, de sorte à ce la nature de ces variables a été modifiée. Dès lors, il est possible de réaliser toutes les analyses faites sur une variable catégorielle quand on a une variable métrique ; l’inverse n’est en revanche pas possible. Par exemple, si vous avez les poids de vos participants, vous pouvez traiter cette variable comme une variable quantitative ; vous pouvez également la traiter comme une variable ordinale en attribuant une étiquette aux personnes (par exemple : obèse, en surpoids, corpulence normale, mince, maigre ou anorexique) ; enfin, cette variable peut être transformée en une variable nominale (sans ordonnancement) en classant les individus comme présentant un trouble de la masse corporelle (incluant dès lors les obèses et les anorexiques) ou comme ne présentant pas de troubles de la masse corporelle. Lorsque vous devez choisir un test, il vous faut pouvoir identifier correctement la nature des variables. Si vous éprouvez certaines difficultés pour identifier la nature des variables que vous manipulez, deux règles vous permettront de vous aidez : Si vous hésitez entre une variable ordinale et une variable qualitative, considérez que la variable est qualitative puisque il est acceptable d’appliquer les tests destinés à traiter des variables qualitative sur des variables ordinales ; Si vous hésitez entre une variable ordinale et une variable quantitative, considérez le nombre de modalités différentes qui sont à votre disposition. Si votre variable présente au moins 8 modalités différentes, vous pourrez la considérer comme une variable quantitative. En-dessous de 8 modalités différentes, préférez considérer la variable comme étant ordinale8. 5.2 Les objets et leur nature dans R 5.2.1 La nature des variables et des objets dans R Le chiffre 5 et le mot “bonjour” ne sont pas de même nature et vous ne pourriez pas faire les mêmes traitements statistiques sur des valeurs textuelles que sur des valeurs numériques. Parmi les natures d’objet, celles qui sont particulièrement importantes sont les valeurs numériques, les valeurs entières, les chaînes de caractères, les valeurs logiques et les facteurs. Nous allons illustrer ces différentes natures d’informations et, grâce à la fonction class, vous pourrez identier la manière dont R interprète chacun des objets que vous allez créer. Commençons par la situation la plus simple : les logiques. Les logiques se résument à deux valeurs possible VRAI (TRUE en anglais) ou FAUX (FALSE en anglais). Remarquez que TRUE et FALSE sont en majuscule sans guillemets. logique&lt;-TRUE # l&#39;alternative est FALSE logique2&lt;-F # TRUE et FALSE peuvent être résumés par T et F class(logique) # indique que c&#39;est une valeur logique ## [1] &quot;logical&quot; class(logique2) # indique que c&#39;est une valeur logique ## [1] &quot;logical&quot; Ensuite, nous avons les valeurs numériques. Les valeurs numériques peuvent être des valeurs avec ou sans virgules. Lorsqu’on a un ensemble de nombres sans virgule, R va leur attribuer comme nature ‘entier’ (integer en anglais). La raison de cette distinction est que R peut communiquer avec d’autres langages de programmation, notamment C et fortran qui requièrent moins d’espace et vont plus vite pour réaliser des calculs sur les entiers que sur les numériques. Pour vous, cette distinction n’a pas d’importance, excepté de savoir que R traite les entiers et les numériques comme des nombres. numerique&lt;-2.3 # valeur numérique avec virgule. Remarquez que le séparateur de décimale est un point numerique.sans.virgule&lt;-5 # valeur numérique sans virgule. entier &lt;-as.integer(5) # valeur qui sera considérée comme un entier par R class(numerique) # considérée par R comme une valeur numérique. (numeric) ## [1] &quot;numeric&quot; class(numerique.sans.virgule)# considérée par R comme une valeur numérique. (numeric) ## [1] &quot;numeric&quot; class(entier)# considérée par R comme une chaîne un entier. (integer) ## [1] &quot;integer&quot; Enfin, la dernière nature que nous aborderons sont les valeurs textuelles, les chaînes de caractères. Dans R, elles sont toujours entre guillemets, simples ou double. chaine_textuelle1&lt;-&quot;hello world&quot; # remarquez la présence de guillemets doubles chaine_textuelle2&lt;-&#39;hello world&#39; # remarquez la présence de guillemets simples class(chaine_textuelle1) # considérée par R comme une chaîne de lettres. (character) ## [1] &quot;character&quot; class(chaine_textuelle2) # également considérée par R comme une chaîne de lettres. ## [1] &quot;character&quot; Ces chaînes de caractères peuvent être utilisés pour de l’analyse textuelle par exemple. Cependant, quand on veut comparer des conditions expérimentales, il faut indiquer à R qu’il s’agit d’un facteur. Comme pour la distinction entre un entier et un numérique, le facteur est plus gourmand en mémoire, raison pour laquelle R privilégie les chaîne de lettres. facteur&lt;-as.factor(&quot;hello world&quot; ) class(facteur) ## [1] &quot;factor&quot; Une règle d’or que vous devriez toujours appliquer est de vérifier si R interprète correctement vos données : est-ce qu’il ne considère des valeurs numériques comme du texte ou un facteur comme une chaîne de caractères. Lorsque ce n’est pas le cas, on peut changer la nature d’une information dans le format souhaité en le faisant précéder par “as.”. Dans les exemples ci-dessus, j’ai utilisé la fonction as.factor pour transformer une chaîne de caractères en facteur, et j’ai utilisé as.integer pour transformer une valeur numérique en entier. Il y a deux situations où il faut être particulièrement prudent : la transformation d’un facteur en valeur numérique et la transformation d’une valeur en logique. Commençons par la transformation d’une valeur considérée comme un facteur en valeur numérique. Si je reprends ma valeur numérique et que je la transforme en facteur, je n’ai pas de souci. numerique ## [1] 2.3 numerique.f&lt;-as.factor(numerique ) # transformation de la valeur numérique en facteur class(numerique.f) # R l&#39;interprète comme facteur ## [1] &quot;factor&quot; A présent, si j’essaie de transformer la valeur en numérique, la valeur n’est plus 2.3 mais 1 : as.numeric(numerique.f) # transformation d&#39;un facteur en numérique, renvoie ici la valeur de 1. ## [1] 1 La raison de ce phénomène est qu’un facteur peut être ordonné. Par exemple, les modalités “petit”, “moyen” et “grand” ont un ordre, et vous pourriez vouloir réaliser des analyses en fonction du rang en attribuant les valeurs 1, 2, et 3 à ces différentes modalités. En l’occurrence, ‘numerique.f’ est la première modalité et la transformation en numérique renvoie dans ce cas le chiffre 1. as.numeric(numerique.f) # transformation d&#39;un facteur en numérique, renvoie ici la valeur de 1. ## [1] 1 Pour pouvoir transformer une valeur considérée comme facteur en numérique, il est nécessaire de passer par une étape intermédiaire, qui consiste à transformer l’information en caractère avant de la transformer en numérique. numerique.f&lt;-as.character(numerique.f) # étape intermédiaire consistant à transformer le facteur en une chaîne de caractère as.numeric(numerique.f) # transformation de la chaîne de caractère en numérique, ce qui renvoie 2.3 ## [1] 2.3 La seconde situation qui mérite notre attention est la situation de la transformation en logique. Toute valeur numérique différente de 0 est considérée comme vraie, et le 0 est considéré comme faux. Pour le comprendre, il faut simplement comprendre que, par convention, faux vaut 0 en langage de programmation, et théoriquement vrai vaut 1. R étend cette logique à tous les nombres différents de 0. Pour les chaînes de texte, l’information n’est pas interprétable en termes de logique et R renvoie NA. On ne peut donc pas l’interpréter en tant que tel mais il est possible de transformer des valeurs textuelles en valeur logique en utilisant d’autres stratégies. as.logical(1) # une valeur numérique transformée en valeur logique vaut toujours vrai si elle est différente de 0 ## [1] TRUE as.logical(-1)# une valeur numérique transformée en valeur logique vaut toujours vrai si elle est différente de 0 ## [1] TRUE as.logical(0) # une valeur numérique transformée en valeur logique vaut toujours faux si elle est égale 0 ## [1] FALSE as.logical(&quot;hello world&quot;) # renvoie NA, pour not available car une valeur textuelle n&#39;est pas interprétable en termes logiques. ## [1] NA as.logical(facteur)# renvoie NA, pour not available car une valeur textuelle n&#39;est pas interprétable en termes logiques. ## [1] NA Les valeurs manquantes Les valeurs manquantes sont désignées dans R par NA, pour not available. Cette notion est essentielle car il faudra prendre régulièrement des décisions sur les valeurs manquantes. Il est donc important d’être capable de les identifier. 5.2.2 Les différents types d’objets Jusqu’à présent, nous avons traité d’objets pour lesquels il n’y avait qu’une information, un chiffre par exemple. Cependant, les traitements des données vont impliquer de manipuler plusieurs informations en même temps. La difficulté à laquelle vous allez être confronté.e est qu’il existe plusieurs types d’objets, qui ont chacun leurs propriétés et qu’il faut connaître pour pouvoir utiliser correctement R. Par exemple, pour calculer une moyenne, il vous faut un vecteur de valeurs numériques. Si vous ne savez pas ce qu’est un vecteur, vous ne pourrez pas utiliser la fonction. De même, si une fonction requiert une matrice, il faut savoir à quoi cela correspond. Les termes ‘matrices’ ou ‘vecteurs’ font généralement assez peur. Cependant,il s’agit de termes compliqués pour des notions qui peuvent être très largement simplifiées. Bien que ma description soit volontairement imprécise et incomplète, elle suffira largement à la compréhension de la suite. Pour en revenir à notre propos, il existe dans R une multitude de classes d’objets. Si ces différentes classes d’objet ont leur raison d’être, cela dépasse largement les objectifs d’un chapitre d’introduction à R. Nous focaliserons donc sur les manières les plus fréquentes pour stocker, manipuler et traiter un ensemble de données : les vecteurs, les matrices, les dataframes, les tibbles et les listes. Imaginons qu’au lieu de n’avoir qu’une seule valeur numérique, nous en ayant 10. Par exemple, les résultats à un examen de statistiques. Les données pourraient être 12, 13, 9, 8, 15, 18, 4, 11, 13. Nous voudrions pouvoir les regrouper dans un seul objet. Nous pouvons le faire en créant un vecteur. Un vecteur est un ensemble d’informations de même nature, c’est-à-dire seulement des numériques, seulement des logique ou encore seulement des chaînes de caractères, qui n’ont qu’une dimension. Cela signifie que vous pourriez organiser les informations dans une seule colonne OU dans une seule ligne d’un tableau. Dans R, on crée ce vecteur avec la fonction c où chaque élément du vecteur est séparé des autres par une virgule. notes&lt;-c(12, 13, 9, 8, 15, 18, 4, 11, 13) # Création d&#39;un vecteur avec 10 valeurs notes ## [1] 12 13 9 8 15 18 4 11 13 On identifie que stocker un ensemble d’éléments dans un seul objet plutôt que dans plusieurs présentent différents avantages. Premièrement, cela évite d’encombrer la mémoire de R avec une multitude d’objets qui risqueraient de vous perdre. Ensuite, et surtout, cela permet de réaliser des traitements sur ces objets. Par exemple, nous avons évoqué la fonction mean qui permet de calculer des moyennes. Cette fonction a comme argument un vecteur de valeurs numériques. Nous pouvons donc calculer la moyenne sur notre objet ‘notes’. mean(notes) # calcul de la moyenne des notes ## [1] 11.44444 On peut également créer des vecteurs sur des chaînes de caractères. Le point important ici est qu’il ne faut pas oublier de mettre les guillemets pour chaque élement du vecteur. mot_de_fin&lt;-c(&quot;Cordialement&quot;, &quot;respectueusement&quot;, &quot;amicalement&quot;, &quot;amitiés&quot;, &quot;bonne journée&quot;,&quot;bien à toi&quot;,&quot;bien à vous&quot;) mot_de_fin ## [1] &quot;Cordialement&quot; &quot;respectueusement&quot; &quot;amicalement&quot; &quot;amitiés&quot; ## [5] &quot;bonne journée&quot; &quot;bien à toi&quot; &quot;bien à vous&quot; Notez que si vous mélangez des informations de différentes natures, R va harmoniser la nature des informations en fonction de ce qui semble être le plus cohérent. Par exemple, si on mélange des lettres et des chiffres, il va considérer l’ensemble des éléments du vecteur comme étant des chaînes de caractères. c(&quot;a&quot;,1) # remarquez les guillemets dans la sortie de résultats autour du 1, indiquant qu&#39;il est interprété comme une chaîne de caractères. ## [1] &quot;a&quot; &quot;1&quot; Les matrices sont des tableaux de données dans lesquels il n’y a que des observations de même nature (que des chiffres, que des chaînes de lettres, que des logiques). Une matrice est une table à deux entrées, ayant donc des lignes et des colonnes. La matrice se différencie donc du vecteur par le fait que la matrice a plusieurs lignes ET plusieurs colonnes. Les matrices mériteraient un chapitre complet les concernant mais les compétences requises dépasseraient très largement le niveau d’introduction. .cl-1bcc5afc{}.cl-1bc0f7f2{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-1bc4f7da{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1bc5140e{width:0.472in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1bc51418{width:0.472in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1bc51419{width:0.472in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1bc51422{width:0.472in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 5.1: Exemple de matrice ayant 3 lignes et 3 colonnes. V1V2V3147258369 Générer automatiquement des valeurs. Il existe une multitude de fonctions dans R qui permettent de générer automatiquement des valeurs sans avoir à les répéter Parmi ces fonctions, on retrouve seq qui permet de générer une séquence (par exemple, répète 3 fois 4 et ensuite 3 fois 5), rep (qui permet de répéter une information), ou encore gl qui permet de générer un facteur. Lorsque les valeurs à générer est une séquence de chiffres, on peut utiliser les deux points. 1:5 # crée une séquence de chiffres de 1 à 5 ## [1] 1 2 3 4 5 rep(x=1, times = 9) # répète 9 fois le chiffre 1 ## [1] 1 1 1 1 1 1 1 1 1 seq(from = 1, to = 9, by = 2) # crée une séquence allant de 1 à 9 en ayant un écart de 2 à chaque fois ## [1] 1 3 5 7 9 gl(n= 2, k = 5 , labels = c(&quot;condition 1&quot;, &quot;condition 2&quot;)) # crée un facteur ayant 2 modalités, chacune étant répétée 5 fois. ## [1] condition 1 condition 1 condition 1 condition 1 condition 1 condition 2 ## [7] condition 2 condition 2 condition 2 condition 2 ## Levels: condition 1 condition 2 Nous allons recréer cette matrice en utilisant la fonction matrix. matrice&lt;-matrix(data = 1:9, # les données sont une séquence de chiffres allant de 1 à 9 nrow = 3, # elles doivent être réparties dans 3 lignes ncol = 3, # et dans 3 colonnes - un des deux arguments peut être omis puisque si on a le nombre de lignes on a le nombre de colonnes byrow = F) # logique qui permet d&#39;indiquer si on veut que les valeurs soient organisées par ligne ou par colonne. Ici, par colonne ## Notez que j&#39;ai commenté chacun des arguments à l&#39;intérieur de la fonction ## et que la fonction a été utilisée sur plusieurs lignes pour plus de lisibilité. matrice ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 Dans la plupart des situations, vos données contiendronsdes valeurs de nature différente. Il y aura des valeurs numériques, mais également des facteurs ou des identifiants de participants. Dans ce cas, la classe d’objet dans R n’est pas une matrice mais un dataframe. Pour le formuler autrement, un dataframe a toutes les propriétés d’une matrice (s’il ne contient que des valeurs de même nature, le dataframe pourra être considéré comme une matrice), mais a également la propriété de pouvoir gérer des informations de nature différente à la condition que le nombre de ligne soit toujours identique. Ainsi, on ne peut pas avoir 5 valeurs dans la première colonne et 4 dans la seconde sans préciser qu’il y a une valeur qui manque dans la seconde colonne. Le Tableau 5.2 un exemple qui pourrait représenter le genre et la taille de différents individus : .cl-1bf70414{}.cl-1be73e80{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-1bee8500{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1bee8514{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1beec376{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1beec380{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1beec38a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1beec38b{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1beec394{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1beec395{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 5.2: Exemple de dataframe avec une colonne contenant un facteur et une colonne contenant des valeurs numériques. sexetailleHomme176Femme173Homme180Femme165 Dans R, votre dataframe prendra la forme suivante : sexe&lt;-c(&quot;Homme&quot;, &quot;Femme&quot;,&quot;Homme&quot;, &quot;Femme&quot;) taille&lt;-c(176,173,180,165) dt&lt;-data.frame(sexe=sexe, taille=taille) dt ## sexe taille ## 1 Homme 176 ## 2 Femme 173 ## 3 Homme 180 ## 4 Femme 165 Ce format est donc bien plus flexible que la matrice. Il s’agit du format le plus conventionnel avec lequel vous pouvez réaliser le plus d’opérations. Néanmoins, nous devons aborder une autre manière de gérer des jeux de données qui est les tibbles. Ce format a été proposé par l’équipe de tidyverse (Wickham et al., 2019) pour faciliter la manipulation et améliorer l’affichage dans la console. Néanmoins, tout ce que vous pouvez faire avec un tibble, vous pouvez le faire avec un dataframe, l’inverse n’est pas vrai. Par exemple, vous ne pouvez pas donner des noms aux lignes dans un tibble, ni changer la nature d’une variable. Certaines fonctions vont requérir un dataframe, et les tibbles seront incompatibles. Alors, finalement, pourquoi aborder ce type de format ? Pour deux raisons : la première est que les tibbles est le format dans lequel vos données seront importées si vous utilisez excel pour créer vos données et le package readxl et la seconde raison est que ce format est particulièrement utile pour manipuler les données, notamment avec le package dplyr. Concrètement, voici à quoi va ressembler un tibble par rapport au dataframe. # on utilise ici la fonction tbl pour transformer notre dataframe # en tibble mais vous n&#39;aurez normalement jamais à utiliser cette fonction. dt_tbl&lt;-tibble(dt) dt_tbl ## # A tibble: 4 × 2 ## sexe taille ## &lt;chr&gt; &lt;dbl&gt; ## 1 Homme 176 ## 2 Femme 173 ## 3 Homme 180 ## 4 Femme 165 Remarquez que le tibble a fourni automatiquement les informations sur le nombre d’observations, le nombre de colonnes, ainsi que la nature des variables (character et double, pour numeric et integer). Vous pouvez obtenir ces informations pour un dataframe grâce à la fonction str ## &#39;data.frame&#39;: 4 obs. of 2 variables: ## $ sexe : chr &quot;Homme&quot; &quot;Femme&quot; &quot;Homme&quot; &quot;Femme&quot; ## $ taille: num 176 173 180 165 Evidemment, de même qu’il est assez aisé de passer d’un dataframe à un tibble, il est assez simple de passer d’un tibble à un dataframe avec la fonction as.data.frame. class(dt_tbl) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; dt_tbl&lt;-as.data.frame(dt_tbl) class(dt_tbl) ## [1] &quot;data.frame&quot; Le dernier format pour stocker des informations dans R que nous allons aborder est la liste. Il y a deux raisons qui motivent à aborder ce type de format. La première est que les listes sont les types d’objets qui permettent de stocker des informations de la manière la plus flexible qui soit, plusieurs informations de même taille, plusieurs matrices, plusieurs dataframes ou simplement une valeur ou un vecteur. Cela peut être particulièrement utile quand on doit réaliser une tâche itérative. La seconde raison est que les sorties de résultats de la plupart des fonctions sont stockées dans des listes. Ces informations ne seront que rarement directement utilisable. En revanche, des fonctions génériques, comme print ou summary, permettront d’avoir une présentation adaptée des résultats. Illustrons la flexbilité d’une liste en créant un liste où nous allons stocker la plupart des informations que nous avons créé dans la mémoire de R. ma.liste&lt;-list(# on crée un liste des objets de taille différentes dataframe = dt, # on ajoute un dataframe vecteur = notes, # un vecteur matrice = matrice, # une matrice valeur = logique2) # et une valeur unique. On peut évidemment continuer ma.liste ## $dataframe ## sexe taille ## 1 Homme 176 ## 2 Femme 173 ## 3 Homme 180 ## 4 Femme 165 ## ## $vecteur ## [1] 12 13 9 8 15 18 4 11 13 ## ## $matrice ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## $valeur ## [1] FALSE Par la suite, on peut accéder à n’importe quel élément d’un dataframe ou d’une liste en utilisant le symbole “$”. dt$sexe ## [1] &quot;Homme&quot; &quot;Femme&quot; &quot;Homme&quot; &quot;Femme&quot; ma.liste$vecteur ## [1] 12 13 9 8 15 18 4 11 13 5.3 La nature des variables avec easieR Quand vous importez les données, easieR, en plus de l’importation, fournit plusieurs informations importantes à considérer : Le nombre d’observations manquantes par variable. Si aucune information de cette nature n’apparaît dans la console, c’est qu’il n’y a pas de valeurs manquantes. Le nombre d’observations et de variables. La nature des variables qui prendra essentiellement deux formes : factor pour les variables qualitatives ; numeric (ou integer) pour les variables quantitatives. C’est à cette étape qu’il faut s’assurer que les données correspondent à ce qu’on est censé avoir : est-ce le bon nombre d’observations ? Est-ce que les variables sont dans le bon format. Par exemple, est-ce qu’une variable numérique n’est pas considérée comme un facteur parce que le caractère de séparation de décimale n’a pas été correctement choisi (la virgule à la place du point ou inversément) ? Ici, il est à noter que les variables ordinales, pour pouvoir être utilisées comme tel, doivent avoir un format numérique. Il est possible de faire la transformation directement dans R. Cependant, si l’objectif est d’éviter d’utiliser R en ligne de commande, il est préférable de l’anticiper en préparant correctement les données dans le tableur. Références Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://doi.org/10.21105/joss.01686 à la différence, une échelle d’intervalle, le rapport ne fait pas nécessairement sens. Par exemple, une personne qui a 10 à une échelle d’anxiété n’a pas deux fois plus d’anxiété qu’une personne qui a 5. Ces nuances ne sont néanmoins pas très importantes dans le cadre de ce cours car elles n’impacteront pas la manière dont les analyses seront menées.↩︎ Cette règle est arbitraire. Certains auteurs considèrent que 5 modalités différentes sont suffisantes, alors que certains logiciels, comme le logiciel lisrel, considèrent a priori que toutes les variables sont ordinales sauf si le nombre de modalités différentes est supérieur à 15. Dans ce cas, un message vous indiquera que la variable est considérée comme métrique.↩︎ "],["organiser-les-données.html", "Chapter 6 Organiser les données", " Chapter 6 Organiser les données Les ingrédients centraux pour tester une hypothèse sont les données. Aucun manuel, aucun enseignant soulignera suffisamment à quel point il est essentiel d’assurer le plus grand soin pour avoir des données de qualité. Si les données que vous avez récoltées n’ont pas été récoltées avec soin, les traitements statistiques réalisés seront vains. Pire si vous identifiez mal la nature de vos données, les traitements que vous réaliserez seront inadaptés et donc erronés. En revanche, ce qui peut être expliqué est la manière d’organiser correctement votre jeu de données. Quelques règles doivent être respectées pour vous assurer que vos données puissent être utilisables pour les traitements ultérieurs. Vos données représentent un tableau à double entrée, c’est-à-dire un dataframe, où les lignes représentent les observations et les colonnes représentent vos variables. Notez que le mot observation doit être pris au sens large. Dans certains cas, notamment quand on veut construire deux formes parallèles d’une tâche, il peut être intéressant de vérifier si chaque stimulus d’une version présente des caractéristiques similaires au stimulus qui lui est apparié dans l’autre version de la tâche. Dans ce cas, chaque stimulus pourra être considéré comme une observation. Il y a une exception à cette organisation : les données avec des mesures répétées. Si un individu passe par différentes conditions expérimentales, le nom de la colonne va représenter la modalité de la variable indépendante en mesure répétée et le contenu en-dessous de ce nom de colonne va représenter la variable dépendante mesurée dans chacune des conditions expérimentales. Si nous reprenons l’exemple du dataframe que nous avons présenté plus haut (avec la taille et le sexe), on pourrait avoir une mesure de la taille à 4 ans, à 10 ans et à 15 ans. Le jeu de données prendrait alors la forme du Tableau 6.1. .cl-1c3b03bc{}.cl-1c2fb732{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-1c339ffa{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1c33a004{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1c33e280{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c33e28a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c33e294{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c33e295{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c33e29e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c33e29f{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 6.1: Exemple de données pour lesquelles il y aurait plusieurs mesures chez les mêmes individus sexeTaille 4 ansTaille 10 ansTaille 15 ansHomme102138176Femme108135173Homme105140180Femme99142165 On se rend compte ici que la structure des données nous permet d’identifier si les mesures sont à mesures répétées ou à groupes indépendants. Puisque chaque ligne représente un individu et qu’on est sur la même ligne, il s’agit ici de données en mesures répétées. En revanche, on ne peut pas être totalement certain qu’il s’agit de groupes indépendants si les données sont organisées de la manière utilisée dans le Tableau 6.2 : .cl-1c5b32c2{}.cl-1c4ea1b0{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-1c532f32{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1c532f3c{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1c535caa{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c535cbe{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c535cbf{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c535cc8{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c535cc9{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c535cd2{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 6.2: Exemple de données pour lesquelles il peut y avoir ambiguité sur la nature répétée ou à groupes indépendants de l'âge des personnes sexeAgeTailleHomme4 ans102Femme4 ans108Homme4 ans105Femme4 ans99Homme10 ans138Femme10 ans135Homme10 ans140Femme10 ans142Homme15 ans176Femme15 ans173Homme15 ans180Femme15 ans165 Pour pouvoir interprété avec certitude le jeu de données, il faut absolument avoir un identifiant pour les observations. Dans le Tableau 6.3, les données représentent des données à groupes indépendants alors que dans le Tableau 6.4, elles représentent des données à mesure répétées. On le sait car les identifiants des observations ne sont pas les mêmes entre les différents âges dans le Tableau ?? alors qu’ils sont identiques aux différents âges dans le Tableau 6.4. .cl-1c791ff8{}.cl-1c6cdb12{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-1c7238fa{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1c723904{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1c72592a{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c725934{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c725935{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c72593e{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c72593f{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c725948{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 6.3: Données à groupes indépendants IdentifiantsexeAgeTailletyqHomme4 ans102xygFemme4 ans108btyHomme4 ans105zrgFemme4 ans99cbxHomme10 ans138skdFemme10 ans135tqeHomme10 ans140jxkFemme10 ans142zmeHomme15 ans176rkyFemme15 ans173jgwHomme15 ans180topFemme15 ans165 .cl-1c9b1932{}.cl-1c8fd46e{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-1c93ca10{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1c93ca1a{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1c94b48e{width:1.5in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c94b4a2{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c94b4a3{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c94b4ac{width:1.5in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c94b4ad{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c94b4b6{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c94b4c0{width:1.5in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c94b4c1{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1c94b4c2{width:3in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 6.4: Données en mesures répétées IdentifiantsexeAgeTaillexmhHomme4 ans102tcvFemme4 ans108jxwHomme4 ans105crdFemme4 ans99xmhHomme10 ans138tcvFemme10 ans135jxwHomme10 ans140crdFemme10 ans142xmhHomme15 ans176tcvFemme15 ans173jxwHomme15 ans180crdFemme15 ans165 La différence de présentation entre les données présentées dans le Tableau 6.1 et celles présentées dans le Tableau 6 est ce qu’on appelle un format large (Tableau 6.1) et un format long (Tableau 6.4). Pour terminer sur l’organisation des données, il est nécessaire de respecter quelques règles pour que l’importation se passe le mieux possible, respectez les règles suivantes : Votre feuille de données ne contient que les données (pas de graphique, pas commentaires, pas de mise en forme …) Les noms des variables doivent être courts (pas plus de 8 caractères ) rendant ainsi leur utilisation plus simple, ne doivent contenir ni accents, ni espaces. Pour enlever les espaces, utilisez la fonction rechercher/remplacer. Dans la fenêtre « rechercher », tapez un espace, dans la fenêtre « remplacer », tapez un underscore (le tiret en-dessous du 8 « _ ») ou un point. Il ne doit pas y avoir de ligne ou de colonne vide avant et dans la matrice de données. Les données commencent à la cellule A1 et s’étendent jusqu’à la dernière variable et la dernière observation sans discontinuer. Bien que ce ne soient pas indispensable lorsqu’on utilise excel mais est indispensable pour les fichiers csv, une bonne pratique consiste à indiquer dans les cellules vides la valeur NA (not available) que R reconnait comme étant une valeur non disponible. A nouveau, il est possible d’utilisez la fonction rechercher/remplacer dans le logiciel que vous avez utilisé pour stocker vos données. Si des modifications doivent être faites avant l’importation des données, ne travaillez jamais sur le fichier de données original. Enregistrez d’abord le fichier sous un autre nom. L’importation des nombres avec décimales ne pose pas de souci si vos données sont dans un fichier excel. En revanche, s’il s’agit d’un fichier csv, les nombres avec décimales sont considérés comme tels dans R à partir du moment où l’unité est séparée de la décimale par un point (« . »). Si vos décimales sont séparées des unités par des virgules, il faudra le préciser explicitement lors de l’importation des données. Ayez systématiquement un identifant pour chaque observation (càd. pour chaque participant). Si avec le temps, R s’est montré de plus en plus flexible pour la gestion des données, on ne peut que vous encourager à être très rigide sur la structure de la base de données. La première raison est que votre base de données représentent les ingrédients de vos analyses. Si les ingrédients sont de mauvaises qualités, les analyses seront de mauvaise qualité. La seconde raison est que faire preuve de laxisme sur les données est le meilleur moyen d’être confronté à des messages d’erreurs dans R que vous ne comprendrez pas. "],["préparer-les-données.html", "Chapter 7 Préparer les données 7.1 Dans R 7.2 Avec easieR", " Chapter 7 Préparer les données Jusqu’à présent, les données que nous avons utilisées ne correspondent pas à un vrai jeu de données sur lequel on voudrait réaliser effectivement des traitements. R permet de travailler avec des données issues de différents formats. Les deux formats de données les plus fréquemment utilisés sont les fichiers excel et les fichiers csv. Nous focaliserons sur ces deux formats, bien que R soit en mesure d’importer des fichiers d’autres natures grâce au package foreign (R Core Team, 2022). Les données sont rarement utilisable directement et il est, dans la grande majorité des cas, nécessaire de réaliser des opérations pour que les données puissent être utilisées pour tester les hypothèses. Cela peut passer par la suppression des observations d’entraînement, la transformation d’une variable, ou l’addition des items d’un questionnaire pour obtenir un score global. L’objectif de ce chapitre est d’illustrer comment on importe des données et comment on peut les préparer. 7.1 Dans R 7.1.1 L’importation A présent que vous avez précisé le répertoire de travail où se trouve vos données, que vous avez identifié que R trouve effectivement votre fichier dans le dossier de votre répertoire de travail avec la fonction dir, et que vous vous êtes assuré que les règles de construction des jeux de données ont été scrupuleusement respectées, l’étape suivante consiste à importer les données. Cette étape peut être réalisé avec la fonction du package readxl si vos données sont stockées dans un fichier excel, avec la fonction read.csv2, inclue de base dans R, pour un fichier csv. Pour la suite du chapitre, nous allons utiliser des données d’amorçage sémantique (données non publiées mais pour des données publiées avec cette tâche et une description en détail de la tâche, voir Stefaniak et al., 2010). Plus précisément, la tâche que les participants devaient réaliser est une tâche de double décision lexicale. Dans cette tâche, les participants doivent déterminer le plus rapidement et le plus précisémant possible si deux chaînes de lettres présentées simultanément à l’écran sont des mots (par exemple, MOIS - CADEAU) ou non (par exemple, AJÛT - MARRER). A leur insu, certaines paires de mots sont reliés sémantiquement (par exemple, MAIN - BAGUE) alors que d’autres ne le sont pas. Dans cette tâche, on s’attend à ce que les items reliés sémantiquement soient traités plus rapidement que ceux qui ne le sont pas. La tâche est composée de 185 essais répartis entre 5 essais d’entraînement (les 5 premiers) et 180 essais à inclure dans les analyses. Dans le jeu de données mis à disposition, nous avons les données de 40 participants. Les variables d’intérêt sont : Subject : l’identifiant du participant ; Attribute1 : le mot présenté en première position ; Attribute2 : le mot présenté en deuxième position ; CATEGORIE : la condition expérimentale de la paire de mot (SEMANTIQUE = paire sémantiquement reliés ; contre = paire contrebalancée entre les versions de la tâche ; rempl = paire de mots de remplissage ; pseudo = paire contenant au moins un pseudomot) Running : la variable qui permet de distinguer les essais d’entraînement des esssais cibles, qui doivent être analysés ; TextDisplay6.RT : le temps de réaction RC : la réponse correcte Il est important de noter ici que les données ont été préparées pour qu’il n’y ait aucun piège. Autrement dit, les règles présentées précédemment sont scrupuleusement appliquées ici. Pour vos données, il faut être attentif aux règles décrites précédemment. Il faut également noter que certaines variables d’intérêt dans le cadre de mon étude ne seront pas utilisées car cela n’apporteraient pas une réelle plus-value à l’exercice. 7.1.1.1 Importer un fichier excel Pour importer, des données en formation excel, on va s’appuyer sur le package readxl (Wickham &amp; Bryan, 2022) et on va utiliser les fonctions read_excel et excel_sheets La premère étape consiste à vérifier si le fichier est effectivement dans mon répertoire de travail avec la fonction dir dir() Cette fonction m’indique que mon fichier “semantic_priming.xlsx” est présent dans le répertoire. A partir d’ici,je pourrais ouvrir le fichier excel pour voir quel est le nom de la feuille de calcul que je dois importer mais une manière plus rapide de le faire est d’utiliser la fonction excel_sheets. Deux points sont importants ici. Il faut avoir chargé le package readxl (Wickham &amp; Bryan, 2022), ce qui normalement devrait être fait (voir la ligne 34 du script) et il faut que le fichier excel soit fermé. Dans le cas contraire, vous aurez un message vous indiquant que le fichier excel ne peut pas être ouvert alors que le chemin d’accès est correct. excel_sheets(&quot;./introR/semantic_priming.xlsx&quot;) ## [1] &quot;tout_direct&quot; En l’occurrence, il n’y a qu’une seule feuille de calcul intitulée “tout_direct”. On peut donc importer cette feuille avec la fonction read_excel. Nous devons préciser deux arguments : le chemin d’accès au fichier, c’est-à-dire le nom du fichier et la feuille de calcul que nous souhaitons importer. Ici, il est important de noter que, pour pouvoir utiliser ultérieurement les données, on va les stocker dans un objet appelé “data.xls”. data.xls&lt;-read_excel(path = &quot;./introR/semantic_priming.xlsx&quot;, sheet = &quot;tout_direct&quot;) Ainsi, en tapant ‘data.xls’, vous identifiez qu’il s’agit d’un tibble composé de 7400 lignes et 59 variables. data.xls ## # A tibble: 7,400 × 59 ## ExperimentName Subject Session Display.RefreshRate Group RandomSeed ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 direct1 1 1 60.1 1 2136425444 ## 2 direct1 1 1 60.1 1 2136425444 ## 3 direct1 1 1 60.1 1 2136425444 ## 4 direct1 1 1 60.1 1 2136425444 ## 5 direct1 1 1 60.1 1 2136425444 ## 6 direct1 1 1 60.1 1 2136425444 ## 7 direct1 1 1 60.1 1 2136425444 ## 8 direct1 1 1 60.1 1 2136425444 ## 9 direct1 1 1 60.1 1 2136425444 ## 10 direct1 1 1 60.1 1 2136425444 ## # ℹ 7,390 more rows ## # ℹ 53 more variables: SessionDate &lt;chr&gt;, SessionTime &lt;dttm&gt;, Block &lt;dbl&gt;, ## # Attribute1 &lt;chr&gt;, Attribute2 &lt;chr&gt;, CATEGORIE &lt;chr&gt;, essais &lt;chr&gt;, ## # essais.Cycle &lt;chr&gt;, essais.Sample &lt;chr&gt;, expe &lt;chr&gt;, expe.Cycle &lt;chr&gt;, ## # expe.Sample &lt;chr&gt;, List1 &lt;chr&gt;, List1.Cycle &lt;chr&gt;, List1.Sample &lt;chr&gt;, ## # Procedure &lt;chr&gt;, REPCORRECTE &lt;chr&gt;, Running &lt;chr&gt;, TextDisplay1.ACC &lt;chr&gt;, ## # TextDisplay1.CRESP &lt;chr&gt;, TextDisplay1.DurationError &lt;chr&gt;, … Pour les personnes souhaitant voir leurs données dans l’ensemble, la fonction View vous permettra d’ouvrir un nouvel onglet dans Rstudio dans lequel vous aurez l’intégralité des données. View(head(data.xls)) 7.1.1.2 Importer un fichier csv Pour importer un fichier csv, la fonction utilisée sera read.csv2. Si la procédure est sensiblement la même, quelques étapes et quelques arguments supplémentaires doivent être réalisées/précisés. Tout d’abord, on ne peut pas faire l’économie d’ouvrir le fichier car il faut connaître le caractère qui sépare chacune des colonnes. Il faut ouvrir le fichier avec le bloc note car si on l’ouvre avec un logiciel tel qu’excel, il ne sera pas possible d’identifier le symbole qui sert de séparateur de colonnes. En l’occurrence, le séparateur est le point-virgule (“;”). Ouvrir le fichier permet également de s’assurer que la première ligne du fichier correspond au nom des colonnes. Lors de l’importation, on va donc indiquer si la première ligne correspond au nom des variables avec l’argument header = T, on va indiquer le caractère qui sépare chaque colonne avec l’argument sep. Les valeurs possibles sont l’espace, la tabulation, le point-virgule et la virgule. L’espace est à déconseiller car les valeurs textuelles pourraient être traitées de manière inadaptée. Dans ce cas, il faut réengistrer le fichier avec un autre séparateur. Ceci est possible en utilisant par exemple excel, enregistrer sous et en choisissant le format. Il est également nécessaire de préciser le caractère qui va indiquer la séparation des décimales (le point ou la virgule), ainsi que la manière dont les valeurs manquantes ont été gérées. Fondamentalement on peut utiliser n’importe quelle valeur pour signaler les valeurs manquantes. Etant habitué à R, le code que j’utilise est NA. Notez qu’il n’est pas absolument nécessaire de le faire dans excel mais qu’il est indispensable de le faire avec un fichier csv. Enfin, l’argument check.names va permettre de vérifier si les noms utilisés comme nom de variables sont valides et les corriger le cas échéant. data.csv&lt;-read.csv2(file = &quot;semantic_priming.csv&quot;, header = T, sep = &quot;;&quot;, # valeurs possibles &quot;&quot;, &quot;\\tab&quot; ou &quot;,&quot; dec =&quot;.&quot;, # valeurs possibles &quot;.&quot; ou &quot;,&quot; na.string = &quot;NA&quot;, check.names=T) A nouveau, il est possible de voir les données importées en utilisant la fonction View. 7.1.2 Préparer les données Il n’est pas possible d’explorer toutes les potentialités de prétraitements des données qu’offre R mais l’exemple de travail utilisé permet d’illustrer quelques-unes de ces potentialités. Concrètement, pour que les données soient utilisables (nous allons faire l’hypothèse que l’analyse que nous voulons réaliser est une analyse de variance à mesure répétée d’un côté sur les temps de réaction et de l’autre sur les réponses correctes), nous allons devoir suivre les étapes suivantes : sélectionner les variables d’intérêt pour faciliter la manipulation des données ; supprimer les observations relatifs aux essais d’entraînement ; calculer le pourcentage de réponses correctes par participant et par conditions (la somme totale ne conviendrait pas ici étant donné qu’il n’y a pas le même nombre d’items dans toutes les conditions, càd. 30 items dans chacune des trois conditions mots et 90 dans la condition pseudomot) ; filtrer les temps de réaction pour ne conserver que ceux pour lesquels la réponse est correcte ; vérifier qu’il n’y a pas de temps de réaction correspondant à des réponses anticipées ou à des réponses inattentives. Habituellement, on gère ce cas de figure en utilisant la médiane mais en l’occurrence, par souci pédagogique, on va considérer que les réponses anticipées sont les temps inférieurs à 200 ms et les réponses inattentives ceux supérieurs à 2000 ms. calculer la moyenne des temps de réaction restant par participant et par condition. Combiner le jeu de données sur les réponses correctes et sur les temps de réaction. 7.1.2.1 Sélectionner des variables Pour sélectionner des variables, on va utiliser la fonction select du package dplyr. Il suffit de donner le nom du jeu de données et les variables qu’il faut sélectionner en les séparant par une virgule. Il existe néanmoins une difficulté ici. La fonction select existe dans plusieurs packages et peut créer un conflit car elle ne s’utilise pas de la même manière en fonction du package. Pour éviter le conflit, on peut préciser le package dans lequel il faut aller chercher la fonction en indiquant le nom du package suivi de deux-points, c’est-à-dire dplyr::. Nous conserverons uniquement les variables qui ont été décrites plus haut, à savoir : Subject, Attribute1, Attribute2, CATEGORIE, Running, TextDisplay6.RT, RC. data.et1&lt;-data.xls%&gt;%dplyr::select( Subject, Attribute1,Attribute2,CATEGORIE, Running, TextDisplay6.RT,RC) 7.1.2.2 Sélectionner des observations sur la base d’une valeur textuelle A présent que le jeu de données est plus lisible, nous pouvons supprimer les observations correspondant aux essais. On trouve cette information dans la variable ‘Running’. Grâce à la fonction unique, on peut identifier les différentes valeurs individuelles dans la variable ‘Running’. unique(data.et1$Running) ## [1] &quot;essais&quot; &quot;expe&quot; &quot;List1&quot; Sans surprise, les essais s’appellent les essais. Pour sélectionner des valeurs textuelles, on a deux possibilités : 1) selectionner les chaînes que nous souhaitons conserver ; 2) supprimer celles que nous ne voulons pas conserver. Etant donné qu’il y a 3 possibilités (“essais”, “expe”, “List1”) et qu’on veut garder tout ce qui n’est pas “essais”, on va donc décider d’utiliser la seconde stratégie. La logique consiste à sélectionner tout ce qui ne correspond pas à un critère. En l’occurrence, notre critère consiste à ne pas être un essai. Pour atteindre cet objectif, en plus d’utiliser la fonction filter, il faut connaître la liste des opérateurs logiques dans R. Le Tableau 7.1 une liste non exhaustive. .cl-1d4d5fc0{}.cl-1d42a7a6{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-1d46a2a2{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1d46bfda{width:0.973in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1d46bfe4{width:3.776in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1d46bfe5{width:0.973in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1d46bfee{width:3.776in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1d46bfef{width:0.973in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1d46bff0{width:3.776in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1d46bff8{width:0.973in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1d46bff9{width:3.776in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1d46c002{width:0.973in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1d46c003{width:3.776in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1d46c00c{width:0.973in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1d46c00d{width:3.776in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1d46c016{width:0.973in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1d46c020{width:3.776in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1d46c021{width:0.973in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1d46c02a{width:3.776in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 7.1: Liste des opérateurs logiques les plus fréquents OpérateurSignification==égal à !=Différent de&gt;plus grand&gt;=plus grand ou égal (le égal se met toujours à droite)&lt;plus petit&lt;=plus petit ou égal (le égal se met toujours à droite)&amp;et|ou Dans notre exemple, nous allons devoir utiliser l’opérateur “!=” pour indiquer que nous voulons conserver tous les valeurs qui sont différentes de “essais”. data.et2&lt;-filter(data.et1, Running!=&quot;essais&quot;) Pour déterminer si l’opération s’est déroulée avec succès, on peut vérifier si le nombre d’observations a été réduit de manière cohérente avec ce qu’on attend. Ainsi, on sait que nous avons 40 participants, et nous savons qu’il y a 5 essais par participants. On doit donc avoir une réduction de 200 observations entre les données à l’étape 1 (data.et1) et les données à l’étape 2 (data.et2). On obtient cette information avec la fonction dim, qui nous permet de constater que le nombre d’observations est passé de 7400 à 7200. dim(data.et1) ## [1] 7400 7 dim(data.et2) ## [1] 7200 7 7.1.2.3 Réaliser des opérations mathématiques : calcul de pourcentages de réponses correctes L’étape suivante consiste à obtenir le pourcentage de réponse correcte par condition. Grâce à la fonction unique, on est en mesure d’identifier qu’il y a 4 conditions (“contre”, “pseudo”, “rempl”, “SEMANTIQUE”) unique(data.et2$CATEGORIE) ## [1] &quot;contre&quot; &quot;pseudo&quot; &quot;rempl&quot; &quot;SEMANTIQUE&quot; Connaissant les données, j’avais indiqué qu’il y avait 30 items par condition sauf pour les pseudomots où il y a 90 items. On peut néanmoins s’assurer que cette répartition est correcte et que, individuellement, il n’y a pas d’observation manquante. On peut obtenir cette information avec la fonction table. table(data.et2$Subject, data.et2$CATEGORIE) ## ## contre pseudo rempl SEMANTIQUE ## 1 30 90 30 30 ## 2 30 90 30 30 ## 3 30 90 30 30 ## 4 30 90 30 30 ## 5 30 90 30 30 ## 6 30 90 30 30 ## 7 30 90 30 30 ## 8 30 90 30 30 ## 9 30 90 30 30 ## 10 30 90 30 30 ## 11 30 90 30 30 ## 12 30 90 30 30 ## 13 30 90 30 30 ## 14 30 90 30 30 ## 15 30 90 30 30 ## 16 30 90 30 30 ## 17 30 90 30 30 ## 18 30 90 30 30 ## 19 30 90 30 30 ## 20 30 90 30 30 ## 21 30 90 30 30 ## 22 30 90 30 30 ## 23 30 90 30 30 ## 24 30 90 30 30 ## 25 30 90 30 30 ## 26 30 90 30 30 ## 27 30 90 30 30 ## 28 30 90 30 30 ## 29 30 90 30 30 ## 30 30 90 30 30 ## 31 30 90 30 30 ## 32 30 90 30 30 ## 33 30 90 30 30 ## 34 30 90 30 30 ## 35 30 90 30 30 ## 36 30 90 30 30 ## 37 30 90 30 30 ## 38 30 90 30 30 ## 39 30 90 30 30 ## 40 30 90 30 30 Nous pouvons à présent calculer le pourcentage de réponses correctes par participant et par condition. Cette étape est un peu plus complexe et illustre comment il est possible d’enchaîner les opérations avec le package dplyr. Ainsi, nous allons réaliser l’opération en 3 fois. Dans un premier temps, on va indiquer sur quel jeu de données les opérations doivent être réalisées. Dans un second temps, on indique les variables sur lesquelles nous souhaitons faire le regroupement. Enfin, on calcule le pourcentage de réponses correctes dans une nouvelle variable (‘perc’). Il existe sans doute différentes approches pour cette dernière étape. Nous allons privilégier une approche assez simple. Etant donné que les réponses correctes sont des 1 et des 0, en calculant la moyenne par participant et par condition, nous aurons la proportion de réponses correctes qu’il suffira de multiplier par 100. Les symboles %&gt;% s’appellent des pipes et permettent de lier les opérations entre elles. En d’autres termes, on peut réaliser l’ensemble des opérations en une seule fois. RC_percent &lt;- data.et2 %&gt;% # le résultat sera stocké dans RC percent et on effectue des opérations sur data.et2 group_by(Subject, CATEGORIE )%&gt;% # un regroupe les données par les modalités des variables Subject et CATEGORIE summarise(perc_RC = mean(RC)*100 ) # On résume les données en calculant le pourcentage de réponses correctes ## `summarise()` has grouped output by &#39;Subject&#39;. You can override using the ## `.groups` argument. 7.1.2.4 Sélectionner des observations sur la base d’une valeur numérique Nous voulons à présent sélectionner les réponses correctes. On va utiliser une procédure similaire à celle utilisée pour supprimer les essais, avec comme différence le fait que nous allons conserver les observations pour lesquelles la réponses vaut 1 en utilisant l’opérateur “==”. La seconde différence est que les valeurs numériques ne sont pas entre guillemets contrairement aux chaînes de caractères. table(data.et2$RC) # il y a 5679 observations correctes ## ## 0 1 ## 1521 5679 data.et4&lt;-filter(data.et2, RC==1) # on sélectionne les observations pour lesquelles la réponse est correcte dim(data.et4) # on obtient effectivement 5679 observations ## [1] 5679 7 7.1.2.5 Combiner plusieurs critères de sélection L’avant-dernière étape consiste à vérifier qu’il n’y a pas de réponses anticipées ou des réponses inattentives. Nous allons fixer comme critères 200 ms pour les réponses anticipées. Cela signifie que nous considérons qu’il n’est pas possible de répondre en moins de 200 millisecondes en ayant traité de manière volontaire les stimuli. Dans cet exemple, ce raisonnement est un peu fallacieux car les items sont présentés pendant 400 ms sans qu’on ne puisse répondre et un écran blanc apparaît ensuite. Ainsi, avec un critère à 200 ms, cela laisse 600 ms pour répondre ce qui peut avoir été suffisant. L’objectif ici étant d’être pédagogique, nous conserverons tout de même ce critère. Les réponses inattentives sont les réponses pour lesquelles les latences sont tellement longues qu’on doit considérer que le participant a répondu suite à une inattention et ne reflète pas réellement la manière dont il se comporte dans ce type de tâche. Nous fixerons de manière arbitraire ce critère à 2000 ms. Nous allons donc filtrer les données en appliquant deux critères de sélection. data.et5&lt;-data.et4 %&gt;% filter(TextDisplay6.RT&gt;200) %&gt;% # on conserve les temps supérieurs à 200 ms filter(TextDisplay6.RT&lt;2000) # on conserve les temps inférieurs à 2000 ms Remarquez que nous avons ici aussi utilisé les pipes pour réaliser plusieurs opérations en une fois. 7.1.2.6 Réaliser des opérations mathématiques : calcul de la moyenne des temps de réaction A présent, nous pouvons faire la moyenne des temps de réaction sur les items restant. Nous avons déjà vu comment réaliser cette opération à l’étape 3. data.et6 &lt;- data.et5 %&gt;% # le résultat sera stocké dans RC percent et on effectue des opérations sur data.et2 group_by(Subject, CATEGORIE )%&gt;% # un regroupe les données par les modalités des variables Subject et CATEGORIE summarise(M_TR = mean(TextDisplay6.RT) ) # On fait la moyenne des temps de réaction par participant et par condition ## `summarise()` has grouped output by &#39;Subject&#39;. You can override using the ## `.groups` argument. 7.1.2.7 Réaliser une jointure entre des jeux de données Il nous reste plus qu’à regrouper dans une base de données finales les données avec les pourcentages de réponses correctes et la moyenne des temps de réaction. On obtient ce résultat en utilisant la fonction left_join, où les deux premiers arguments sont les jeux de données à combiner et l’argument by correspond au nom des variables qui permettent de faire l’appariement. Ici, je les ai donné de manière explicite en indiquant qu’il faut prendre en compte la variable « Subject » et la variable « CATEGORIE », qui s’appellent de la même manière dans les différentes base de données. data_final&lt;-left_join(data.et6, RC_percent, by=c(&quot;Subject&quot;=&quot;Subject&quot;,&quot;CATEGORIE&quot;=&quot;CATEGORIE&quot; )) 7.2 Avec easieR 7.2.1 Importer les données Pour commencer à utiliser easieR, nous allons débuter par l’importation de données. L’exemple de travail que nous allons utiliser est le suivant : dans une publicité, Georges Clooney va chercher un café au même moment qu’une jeune femme. Il s’aperçoit que c’est la dernière capsule et la laisse donc à la jeune femme, qui était venue chercher un café pour Jean Dujardin. La question que se pose les chercheur est de savoir comment on interprète cette publicité. Est-ce que Georges Clooney se laisse manipuler par les femmes, est-ce qu’il s’agit d’un gentleman ou est-ce que Jean Dujardin a eu raison de manipuler Clooney pour arriver à ses fins. Les données sont disponibles dans le feuille de calcul appelée ‘Clooney’. 7.2.1.1 Avec les boîtes de dialogue Pour importer des données avec les boîtes de dialogue, il suffit de lancer easieR avec la fonction easieR easieR() La boîte de dialogue de la Figure 7.1 apparaît. Il faut choisir “Donnees - (importation, exportation, sauvegarde) et cliquer sur OK. Figure 7.1: Fenêtre d’accueil de easieR. Choisir : Donnees - (importation, exportation, sauvegarde) De manière assez transparente, pour importer les données, il faut choisir dans la boîte de dialogue de la Figure 7.2 “importer des donnees” Figure 7.2: Choisir : importer des donnees Le package easieR permet d’importer des données de 4 types de format : CSV, txt, excel et les fichiers SPSS. En réalité, R est en mesure d’importer d’autres formats de données, mais les 4 formats proposés dans easieR représentent les outils utilisés le plus fréquemment utilisés par les psychologues. En l’occurrence, nous travaillerons avec des fichiers excel (voir Figure 7.3). Il est possible d’accéder directement à cette boîte de dialogue grâce à la fonction import() Figure 7.3: Format du fichier de données à importer. Une fois le format du fichier décidé, il ne reste plus qu’à choisir le fichier de données (Figure @ref(fig=import4)). Figure 7.4: Choix du fichier de données, ‘illustration.easieR.xlsx’ en l’occurrence. La plupart du temps, les chercheurs utilisent la première ligne du fichier de données pour indiquer le nom des variables. Cependant, ce n’est pas toujours le cas. Il est donc nécessaire d’indiquer si la première ligne correspond effectivement au nom des variables (Figure 7.5). Figure 7.5: Est-ce que la première ligne correspond aux variables ? De la même manière, quand une valeur est manquante, il y a plusieurs manières d’indiquer les valeurs manquantes. Si la cellule est vide, on peut laisser “NA” en revanche, si une valeur par défaut (comme -9999) est utilisée alors, il faut l’indiquer dans la Figure 7.6. Figure 7.6: Valeurs servant à représenter les valeurs manquantes. Si la cellule est vide, laissez NA. L’étape suivante consistera à choisir la feuille de calcul qui nous intéresse. En l’occurrence, il s’agit de la feuille ‘Clooney’ (Figure 7.7). Figure 7.7: Format du fichier de données à importer. Et il faut terminer en donnant un nom aux données qui sera utilisé par la suite dans R. Par défaut, c’est le nom de la feuille de calcul (Figure 7.8). Ce jeu de données doit avoir un nom qui ne contient pas de caractères spéciaux (excepté _ ou .), pas de caractères avec des accents ou des cédilles, et préférez des noms cours (max 20 caractères). Figure 7.8: Nom attribué aux données pour les utiliser ensuite dans R. Le jeu de données apparaît à présent dans la console (Figure 7.9). Figure 7.9: Tableau du jeu de données. Ainsi que des informations sur les données. En l’occurrence, il s’agit d’un jeu de données avec 67 observations, 3 variables qui sont toutes les trois des variables catégorielles (Figure 7.10). Figure 7.10: Structure du jeu de données. Remarques importantes Trois remarques doivent être faites : Les explications des boîtes de dialogue sont fournies dans la console quand on utilise easieR, ce qui permet de savoir à quoi elles correspondent même si on ne le sait pas (Figure 7.11) ; Figure 7.11: Information des boîtes de dialogue qui sont affichées dans la console. easieR va changer automatiquement le répertoire de travail pour travailler dans le même répertoire que celui où se trouve le jeu de données ; dans la console apparaît un ligne appelée “call”. Cette ligne correspond à la ligne de commande (il faut enlever les guillemets au début et à la fin) pour importer le fichier de données (Figure 7.12). Figure 7.12: Ligne de commande qui correspond à l’importation du fichier. Attention, pour l’utiliser, il faut supprimer les guillemets des extrémités. 7.2.1.2 En ligne de commande On peut également utiliser easieR en ligne de commande. Cette fonction a 8 arguments : file : le nom du fichier dir : le répertoire où se trouve le fichier type : le type de fichier dec : lorsqu’on utilise un fichier csv ou txt, le séparateur de décimale peut être soit un point, soit une virgule. Il faut le préciser. Dans excel, la fonction le détecte automatiquement. sep : dans les fichiers txt et csv, les colonnes peuvent être séparés par différents types de caractères (tabulation, virgule, espace ou point-virgule). Il faut alors indiquer quel est le séparateur de colonnes. Dans excel, il l’identifie par défaut. na.strings : caractères utilisés pour indiquer qu’une valeur est manquante. NA est la valeur par défaut. sheet : pour les fichiers excel, il faut indiquer la feuille de calcul dans laquelle se trouve les données name : correspond au nom qu’on veut attribuer au jeu de données dans R. Ce jeu de données doit avoir un nom qui ne contient pas de caractères spéciaux (excepté _ ou .), pas de caractères avec des accents ou des cédilles, et préférez des noms cours (max 20 caractères). De manière concrète pour importer le même fichier que celui importé en boîte de dialogue, on utiliser la fonction suivante : import(file=&#39;illustration.easieR.xlsx&#39;, dir=&#39;C:/Users/mon_repertoire/Cours de statistiques/Livre/introR&#39;, type=&#39;Fichier Excel&#39;, dec=&#39;.&#39;, sep=&#39;;&#39;, na.strings=&#39;NA&#39;, sheet=&#39;Clooney&#39;, name=&#39;Clooney&#39;) Le jeu de données apparaît à présent dans la console (Figure 7.9). Ainsi que des informations sur les données. En l’occurrence, il s’agit d’un jeu de données avec 67 observations, 3 variables qui sont toutes les trois des variables catégorielles 7.10. Vous avez donc à présent importer le jeu de données présenté dans le Tableau 1. 7.2.2 Préparer les données Références R Core Team. (2022). Foreign: Read data stored by ’minitab’, ’s’, ’SAS’, ’SPSS’, ’stata’, ’systat’, ’weka’, ’dBase’, ... https://CRAN.R-project.org/package=foreign Stefaniak, N., Meulemans, T., &amp; Willems, S. (2010). Semantic hyperpriming in normal aging: A consequence of instructions? [Article]. Aging, Neuropsychology, and Cognition, 17(5), 615–632. https://doi.org/10.1080/13825585.2010.492205 Wickham, H., &amp; Bryan, J. (2022). Readxl: Read excel files. https://CRAN.R-project.org/package=readxl "],["les-statistiques-descriptives.html", "Chapter 8 Les statistiques descriptives 8.1 Introduction 8.2 Un exemple de travail 8.3 Aspects théoriques 8.4 Les statistiques descriptives avec R 8.5 Traiter des données quantitatives 8.6 Faire une représentation graphique des données 8.7 Pourquoi utiliser la loi normale pour comprendre les graphiques en violon ? 8.8 Traiter des données qualitatives 8.9 Statistiques descriptives avec easieR", " Chapter 8 Les statistiques descriptives “Si les statistiques vous ennuient, c’est sans doute parce que vous n’avez pas les bons chiffres.” — Joe Martin Résumé L’objectif de cette page est de découvrir comment obtenir les principales statistiques descriptives pour des variables quantitatives et qualitatives en utilisant R. Nous apprendrons à calculer des mesures essentielles telles que la moyenne, la médiane, l’écart-type, les quartiles, ainsi que des résumés de fréquences pour les variables qualitatives. Pour les utilisateurs de R en ligne de commande Dans un premier temps, nous nous concentrerons sur les variables quantitatives en utilisant principalement deux fonctions du package psych (Revelle, 2025) à savoir describe et describeby. Ce package offre une méthode simple et rapide pour générer des résumés détaillés de vos données numériques. À travers des exemples concrets et des exercices d’application, vous pourrez vérifier votre compréhension et appliquer les concepts abordés. Ensuite, nous traiterons des variables qualitatives. Bien que le package psych (Revelle, 2025) ne soit pas utilisé pour ces variables, nous verrons comment obtenir des résumés de fréquences et des proportions en utilisant d’autres fonctions intégrées dans R, comme table et prop.table. Des exemples pratiques vous permettront de comprendre comment analyser et interpréter la répartition des catégories pour les variables qualitatives. Enfin, pour compléter notre analyse, nous aborderons la visualisation de la distribution des données quantitatives à l’aide de graphiques en violon. Ces graphiques, créés avec le package ggplot2, combinent densité et quartiles, offrant une vue d’ensemble complète de la distribution des variables quantitatives. Nous apprendrons à générer et interpréter ces graphiques pour mieux comprendre la forme et les caractéristiques des distributions de nos données. Pour les utilisateurs de easieR Nous aborderons comment obtenir les statistiques descriptives via l’interface de easieR et en ligne de commande Prérequis D’un point de vue théorique être en mesure d’identifier la nature d’une variable. Pour les utilisateurs de R Avoir installé R et Rstudio et les principaux packages psych (Revelle, 2025) et ggplot2 (Wickham, 2016a). Savoir installer et charger des packages dans R. Savoir importer un fichier excel dans R. Connaître et utiliser les règles de bonnes pratiques dans R. Pour les utilisateurs d’easieR avoir installé R avoir installé easieR avoir installé pandoc être en mesure d’importer un fichier de données 8.1 Introduction Commencer ses analyses par les statistiques descriptives reste une étape indispensable quand il s’agit d’analyser les données. Elles permettent en effet d’avoir une représentation de prime abord des résultats avant d’éprouver des hypothèses d’un point de vue inférentiel. Cette étape permet aussi de s’assurer de la compatibilité des valeurs de notre jeu de données avec les données possibles (i.e., on ne peut pas avoir un âge négatif) ainsi que de la distribution des données (on imagine aisément que l’âge de décès ne suivra pas une distribution normale). Par exemple, si un chercheur émet l’hypothèse que lever les mains en l’air rend heureux. Il compare un groupe de personnes qui doivent lever leur main 20 minutes par jour et un groupe qui ne le fait pas. Quel sens cela aurait d’aller vérifier la significativité de la différence entre ces deux groupes si cet expérimentateur s’apercevait de prime abord que la moyenne relative à la sensation de bonheur du groupe qui ne lève pas les mains est supérieure à celle de ceux qui lèvent la main ? Si la différence s’avérait significative, la seule conclusion possible serait qu’il ne faut absolument pas lever les mains pour être heureux. Un exemple farfelu… mais peut-être pas tant que cela. Au-delà du côté farfelu de cette hypothèse, il faut se rappeler que les hypothèses doivent systématiquement être construites sur des éléments théoriques solides. Il n’est dès lors pas acceptable de penser des statistiques sans les mettre en relation avec les résultats obtenus dans d’autres recherches et sans les mettre en relation avec le plan expérimental qui a été utilisé pour acquérir les données. Nous reviendrons à ce titre sur l’importance de réfléchir aux statistiques lors de la construction du plan expérimental. L’exemple proposé ci-dessus, bien que caricatural, est en réalité très souvent observé dans la littérature scientifique, car beaucoup de chercheur·euses oublient que la science ne consiste pas à confirmer leur hypothèse, mais à proposer des modèles qu’on tolérera en attendant de l’infirmer. L’objectif n’est donc pas de confirmer des hypothèses, mais d’infirmer des modèles (Popper, 1959). Cette notion est particulièrement importante, car elle est à la base de la réflexion concernant un protocole expérimental : plutôt que de se demander « comment il est possible de confirmer une hypothèse », il faudrait se demander « quelle est la pire condition à laquelle un modèle pourrait être exposé pour s’assurer qu’il a du sens ? ». Si cet encart peut paraître être très lié à de la méthodologie expérimentale plutôt qu’à des statistiques, il a pour objectif de mettre en exergue que tout est lié : il n’est pas possible de comprendre correctement les statistiques sans comprendre la méthodologie et sans avoir les bases théoriques qui sous-tendent des hypothèses (Popper, 1959). Comme tout logiciel de statistiques qui se respecte, R permet de réaliser un ensemble de statistiques descriptives telles qu’une moyenne. Il est dès lors nécessaire de présenter brièvement les possibilités offertes par le logiciel. 8.2 Un exemple de travail Il faut noter que l’exemple présenté ici l’est à titre pédagogique. Des exemples pratiques seront mis à disposition de la section ad hoc. Néanmoins, afin de permettre de suivre les exercices, les données brutes utilisées pour l’exemple sont présentées ci-dessous. Des chercheurs en informatique ont développé un indice pour mesurer la sensibilité des souris : le Mickey. Il s’agit du plus petit mouvement détectable par un souris (de l’ordre d’un centième de millimètre). Voici les valeurs de Mickey pour un échantillon de 50 souris. Pour suivre le contenu de ce chapitre, il est possible de copier coller le code ci-dessous dans la console R, ou mieux dans un script. Mickey&lt;-c(0.0113330511711523, 0.0105185934305092, 0.0122513865231973, 0.00799584561999877, 0.0132469458473191, 0.0157068027032564, 0.00566814411520385, 0.00651748542037685, 0.0136296612222217, 0.0131472071409701, 0.0112584029308029, 0.0073426743605309, 0.00819412905929006, 0.0136385776747903, 0.00862091807205004, 0.00281600176335617, 0.0110708097988487, 0.0166980801865014, 0.010854177755818, 0.0149495260876833, 0.00718270492413349, 0.0115037061826869, 0.00636648780668218, 0.00882227263188745, 0.0110918586149659, 0.010824072121312, 0.0140521995836573, 0.007002510588409, 0.00684296297629619, 0.00846894194399499, 0.00951172046799207, 0.0035962148433201, 0.0141868125694707, 0.00833252179279338, 0.0130351412904844, 0.0112662065604392, 0.0132115045987431, 0.00750178351850188, 0.00605748408387878, 0.00901230294010093, 0.00756045671627793, 0.0149879411056116, 0.0132127370139097, 0.0145783151332873, 0.0115756506296022, 0.0117585225313167, 0.0145584346705547, 0.0112858517472226, 0.00884871032853877, 0.0159595622769445) 8.3 Aspects théoriques 8.3.1 Les indices de tendance centrale Trois indices de tendance centrale sont communément utilisés en statistiques : la moyenne, la médiane et le mode. Plus récemment, un quatrième indice a commencé à émerger : les moyennes tronquées (Wilcox, 2012). Il existe par ailleurs des indices moins connus tels que le M-estimator de Huber (Huber, 1981) ou la moyenne winsorisée (Wilcox, 2012). 8.3.1.1 Le mode Le mode représente la valeur la plus représentée dans l’effectif. Par exemple, si 2 personnes ont la valeur 5 et 3 personnes ont la valeur 10, le mode sera 10, car il y a plus de personnes qui ont 10 que de personnes qui ont 5. Cet indice de tendance central peut être utilisé sur tous les types de variables (catégorielle, ordinale, métrique). Elle est particulièrement adaptée pour les variables qualitatives. Calculer le mode sur une variable numérique a relativement peu de sens, du moins si elle est continue, car il est très rare qu’il y ait plusieurs observations avec exactement la même valeur. Nous allons recatégoriser ces valeurs en valeurs inférieures à 0.005, entre 0.005 et 0.1 , entre 0.01 et 0.015 et au-dessus : Mickey.range&lt;-ifelse(Mickey&lt;0.005, &quot;&lt;0.005&quot;, ifelse(Mickey&gt;0.005 &amp; Mickey&lt;0.01, &quot;0.005&lt;x&lt;0.01&quot;, ifelse(Mickey&gt;0.015, &quot;&lt;0.015&quot;,&quot;0.01&lt;x&lt;0.015&quot; ))) Mickey.range ## [1] &quot;0.01&lt;x&lt;0.015&quot; &quot;0.01&lt;x&lt;0.015&quot; &quot;0.01&lt;x&lt;0.015&quot; &quot;0.005&lt;x&lt;0.01&quot; &quot;0.01&lt;x&lt;0.015&quot; ## [6] &quot;&lt;0.015&quot; &quot;0.005&lt;x&lt;0.01&quot; &quot;0.005&lt;x&lt;0.01&quot; &quot;0.01&lt;x&lt;0.015&quot; &quot;0.01&lt;x&lt;0.015&quot; ## [11] &quot;0.01&lt;x&lt;0.015&quot; &quot;0.005&lt;x&lt;0.01&quot; &quot;0.005&lt;x&lt;0.01&quot; &quot;0.01&lt;x&lt;0.015&quot; &quot;0.005&lt;x&lt;0.01&quot; ## [16] &quot;&lt;0.005&quot; &quot;0.01&lt;x&lt;0.015&quot; &quot;&lt;0.015&quot; &quot;0.01&lt;x&lt;0.015&quot; &quot;0.01&lt;x&lt;0.015&quot; ## [21] &quot;0.005&lt;x&lt;0.01&quot; &quot;0.01&lt;x&lt;0.015&quot; &quot;0.005&lt;x&lt;0.01&quot; &quot;0.005&lt;x&lt;0.01&quot; &quot;0.01&lt;x&lt;0.015&quot; ## [26] &quot;0.01&lt;x&lt;0.015&quot; &quot;0.01&lt;x&lt;0.015&quot; &quot;0.005&lt;x&lt;0.01&quot; &quot;0.005&lt;x&lt;0.01&quot; &quot;0.005&lt;x&lt;0.01&quot; ## [31] &quot;0.005&lt;x&lt;0.01&quot; &quot;&lt;0.005&quot; &quot;0.01&lt;x&lt;0.015&quot; &quot;0.005&lt;x&lt;0.01&quot; &quot;0.01&lt;x&lt;0.015&quot; ## [36] &quot;0.01&lt;x&lt;0.015&quot; &quot;0.01&lt;x&lt;0.015&quot; &quot;0.005&lt;x&lt;0.01&quot; &quot;0.005&lt;x&lt;0.01&quot; &quot;0.005&lt;x&lt;0.01&quot; ## [41] &quot;0.005&lt;x&lt;0.01&quot; &quot;0.01&lt;x&lt;0.015&quot; &quot;0.01&lt;x&lt;0.015&quot; &quot;0.01&lt;x&lt;0.015&quot; &quot;0.01&lt;x&lt;0.015&quot; ## [46] &quot;0.01&lt;x&lt;0.015&quot; &quot;0.01&lt;x&lt;0.015&quot; &quot;0.01&lt;x&lt;0.015&quot; &quot;0.005&lt;x&lt;0.01&quot; &quot;&lt;0.015&quot; On peut à présent comptabiliser les valeurs uniques différentes : tab&lt;-table(Mickey.range) tab ## Mickey.range ## &lt;0.005 &lt;0.015 0.005&lt;x&lt;0.01 0.01&lt;x&lt;0.015 ## 2 3 19 26 Dans la table ci-dessus, on constate que ce sont les valeurs comprises entre 0.005 et 0.01 qui sont les plus fréquentes, ce qui représente donc le mode. tab[which.max(tab)] ## 0.01&lt;x&lt;0.015 ## 26 8.3.1.2 La médiane La médiane représente la valeur x dans un échantillon de N individus qui divisera cet échantillon en deux lorsque les valeurs sont ordonnées de manière croissante. Cet indice de tendance central peut être utilisé avec une variable ordinale et une variable métrique. La raison pour laquelle il est admis d’utiliser la médiane sur les variables ordinales est que cet indice ne nécessite aucune présupposition sur les propriétés d’intervalle des échelles. Par ailleurs, la médiane est particulièrement intéressante pour limiter l’impact des valeurs extrêmes. Par exemple, quand on enregistre les temps de réaction, ils sont le plus souvent de l’ordre de quelques centaines de millisecondes. Pourtant,certains temps sont supérieurs à plusieurs secondes. Cela arrive quand l’individu a été inattentif à cet instant. L’utilisation des médianes permet d’éviter que cette valeur ait un impact trop important sur les résultats. En d’autres termes, la médiane n’est pas sensible aux valeurs extrêmes. Notez néanmoins que cette médiane est souvent utilisée en intra-individuel (lorsque plusieurs mesures de temps de réaction sont réalisées chez le même individu), mais plus rarement sur des comparaisons de groupes (néanmoins, pour outils inférentiels utilisant la médian, voir Wilcox, 2012). Dans notre exemple, on peut commencer par trier les données : Mickey&lt;-sort(Mickey) Mickey ## [1] 0.002816002 0.003596215 0.005668144 0.006057484 0.006366488 0.006517485 ## [7] 0.006842963 0.007002511 0.007182705 0.007342674 0.007501784 0.007560457 ## [13] 0.007995846 0.008194129 0.008332522 0.008468942 0.008620918 0.008822273 ## [19] 0.008848710 0.009012303 0.009511720 0.010518593 0.010824072 0.010854178 ## [25] 0.011070810 0.011091859 0.011258403 0.011266207 0.011285852 0.011333051 ## [31] 0.011503706 0.011575651 0.011758523 0.012251387 0.013035141 0.013147207 ## [37] 0.013211505 0.013212737 0.013246946 0.013629661 0.013638578 0.014052200 ## [43] 0.014186813 0.014558435 0.014578315 0.014949526 0.014987941 0.015706803 ## [49] 0.015959562 0.016698080 L’observation qui va diviser notre échantillon en deux échantillons de taille identique est donc comprise entre la \\(25^e\\) et la \\(26^e\\) observations. On va donc faire la moyenne entre ces deux valeurs. (Mickey[25]+Mickey[26])/2 ## [1] 0.01108133 On peut obtenir cette valeur plus rapidement avec la fonction median : median(Mickey) ## [1] 0.01108133 8.3.1.3 La moyenne Le troisième indice de tendance centrale est la moyenne, qui est la somme des valeurs individuelles \\(x_i\\) divisé par le nombre d’observations N : \\[\\mu=\\frac{\\sum_{i=1}^{N}X_i}{N}\\] Notez que la moyenne est désignée par la lettre grecque \\(\\mu\\) lorsqu’il s’agit d’un paramètre et par la lettre « m » lorsqu’il s’agit d’une estimation du paramètre à partir d’un échantillon. Cette distinction entre lettres romaines et lettres grecques pour distinguer l’évaluation d’un paramètre ou son estimation à partir d’un échantillon s’applique à l’ensemble des indices. Ainsi, pour l’estimation d’une moyenne à partir d’un échantillon, la formule est : \\[m=\\frac{\\sum_{i=1}^{N}X_i}{N}\\] Concrètement, si on prend notre exemple, la moyenne vaut la somme des Mickey : sum(Mickey) ## [1] 0.527654 qu’on va diviser par le nombre d’observation : length(Mickey) # 50 observations ## [1] 50 Ce qui donne : sum(Mickey)/length(Mickey) ## [1] 0.01055308 On peut évidemment obtenir cette valeur directement à l’aide de la fonction mean mean(Mickey) ## [1] 0.01055308 8.3.1.4 La moyenne tronquée La moyenne tronquée est un mélange entre la moyenne et la médiane : la moyenne tronquée représente la moyenne sur les valeurs qui restent après avoir supprimé un certain pourcentage des valeurs les plus extrêmes. On fait donc la moyenne sur $k=1 - 2$ où \\(\\alpha\\) représente la probabilité d’observations tronquées. On multiplie cette probabilité par 2 puisque la troncature s’applique sur les deux extrémités de la courbe. Notez que, en fonction du contexte, la troncature peut s’appliquer sur une des extrémités de la courbe uniquement. Ainsi, pour une moyenne tronquée à 10%, il faut d’abord identifier le nombre d’observations sur lesquelles il faut faire la moyenne. En l’occurrence, nous avons 50 observations, 10% de 50 vaut 5. Comme on fait la troncature sur les deux extrémités, on va donc retirer \\(k= 50\\times 2\\times 0.10 = 10\\). Donc, la moyenne sera réalisée sur \\(R=50 - 10 =40\\) observations restantes. \\[ T=\\frac{1}{n\\times(1-2\\alpha)} \\left( (1-r)\\left(X_{g+1}+X_{n-g}\\right) +\\sum_{i=g+2}^{n-g-1} X_i \\right) \\] où X représente les valeurs ordonnées, n est le nombre d’observation, \\(\\alpha\\) est la troncature à chaque extrémité, g est la part entière de \\(n \\times \\alpha\\) et r est la partie restante après la virgule de \\(n \\times \\alpha\\). Ainsi, on commence par trier les observations Mickey.tri&lt;- sort(Mickey) On identifie la \\(6^e\\) observation Mickey.tri[6] ## [1] 0.006517485 Et la \\(45^e\\) observation Mickey.tri[45] ## [1] 0.01457832 Puisque \\(0.1 \\times 50 = 5.00\\), la part entière de g vaut 5 et la partie restante après la virgule vaut 0. On peut à présent compléter la formule avec les valeurs suivantes : \\[ T=\\frac{1}{n\\times(1-(2\\alpha))} \\left( (1-r)\\left(X_{g+1}+X_{n-g}\\right) +\\sum_{i=g+2}^{n-g-1} X_i \\right) \\] \\[ T=\\frac{1}{50 \\times (1-(2 \\times 0.10))} \\left( (1-0)\\left(0.0065 + 0.0146\\right) +\\sum_{i=7}^{44} X_i \\right) = 0.01062119 \\] Tr&lt;-(1/40 *((1-0)*(Mickey.tri[6]+Mickey.tri[45])+sum(Mickey.tri[7:44]))) Tr ## [1] 0.01062119 Dans R, on obtient la moyenne tronquée en précisant l’argument tr (pour trimmed) de la fonction mean. mean(Mickey, tr=0.10) ## [1] 0.01062119 8.3.1.5 La moyenne winsorisée La moyenne winsorisée fonctionne sur un principe similaire à la moyenne tronquée. Elle s’en distingue par le fait que, au lieu de ne pas inclure les valeurs extrêmes dans le calcul, on ramène les valeurs tronquées aux dernières valeurs à l’intérieur des limites. D’après Wilcox (2012), il s’agit d’un indicateur plus robuste que la moyenne tronquée. Dans notre exemple, la \\(6^e\\) valeur des Mickey lorsqu’on les trie par ordre croissant est Mickey.tri[6] ## [1] 0.006517485 On va donc attribuer cette valeur aux 5 valeurs inférieures à cette valeur : Mickey.win&lt;-Mickey.tri Mickey.win[1:5] &lt;-Mickey.win[6] On fait la même chose pour les observations au-delà de la 45ème position : Mickey.win[46:50]&lt;-Mickey.win[45] Ainsi, les 6 premières d’observations ont la même valeur et les 6 dernières ont la même valeur : Mickey.win ## [1] 0.006517485 0.006517485 0.006517485 0.006517485 0.006517485 0.006517485 ## [7] 0.006842963 0.007002511 0.007182705 0.007342674 0.007501784 0.007560457 ## [13] 0.007995846 0.008194129 0.008332522 0.008468942 0.008620918 0.008822273 ## [19] 0.008848710 0.009012303 0.009511720 0.010518593 0.010824072 0.010854178 ## [25] 0.011070810 0.011091859 0.011258403 0.011266207 0.011285852 0.011333051 ## [31] 0.011503706 0.011575651 0.011758523 0.012251387 0.013035141 0.013147207 ## [37] 0.013211505 0.013212737 0.013246946 0.013629661 0.013638578 0.014052200 ## [43] 0.014186813 0.014558435 0.014578315 0.014578315 0.014578315 0.014578315 ## [49] 0.014578315 0.014578315 On peut à présent faire la moyenne sur ces valeurs mean(Mickey.win) ## [1] 0.01060654 La fonction winsor.mean du package ‘psych’ permet d’obtenir la moyenne winsorisée : library(psych) winsor.mean(Mickey.win, tr=0.10) ## [1] 0.01060654 note : on peut également calculer une variance ou un écart-type winsorisé. 8.3.1.6 Le M-estimateur de Huber9 Il s’agit d’une fonction de minimisation. Il fonctionne sur la base d’un processus itératif (comme le maximum de vraisemblance si cette notion vous parle). Huber (1981) a proposé d’utiliser cette fonction de minimisation pour estimer le paramètre d’une moyenne. \\[\\sum_{i=1}^n \\psi \\left(\\frac{X_i-t}{\\sigma} \\right)= 0 \\] où \\[ \\psi_k(x) = \\left \\{ \\begin{array}{r c l} k, &amp; x \\geq k \\\\ x &amp; -k \\geq x \\leq k \\\\ -k &amp; x \\leq -k \\end{array} \\right .\\] Si, d’un point de vue mathématique, cette fonction peut paraître un peu compliquée, on se rend compte en termes de calcul qu’il s’agit d’une opération assez simple. On commence par définir la moyenne par la médiane. Pour cette fonction, l’idée est de déterminer si, dans un ensemble de données, il existe des valeurs extrêmes et, si c’est le cas, de les remplacer par des valeurs moins extrêmes. Un des moyens pour identifier s’il y a des valeurs extrêmes est de déterminer si certaines valeurs s’éloignent de plus de 1.5 écart moyen à la médiane (mad - voir les indices de dispersion). Pour bien comprendre le fonctionnement de cette fonction, nous prendrons des données qui présentent des valeurs extrêmes. a&lt;-c(1:10, 100, 1000) a ## [1] 1 2 3 4 5 6 7 8 9 10 100 1000 La première étape est donc de déterminer un indice de tendance centrale, et cet indice est la médiane. Dans notre exemple, la médiane de a vaut : median(a) ## [1] 6.5 et le mad mad(a) ## [1] 4.4478 On se rend compte que les valeurs 100 et 1000 sont bien supérieures à 1.5 mad de la médiane. On va donc les remplacer par $médiane + 1.5 mad $ : median(a) + 1.5*mad(a) ## [1] 13.1717 On obtient donc les valeurs suivantes : yy &lt;- pmin(pmax(median(a) - 1.5 * mad(a), a), median(a) + 1.5 * mad(a)) yy ## [1] 1.0000 2.0000 3.0000 4.0000 5.0000 6.0000 7.0000 8.0000 9.0000 ## [10] 10.0000 13.1717 13.1717 On peut à présent calculer la moyenne sur ces valeurs : m&lt;-mean(yy) m ## [1] 6.778617 On va vérifier si l’estimation de notre moyenne s’écarte d’une valeur inférieure à une valeur déterminée de tolérance. Généralement, on considère qu’une tolérance acceptable est \\(10^{-6}\\). Ainsi, il ne faut pas que la moyenne s’écarte de plus de \\(10^{-6} \\times mad\\) en valeur absolue. abs(m-median(a)) &lt; 10^-6 *mad(a) ## [1] FALSE Si l’écart est supérieur, comme c’est le cas ici, on répète l’opération, mais on va comparer la prochaine moyenne estimée à la première moyenne estimée. Concrètement, yy &lt;- pmin(pmax(m - 1.5 * mad(a), a), m + 1.5 * mad(a)) yy ## [1] 1.00000 2.00000 3.00000 4.00000 5.00000 6.00000 7.00000 8.00000 ## [9] 9.00000 10.00000 13.45032 13.45032 On calcule une nouvelle moyenne : m2 &lt;- mean(yy) m2 ## [1] 6.825053 Et on vérifie si l’écart entre la première et la seconde estimation de la moyenne est inférieure à la tolérance : abs(m-m2) &lt; 10^-6 *mad(a) ## [1] FALSE Ce n’est pas le cas, on recommence l’opération yy &lt;- pmin(pmax(m2 - 1.5 * mad(a), a), m2 + 1.5 * mad(a)) yy ## [1] 1.00000 2.00000 3.00000 4.00000 5.00000 6.00000 7.00000 8.00000 ## [9] 9.00000 10.00000 13.49675 13.49675 m3 &lt;- mean(yy) m3 ## [1] 6.832792 abs(m2-m3) &lt; 10^-6 *mad(a) ## [1] FALSE Ce n’est toujours pas le cas, on va donc encore répéter l’opération jusqu’à ce notre test devienne vrai : # 4e itération yy &lt;- pmin(pmax(m3 - 1.5 * mad(a), a), m3 + 1.5 * mad(a)) m4&lt;-mean(yy) abs(m4-m3) &lt; 10^-6 *mad(a) ## [1] FALSE # 5e itération yy &lt;- pmin(pmax(m4 - 1.5 * mad(a), a), m4 + 1.5 * mad(a)) m5&lt;-mean(yy) abs(m4-m5) &lt; 10^-6 *mad(a) ## [1] FALSE # 6e itération yy &lt;- pmin(pmax(m5 - 1.5 * mad(a), a), m5 + 1.5 * mad(a)) m6&lt;-mean(yy) abs(m6-m5) &lt; 10^-6 *mad(a) ## [1] FALSE # 7e itération yy &lt;- pmin(pmax(m6 - 1.5 * mad(a), a), m6 + 1.5 * mad(a)) m7&lt;-mean(yy) abs(m6-m7) &lt; 10^-6 *mad(a) ## [1] FALSE # 8e itération yy &lt;- pmin(pmax(m7 - 1.5 * mad(a), a), m7+ 1.5 * mad(a)) m8&lt;-mean(yy) abs(m8-m7) &lt; 10^-6 *mad(a) ## [1] TRUE A la 8e itération, le critère de tolérance est accepté, nous avons donc obtenu notre M-estimateur qui vaut : m8 ## [1] 6.83434 on obtient le M-estimateur de Huber à l’aide de la fonction huber du package MASS. library(MASS) huber(a) ## $mu ## [1] 6.834339 ## ## $s ## [1] 4.4478 8.3.2 Les indices de dispersion 8.3.2.1 Minimum, maximum et étendue Le minimum est la plus petite valeur à notre disposition, tandis que le maximum est la valeur la plus élevée. min(Mickey) # minimum ## [1] 0.002816002 max(Mickey) # maximum ## [1] 0.01669808 L’étendue est la différence entre la valeur maximale et la valeur minimale max(Mickey)- min(Mickey) ## [1] 0.01388208 On obtient cette étendue en combinant la fonction diff avec la fonction range : diff(range(Mickey)) ## [1] 0.01388208 8.3.2.2 Quartile et centiles Tout comme la médiane permet de séparer 8.3.2.3 Variance 8.3.2.4 Écart-type et intervalle de confiance 8.3.2.5 Écart moyen absolu à la médiane (mad) 8.4 Les statistiques descriptives avec R Ainsi, pour rappel, il est toujours bon d’avoir installé et chargé les packages au début de votre script. Nous aurons besoin du package psych (Revelle, 2025), readxl (Wickham &amp; Bryan, 2022), ggplot2 (Wickham, 2016b) et éventuellement Hmisc (Harrell Jr, 2025) et summarytools (Comtois, 2025) Les packages que vous n’avez pas encore installés peuvent être installés avec les lignes de commande suivantes : install.packages(&quot;ggplot2&quot;) install.packages(&quot;psych&quot;) install.packages(&quot;readxl&quot;) install.packages(&quot;Hmisc&quot;) install.packages(&quot;summarytools&quot;) install.packages(&quot;grid&quot;) install.packages(&quot;titanic&quot;) Pour charger ces packages, il faut utiliser les lignes de commande suivantes : library(&quot;ggplot2&quot;) library(&quot;psych&quot;) library(&quot;readxl&quot;) library(&quot;Hmisc&quot;) library(&quot;summarytools&quot;) library(&quot;grid&quot;) library(&quot;titanic&quot;) 8.5 Traiter des données quantitatives Lorsque l’on cherche à analyser des données quantitatives, il est important de fournir des statistiques descriptives telles que la moyenne, la médiane, l’écart-type, le minimum, le maximum ou encore les quartiles. Ces statistiques permettent d’obtenir les caractéristiques principales d’une variable quantitative. La façon la plus simple et la plus rapide d’y parvenir est d’utiliser la fonction summary. Afin d’illustrer cette fonction (et les suivantes), nous utiliserons le jeu de données mtcars qui est intégré dans R. La base de données mtcars contient des informations sur 32 voitures et 11 variables, telles que la consommation de carburant par miles (mpg), le nombre de cylindres (cyl), la puissance en chevaux (hp), et le poids (wt). Pour ceux qui sont intéressés, vous pouvez obtenir le détail complet du jeu de données en utilisant la fonction ?mtcars. Les premières lignes du jeu de données sont présentées ci-dessous : .cl-1fb50ba0{}.cl-1fa5fd36{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-1fabbdde{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1fabf2e0{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1fabf2f4{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1fabf2fe{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}mpgcyldisphpdratwtqsecvsamgearcarb21.061601103.902.62016.46014421.061601103.902.87517.02014422.84108933.852.32018.61114121.462581103.083.21519.44103118.783601753.153.44017.02003218.162251052.763.46020.221031 On obtient donc les statistiques descriptives en utilisant la fonction summary de ce jeu de données de la manière suivante : data(mtcars) summary(mtcars) ## mpg cyl disp hp ## Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 ## 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 ## Median :19.20 Median :6.000 Median :196.3 Median :123.0 ## Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 ## 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 ## Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 ## drat wt qsec vs ## Min. :2.760 Min. :1.513 Min. :14.50 Min. :0.0000 ## 1st Qu.:3.080 1st Qu.:2.581 1st Qu.:16.89 1st Qu.:0.0000 ## Median :3.695 Median :3.325 Median :17.71 Median :0.0000 ## Mean :3.597 Mean :3.217 Mean :17.85 Mean :0.4375 ## 3rd Qu.:3.920 3rd Qu.:3.610 3rd Qu.:18.90 3rd Qu.:1.0000 ## Max. :4.930 Max. :5.424 Max. :22.90 Max. :1.0000 ## am gear carb ## Min. :0.0000 Min. :3.000 Min. :1.000 ## 1st Qu.:0.0000 1st Qu.:3.000 1st Qu.:2.000 ## Median :0.0000 Median :4.000 Median :2.000 ## Mean :0.4062 Mean :3.688 Mean :2.812 ## 3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.:4.000 ## Max. :1.0000 Max. :5.000 Max. :8.000 Cependant, il existe plusieurs packages R permettant d’obtenir des statistiques descriptives plus précises ou plus directement, notamment la fonction describe du package Hmisc (Harrell Jr, 2025) ou l’intégralité du package summarytools (Comtois, 2025) qui est spécifiquement dédié aux statistiques descriptives. Dans ce document, nous utiliserons principalement le package psych (Revelle, 2025) et plus particulièrement les fonctions describe et describeby qui permettent de fournir rapidement les statistiques descriptives souhaitées. Nous laissons le soin au lecteur intéressé d’explorer les autres fonctions et packages si les informations fournies par le package psych ne sont pas suffisantes. Attention : Si vous avez chargés à la fois les packages psych et hmisc, vous pourriez rencontrer un conflit car ces deux packages possèdent une fonction appelée describe. Cela peut entraîner des erreurs lors de l’exécution de votre code. Pour éviter ce problème, veillez à préciser le package dont vous souhaitez utiliser la fonction en écrivant par exemple : psych::describe ou Hmisc::describe. Cette syntaxe permet de spécifier clairement le package à utiliser dans votre code en faisant préceder la fonction describe par le nom du package suivant de deux fois deux points. desc&lt;-psych::describe(mtcars) .cl-1ffb484a{}.cl-1fed3642{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-1ff20a96{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-1ff24466{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1ff24484{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-1ff24485{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 8.1: Statistiques descriptives des caractéristiques des voitures (mtcars) varsnmeansdmediantrimmedmadminmaxrangeskewkurtosisse132.0020.096.0319.2019.705.4110.4033.9023.500.61-0.371.07232.006.191.796.006.232.974.008.004.00-0.17-1.760.32332.00230.72123.94196.30222.52140.4871.10472.00400.900.38-1.2121.91432.00146.6968.56123.00141.1977.1052.00335.00283.000.73-0.1412.12532.003.600.533.703.580.702.764.932.170.27-0.710.09632.003.220.983.333.150.771.515.423.910.42-0.020.17732.0017.851.7917.7117.831.4214.5022.908.400.370.340.32832.000.440.500.000.420.000.001.001.000.24-2.000.09932.000.410.500.000.380.000.001.001.000.36-1.920.091032.003.690.744.003.621.483.005.002.000.53-1.070.131132.002.811.622.002.651.481.008.007.001.051.260.29 Lorsque l’on execute le code dans R, on peut constater que la fonction describe du package psych fournit des statistiques descriptives pour chaque variable de la base de données mtcars. En regardant les résultats nous savons par exemple qu’il y a 32 individus et que le nombre moyen de cylindre par voiture est de 6.19. On peut également faire apparaître simplement les quantiles de nos variables dans le tableau des résultats si on veut avoir une idée de la distribution de nos données. Calcul des quantiles L’argument quant permet de spécifier les qunatiles à calculer pour chaque variable. desc.data&lt;-psych::describe(mtcars,quant=c(.1,.25,.5,.75,.90)) .cl-20547f6e{}.cl-2042bba8{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-204a161e{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-204a3a2c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-204a3a36{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-204a3a37{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 8.2: Statistiques descriptives de mtcars avec les quantiles varsnmeansdmediantrimmedmadminmaxrangeskewkurtosisseQ0.1Q0.25Q0.5Q0.75Q0.9132.0020.096.0319.2019.705.4110.4033.9023.500.61-0.371.0714.3415.4319.2022.8030.09232.006.191.796.006.232.974.008.004.00-0.17-1.760.324.004.006.008.008.00332.00230.72123.94196.30222.52140.4871.10472.00400.900.38-1.2121.9180.61120.83196.30326.00396.00432.00146.6968.56123.00141.1977.1052.00335.00283.000.73-0.1412.1266.0096.50123.00180.00243.50532.003.600.533.703.580.702.764.932.170.27-0.710.093.013.083.703.924.21632.003.220.983.333.150.771.515.423.910.42-0.020.171.962.583.333.614.05732.0017.851.7917.7117.831.4214.5022.908.400.370.340.3215.5316.8917.7118.9019.99832.000.440.500.000.420.000.001.001.000.24-2.000.090.000.000.001.001.00932.000.410.500.000.380.000.001.001.000.36-1.920.090.000.000.001.001.001032.003.690.744.003.621.483.005.002.000.53-1.070.133.003.004.004.005.001132.002.811.622.002.651.481.008.007.001.051.260.291.002.002.004.004.00 Calcul de la variance La variance de nos variables n’apparait pas dans le tableau et on doit donc ajouter une colonne que l’on va inclure dans le tableau si on désire accéder à cette information. Il suffit d’élever l’écart-type (correspondant à la colonne sd) au carré pour obtenir la variance. #Ajout de la colonne variance desc.data$variance&lt;-desc.data$sd^2 .cl-20a42ce4{}.cl-2096db98{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-209b89f4{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-209baf60{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-209baf6a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-209baf6b{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 8.3: Statistiques descriptives des caractéristiques des voitures (mtcars) - avec variance varsnmeansdmediantrimmedmadminmaxrangeskewkurtosisseQ0.1Q0.25Q0.5Q0.75Q0.9variance132.0020.096.0319.2019.705.4110.4033.9023.500.61-0.371.0714.3415.4319.2022.8030.0936.32232.006.191.796.006.232.974.008.004.00-0.17-1.760.324.004.006.008.008.003.19332.00230.72123.94196.30222.52140.4871.10472.00400.900.38-1.2121.9180.61120.83196.30326.00396.0015,360.80432.00146.6968.56123.00141.1977.1052.00335.00283.000.73-0.1412.1266.0096.50123.00180.00243.504,700.87532.003.600.533.703.580.702.764.932.170.27-0.710.093.013.083.703.924.210.29632.003.220.983.333.150.771.515.423.910.42-0.020.171.962.583.333.614.050.96732.0017.851.7917.7117.831.4214.5022.908.400.370.340.3215.5316.8917.7118.9019.993.19832.000.440.500.000.420.000.001.001.000.24-2.000.090.000.000.001.001.000.25932.000.410.500.000.380.000.001.001.000.36-1.920.090.000.000.001.001.000.251032.003.690.744.003.621.483.005.002.000.53-1.070.133.003.004.004.005.000.541132.002.811.622.002.651.481.008.007.001.051.260.291.002.002.004.004.002.61 On obtient donc une colonne supplémentaire dans la table avec la variance de chacune de nos variables. 8.5.1 Analyse en sous-groupe L’analyse en sous-groupes permet de mieux comprendre les variations au sein d’un jeu de données notamment en fonction de critères spécifiques. Par exemple, des scientifiques font souvent des hypothèses concernant des différences de moyennes entre des conditions. Analyser les données en sous-groupes permet de savoir si les résultats vont dans le sens des hypothèses, et quand ils sont significatifs, ces statistiques descriptives permettent d’interpréter le sens de l’inférence à devoir réaliser. En résumé, diviser les données en groupes permet d’identifier des tendances, des différences ou des comportements particuliers qui seraient invisibles dans une analyse globale. Cette approche permet ainsi d’obtenir des informations plus détaillées et pertinentes pour chaque groupe. La fonction describeby de psych permet à ce titre d’analyser un jeu de données en créant des sous-groupes basés sur une variable catégorielle. # La fonction describeBy permet de calculer des statistiques descriptives pour chaque sous-groupe # basé sur une variable catégorielle, ici le nombre de cylindres des voitures. desc_by_cyl &lt;- psych::describeBy(mtcars, group = mtcars$cyl) desc_by_cyl ## ## Descriptive statistics by group ## group: 4 ## vars n mean sd median trimmed mad min max range skew ## mpg 1 11 26.66 4.51 26.00 26.44 6.52 21.40 33.90 12.50 0.26 ## cyl 2 11 4.00 0.00 4.00 4.00 0.00 4.00 4.00 0.00 NaN ## disp 3 11 105.14 26.87 108.00 104.30 43.00 71.10 146.70 75.60 0.12 ## hp 4 11 82.64 20.93 91.00 82.67 32.62 52.00 113.00 61.00 0.01 ## drat 5 11 4.07 0.37 4.08 4.02 0.34 3.69 4.93 1.24 1.00 ## wt 6 11 2.29 0.57 2.20 2.27 0.54 1.51 3.19 1.68 0.30 ## qsec 7 11 19.14 1.68 18.90 18.99 1.48 16.70 22.90 6.20 0.55 ## vs 8 11 0.91 0.30 1.00 1.00 0.00 0.00 1.00 1.00 -2.47 ## am 9 11 0.73 0.47 1.00 0.78 0.00 0.00 1.00 1.00 -0.88 ## gear 10 11 4.09 0.54 4.00 4.11 0.00 3.00 5.00 2.00 0.11 ## carb 11 11 1.55 0.52 2.00 1.56 0.00 1.00 2.00 1.00 -0.16 ## kurtosis se ## mpg -1.65 1.36 ## cyl NaN 0.00 ## disp -1.64 8.10 ## hp -1.71 6.31 ## drat 0.12 0.11 ## wt -1.36 0.17 ## qsec -0.02 0.51 ## vs 4.52 0.09 ## am -1.31 0.14 ## gear -0.01 0.16 ## carb -2.15 0.16 ## ------------------------------------------------------------ ## group: 6 ## vars n mean sd median trimmed mad min max range skew ## mpg 1 7 19.74 1.45 19.70 19.74 1.93 17.80 21.40 3.60 -0.16 ## cyl 2 7 6.00 0.00 6.00 6.00 0.00 6.00 6.00 0.00 NaN ## disp 3 7 183.31 41.56 167.60 183.31 11.27 145.00 258.00 113.00 0.80 ## hp 4 7 122.29 24.26 110.00 122.29 7.41 105.00 175.00 70.00 1.36 ## drat 5 7 3.59 0.48 3.90 3.59 0.03 2.76 3.92 1.16 -0.74 ## wt 6 7 3.12 0.36 3.21 3.12 0.36 2.62 3.46 0.84 -0.22 ## qsec 7 7 17.98 1.71 18.30 17.98 1.90 15.50 20.22 4.72 -0.12 ## vs 8 7 0.57 0.53 1.00 0.57 0.00 0.00 1.00 1.00 -0.23 ## am 9 7 0.43 0.53 0.00 0.43 0.00 0.00 1.00 1.00 0.23 ## gear 10 7 3.86 0.69 4.00 3.86 0.00 3.00 5.00 2.00 0.11 ## carb 11 7 3.43 1.81 4.00 3.43 0.00 1.00 6.00 5.00 -0.26 ## kurtosis se ## mpg -1.91 0.55 ## cyl NaN 0.00 ## disp -1.23 15.71 ## hp 0.25 9.17 ## drat -1.40 0.18 ## wt -1.98 0.13 ## qsec -1.75 0.65 ## vs -2.20 0.20 ## am -2.20 0.20 ## gear -1.24 0.26 ## carb -1.50 0.69 ## ------------------------------------------------------------ ## group: 8 ## vars n mean sd median trimmed mad min max range skew ## mpg 1 14 15.10 2.56 15.20 15.15 1.56 10.40 19.20 8.80 -0.36 ## cyl 2 14 8.00 0.00 8.00 8.00 0.00 8.00 8.00 0.00 NaN ## disp 3 14 353.10 67.77 350.50 349.63 73.39 275.80 472.00 196.20 0.45 ## hp 4 14 209.21 50.98 192.50 203.67 44.48 150.00 335.00 185.00 0.91 ## drat 5 14 3.23 0.37 3.12 3.19 0.16 2.76 4.22 1.46 1.34 ## wt 6 14 4.00 0.76 3.76 3.95 0.41 3.17 5.42 2.25 0.99 ## qsec 7 14 16.77 1.20 17.18 16.86 0.79 14.50 18.00 3.50 -0.80 ## vs 8 14 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 NaN ## am 9 14 0.14 0.36 0.00 0.08 0.00 0.00 1.00 1.00 1.83 ## gear 10 14 3.29 0.73 3.00 3.17 0.00 3.00 5.00 2.00 1.83 ## carb 11 14 3.50 1.56 3.50 3.25 0.74 2.00 8.00 6.00 1.48 ## kurtosis se ## mpg -0.57 0.68 ## cyl NaN 0.00 ## disp -1.26 18.11 ## hp 0.09 13.62 ## drat 1.08 0.10 ## wt -0.71 0.20 ## qsec -0.92 0.32 ## vs NaN 0.00 ## am 1.45 0.10 ## gear 1.45 0.19 ## carb 2.24 0.42 Cela nous donne des statistiques descriptives détaillées pour les voitures à 4, 6 et 8 cylindres. Cependant, l’analyse des résultats peut être complexe comme on le voit avec la sortie du code, car ces sous-groupes génèrent beaucoup de valeurs à interpréter. Il peut donc être difficile de tirer des conclusions rapidement si l’on regarde toutes les statistiques en même temps et les tableaux peuvent être difficile à lire simultanément. À ce titre, nous allons nous intéresser spécifiquement aux voitures à 4 cylindres, ce qui nous permettra de simplifier l’interprétation des données en nous concentrant sur un sous-groupe particulier.Les résultats sont présentés dans le Tableau @(tab:cylinde) # Ici, nous sélectionnons les statistiques spécifiques aux voitures à 4 cylindres # parmi les résultats obtenus pour tous les sous-groupes de cylindres. desc_4_cyl &lt;- desc_by_cyl[[&quot;4&quot;]] Ce code extrait uniquement les statistiques descriptives pour les voitures à 4 cylindres à partir des résultats complets obtenus précédemment avec la fonction describeBy. .cl-20e77986{}.cl-20dbab42{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-20dfcfba{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-20dfec02{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-20dfec0c{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-20dfec16{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 8.4: Statistiques descriptives des voitures à 4 cylindres. varsnmeansdmediantrimmedmadminmaxrangeskewkurtosisse111.0026.664.5126.0026.446.5221.4033.9012.500.26-1.651.36211.004.000.004.004.000.004.004.000.000.00311.00105.1426.87108.00104.3043.0071.10146.7075.600.12-1.648.10411.0082.6420.9391.0082.6732.6252.00113.0061.000.01-1.716.31511.004.070.374.084.020.343.694.931.241.000.120.11611.002.290.572.202.270.541.513.191.680.30-1.360.17711.0019.141.6818.9018.991.4816.7022.906.200.55-0.020.51811.000.910.301.001.000.000.001.001.00-2.474.520.09911.000.730.471.000.780.000.001.001.00-0.88-1.310.141011.004.090.544.004.110.003.005.002.000.11-0.010.161111.001.550.522.001.560.001.002.001.00-0.16-2.150.16 8.6 Faire une représentation graphique des données Lorsque l’on on mène une analyse descriptive de données sur R, on peut s’aider à l’aide de graphiques, ce qui permet de mieux visualiser leur distribution, leur tendance et les relations entre les variables. Pour cela, le package ggplot2 (Wickham, 2016b) est largement utilisé. Ce package permet de créer une variété de graphiques personnalisés, comme des histogrammes, des graphiques en violon ou des diagrammes de dispersion, facilitant ainsi l’analyse et l’interprétation des données. 8.6.1 Graphique violon Le graphique en violon (violin plot) est une excellente optioin pour visualiser des données quantitatives. Il permet de représenter la distribution des valeurs d’une variable en montrant à la fois la densité et les principales statistiques descriptives. Le package ggplot2 (Wickham, 2016b), avec sa fonction ggplot permet de crée assez aisément un graphique en violon. Pour faciliter la lisibilité, les lignes de commande seront annotées au sein du code. Dans l’exemple suivant, on va représente la consommation de carburant en miles par gallon (mpg). L’axe des ordonnées représente la consommation et l’axe des abscisses est fixé à une valeur constante pour afficher une seule distribution. Le violon montre la densité de la variable, et un point rouge indique la moyenne de la consommation, accompagné d’une barre bleue représentant l’intervalle de confiance autour de cette moyenne. ggplot(data = mtcars, aes(x = 1, y = mpg)) + # data = le jeu de données où on va trouver les variables d&#39;intérêt, mpg en l&#39;occurrence # x = 1 : L&#39;axe des x est constant, ce qui fait que tous les violons seront positionnés sur la même ligne # y = mpg : L&#39;axe des y représente la consommation (miles per gallon) geom_violin(fill = &quot;lightblue&quot;, trim = FALSE) + # fill = &quot;lightblue&quot; : Remplissage uniforme pour le violon avec la couleur &#39;lightblue&#39; stat_summary(fun = mean, geom = &#39;point&#39;, color = &quot;red&quot;) + # Affiche la moyenne de la consommation comme un point rouge stat_summary(fun.data = &#39;mean_sdl&#39;, geom = &quot;errorbar&quot;, color = &quot;blue&quot;) + # Affiche l&#39;intervalle de confiance autour de la moyenne en bleu xlab(&quot; &quot;) + # Aucun titre pour l&#39;axe des x, car il n&#39;y a qu&#39;une seule catégorie ylab(&quot;Consommation (mpg)&quot;) + # Titre de l&#39;axe des y, indiquant la consommation en miles par gallon theme_light() + # Thème léger pour le fond et le style du graphique theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank()) Figure 8.1: Distribution de la consommation de carburant par miles (mpg) # Supprime l&#39;axe des x, car il n&#39;est pas nécessaire ici On peut également utiliser ce type de graphique pour comparer la distribution d’une variable continue en fonction d’une variable catégorielle. Par exemple, dans le code suivant, la variable catégorielle cyl (le nombre de cylindres) est utilisée pour créer des violons séparés pour chaque groupe de cylindres, permettant de comparer les distributions de la consommation pour chaque catégorie de cylindres. # la fonction ggplot permet de faire le graphique # data = mtcars : On utilise le jeu de données mtcars # aes(x = factor(cyl), y = mpg, fill = factor(cyl)) : # - x = factor(cyl) : L&#39;axe des x représente le nombre de cylindres (converti en facteur pour chaque groupe) # - y = mpg : L&#39;axe des y représente la consommation (miles per gallon) # - fill = factor(cyl) : On colore les violons en fonction du nombre de cylindres ggplot(data = mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) + # geom_violin() : Crée un graphique en violon pour montrer la distribution des données # trim = FALSE : On ne découpe pas les bords des violons, laissant toute la distribution visible geom_violin(trim = FALSE) + # stat_summary() : Ajoute une statistique résumée au graphique # fun = mean : On veut afficher la moyenne de chaque groupe (cylindres) comme un point sur les violons. # geom = &#39;point&#39; : Le résumé est représenté par un point stat_summary(fun = mean, geom = &#39;point&#39;) + # stat_summary(fun.data = &#39;mean_sdl&#39;) : Affiche une statistique avec l&#39;intervalle de confiance autour de la moyenne # fun.data = &#39;mean_sdl&#39; : Affiche la moyenne et l&#39;écart-type comme une barre d&#39;erreur autour du point moyen stat_summary(fun.data = &#39;mean_sdl&#39;) + # xlab(&quot;Cylindres&quot;) : Ajoute un titre à l&#39;axe des x, indiquant que l&#39;on mesure les cylindres xlab(&quot;Cylindres&quot;) + # ylab(&quot;Consommation&quot;) : Ajoute un titre à l&#39;axe des y, indiquant que l&#39;on mesure la consommation de carburant par miles (mpg) ylab(&quot;Consommation (mpg)&quot;) + # theme_light() : Applique un thème clair pour le fond et le style du graphique theme_light() Figure 8.2: Distribution de la consommation de carburant par miles (mpg) en fonction du nombre de cylindres (cyl) On observe que chaque violon correspond à une catégorie de cylindre (4,6 ou 8 cylindres) et montre la densité des valeurs de consommation à l’intérieur de cette catégorie. Avec cet exemple on peut dire que les véhicules à 4 cylindres ont une consommation de carburant généralement moins élevée,cela montre qu’elles sont plus économes en carburant tandis que les véhicules à 8 cylindres ont une consommation plus élevée. Les véhicules à 6 cylindres se situent entre ces deux extrêmes. Lire un graphique en violon (violin plot) Dans cet exemple, nous allons expliquer comment lire un graphique en violon en utilisant un exemple de la densité de la loi normale. Le graphique ci-dessous montre la densité de la distribution normale générée avec une moyenne de 50 et un écart-type de 10. Un graphique en violon représente la distribution des données en combinant des éléments d’un diagramme en boîte et la visualisation de la densité des données. Cette visualisation permet de montrer non seulement la tendance centrale (moyenne ou médiane), mais aussi la forme de la distribution et sa dispersion. Cela ressemble à la courbe de densité d’une loi normale, comme celle présentée ici par la courbe rouge. 8.7 Pourquoi utiliser la loi normale pour comprendre les graphiques en violon ? Les graphiques en violon sont souvent utilisés pour visualiser la distribution des données. Cependant, ces graphiques peuvent être difficiles à interpréter sans repères. C’est pourquoi nous utilisons la loi normale (ou distribution gaussienne) comme référence. La loi normale est l’une des distributions les plus courantes en statistiques et possède une forme bien définie et symétrique. En comparant la forme du graphique en violon à celle d’une loi normale, nous pouvons mieux comprendre comment les données se répartissent, identifier les éventuels écarts et repérer les asymétries ou anomalies dans la distribution. En d’autres termes, la courbe de densité de la loi normale nous aide à avoir un repère visuel pour interpréter les graphiques en violon. Dans le graphique en violon, cette densité est simplement représentée dans un axe à 90° par rapport à cette distribution. Figure 8.3: Représentation de la densité d’une distribution normale telle qu’on peut la rencontrer dans un graphe violon. Les graphiques en violon sont souvent présentés avec une orientation horizontale, comme c’est le cas avec le graphique ci-dessus. Cela permet de mieux visualiser la densité des données sur un axe horizontal, ce qui correspond à l’orientation habituelle des graphiques en violon. Exercice Dansla base de données mickey2, on trouve les caractéristiques de souris d’ordinateur. Veuillez répondre aux questions proposées. Utilisez la fonction describe ou bien la fonction describeBy du package psych (Revelle, 2025) pour obtenir des statistiques descriptives sur la variable Prix de notre jeu de données, tout en calculant également la variance. De plus, nous voudrions les quantiles .10, .25, .75, .90 de cette variable. Par ailleurs, on voudrait également les statistiques descriptives de la variable prix en fonction des dpi des soucis. réaliser une analyse descriptive des variables, réaliser une analyse en sous-groupe avec la variable de votre choix dans la base de données mickey2 et réalisez au moins un graphique avec ggplot2. Cliquez pour la solution Nous devons dans un premier temps importer le jeu de données : Mickey&lt;-readxl::read_xlsx(&quot;./statdesc/mickey2.xlsx&quot;) À présent, nous allons définir une fonction appelée Prix, qui extraira spécifiquement la variable Prix du jeu de données Mickey. Cette fonction nous permettra de simplifier l’analyse en nous concentrant uniquement sur cette variable, afin d’explorer ses statistiques descriptives et d’identifier ses principales caractéristiques (Tableau 8.5). .cl-222978bc{}.cl-221cf0ec{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-22217a68{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-22219b74{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-22219b88{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-22219b92{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 8.5: Presentation du jeu de données mickey sourismickeyPoidsPrixDPIhz1.000.0185.0045.99800.00125.002.000.01120.0089.501,200.00500.003.000.0195.0019.991,600.001,000.004.000.0175.00129.00800.00125.005.000.01100.0059.992,400.00500.006.000.02110.0075.001,600.001,000.007.000.0190.00105.991,000.00125.008.000.01105.0089.901,200.00500.009.000.0180.0029.991,600.001,000.0010.000.01130.0049.002,000.00500.0011.000.01115.0085.00800.001,000.0012.000.0195.00120.002,400.00125.0013.000.01125.0095.503,200.00500.0014.000.0185.0060.00800.00125.0015.000.01110.00119.991,800.001,000.0016.000.00100.0050.001,200.00500.0017.000.01120.00110.001,000.00125.0018.000.02105.0069.992,400.001,000.0019.000.01115.0079.993,200.00500.0020.000.0195.0085.991,600.00125.0021.000.0180.00109.001,000.001,000.0022.000.01110.0059.99800.00125.0023.000.01130.0099.001,600.00500.0024.000.01120.0055.504,000.001,000.0025.000.01100.0089.995,000.00500.0026.000.0190.00120.001,000.001,000.0027.000.01125.0079.001,200.00125.0028.000.01105.00135.002,000.00500.0029.000.01115.0065.99800.00125.0030.000.01110.0079.991,600.001,000.0031.000.0195.0049.992,400.00125.0032.000.0085.0089.003,200.001,000.0033.000.01100.00120.501,800.00500.0034.000.01110.00100.002,400.00500.0035.000.01120.0069.004,000.00125.0036.000.0190.00110.991,000.001,000.0037.000.01105.0045.001,800.00125.0038.000.01115.0080.991,600.00500.0039.000.01100.00125.001,200.001,000.0040.000.01110.0090.002,000.00125.0041.000.01130.0075.993,200.00500.0042.000.01115.00120.001,600.001,000.0043.000.0185.0060.002,400.00125.0044.000.01100.0069.993,200.00500.0045.000.01125.00115.001,600.00125.0046.000.0195.0099.991,800.001,000.0047.000.01120.00130.002,400.00125.0048.000.01105.0060.991,600.001,000.0049.000.01110.0060.00800.00500.0050.000.0290.0045.004,000.00500.00 Pour cela, nous allons utiliser le package psych et sa fonction describe, en appliquant ces outils directement à la fonction Prix, qui extrait cette variable du jeu de données Mickey. Mickeymouse &lt;- psych::describe(Mickey$Prix, quant = c(.1, .25, .75, .90)) Mickeymouse$variance &lt;- Mickeymouse$sd^2 Mickeymouse_df &lt;- as.data.frame(Mickeymouse) On obtient le résultat suivant : .cl-22562e0c{}.cl-224d61e6{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-225091b8{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-2250a9e6{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2250a9e7{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 8.6: Statistiques descriptives de la variable prix varsnmeansdmediantrimmedmadminmaxrangeskewkurtosisseQ0.1Q0.25Q0.75Q0.9variance1.0050.0083.6328.5283.0083.9134.0919.99135.00115.01-0.05-0.934.0348.7060.00108.25120.05813.40 On constate que la moyenne des prix est de 83.63, ce qui reflète le prix moyen dans le jeu de données. L’écart-type vaut 28.52, avec des prix allant de 19.99 euros à 135.00 euros. Si on veut effectuer une analyse en sous-groupes, il suffit d’utiliser la même logique que celle que nous avons employée ici. Imaginons que l’on veuille des statistiques descriptives concernant la variable DPI , alors nous pouvons utiliser la fonction describeby de psych. desc_by_dpi &lt;- psych::describeBy(Mickey[,c( &quot;Prix&quot;)], group = Mickey$DPI, mat=T) On obtient nos statistiques descriptives pour chaque sous-groupe de DPI. Tableau . Statistiques descriptives des variables Poids,Prix et hz .cl-228aad12{}.cl-2280257c{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-2283904a{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-2283905e{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-2283ab34{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2283ab35{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2283ab3e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2283ab48{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2283ab49{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2283ab52{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}column nameitemgroup1varsnmeansdmediantrimmedmadminmaxrangeskewkurtosissePrix118001.007.0072.2827.5860.0072.288.8845.99129.0083.011.09-0.3010.42Prix2210001.005.00111.205.27110.00111.201.48105.99120.0014.010.72-1.242.35Prix3312001.005.0086.6826.8889.5086.6815.5750.00125.0075.000.07-1.4812.02Prix4416001.0010.0076.6932.6680.4978.3728.1819.99120.00100.01-0.40-1.1510.33Prix5518001.004.0096.3735.55109.9996.3715.2045.00120.5075.50-0.60-1.8117.78Prix6620001.003.0091.3343.0290.0091.3360.7949.00135.0086.000.03-2.3324.84Prix7724001.007.0084.2832.0869.9984.2829.6549.99130.0080.010.32-1.9012.12Prix8832001.005.0082.0910.1979.9982.0913.3669.9995.5025.510.14-1.944.56Prix9940001.003.0056.5012.0355.5056.5015.5745.0069.0024.000.08-2.336.95Prix101050001.001.0089.9989.9989.990.0089.9989.990.00 Comme vu précedemment, on peut également représenter nos données avec un graphique afin d’obtenir un rendu plus clair et visible. Figure 8.4: Distribution des prix en fonction des DPI de la souris souris On voit que les prix ne semblent pas vraiment dépendre du DPI de la souris. 8.8 Traiter des données qualitatives Nous avons vu que la manière la plus efficace pour décrire les données qualitatives est de fournir la table des effectifs. Il s’agit du nombre d’observations pour chacune des modalités. Si la fonction summary permet de le faire, il semble plus pertinent d’utiliser la fonction table ou ftable en fonction du nombre de variables à prendre en considération Pour illustrer la manière dont fonctionnent ces fonctions, nous pouvons aussi utiliser un jeu de données intégré à R : le jeu de données titanic_train dans le package titanic (Hendricks, 2015). library(titanic) data(titanic_train) Voici les premières lignes du jeu de données : Tableau . Statistiques descriptives du jeu de données titanic .cl-2331320e{}.cl-2325237e{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-2329cda2{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-2329cdac{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-2329fe12{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2329fe26{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2329fe27{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2329fe30{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2329fe3a{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-2329fe3b{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}PassengerIdSurvivedPclassNameSexAgeSibSpParchTicketFareCabinEmbarked103Braund, Mr. Owen Harrismale22.0010A/5 211717.25S211Cumings, Mrs. John Bradley (Florence Briggs Thayer)female38.0010PC 1759971.28C85C313Heikkinen, Miss. Lainafemale26.0000STON/O2. 31012827.92S411Futrelle, Mrs. Jacques Heath (Lily May Peel)female35.001011380353.10C123S503Allen, Mr. William Henrymale35.00003734508.05S603Moran, Mr. Jamesmale003308778.46Q On peut commencer par se demander combien de personnes étaient dans chacune des classes de passagers : table(titanic_train$Pclass) ## ## 1 2 3 ## 216 184 491 On voit qu’il y a 216 personnes en première classe, 184 en deuxième et 491 personnes en troisième classe. Si on se demande dans quelle mesure, le fait d’être sauvé dépend de la classe, nous pouvons croiser les deux variables et faire un tableau à doubles entrées. table(titanic_train$Pclass, titanic_train$Survived) ## ## 0 1 ## 1 80 136 ## 2 97 87 ## 3 372 119 Nous nous rendons compte de deux phénomènes : l’absence de nom aux dimensions de la table la rend difficile à lire ; il est difficile d’apprécier la survie Nous pouvons corriger ces deux défauts en utilisant l’argument dnn et la fonction addmargins. L’argument dnn permet de donner un nom aux dimensions. La première information est le nom de la variable pour les lignes et la seconde est le nom des variables pour les colonnes. tab&lt;-table(titanic_train$Pclass, titanic_train$Survived, dnn=list(&quot;classe&quot;, &quot;Survie&quot;)) tab&lt;-addmargins(tab) tab ## Survie ## classe 0 1 Sum ## 1 80 136 216 ## 2 97 87 184 ## 3 372 119 491 ## Sum 549 342 891 Pour rendre les choses encore plus claires, connaitre la fréquence pour chaque modalité serait un plus. On peut calculer les fréquences sur les effectifs totaux en divisant la table par la somme des effectifs : tab&lt;-round(tab/sum(tab), 3) tab ## Survie ## classe 0 1 Sum ## 1 0.022 0.038 0.061 ## 2 0.027 0.024 0.052 ## 3 0.104 0.033 0.138 ## Sum 0.154 0.096 0.250 Dans certains cas, il est souhaitable d’obtenir ces informations par ligne ou par colonne. A nouveau, R permet d’obtenir ces informations mais l’obtention de ces informations est techniquement un peu plus compliqué. Pour traduire la fonction ci-dessous : on va retourner une nouvelle table grâce à la fonction sweep. Cette nouvelle table contiendra de nouvelles marges grâce à la fonction addmargins. On va ajouter ces marges à l’objet “tab”, sur la première dimension de la table (donc on ajoute des lignes) et dans ses marges, nous souhaitons les totaux de chaque colonne (en ajoutant des lignes), qui sera intitulé “N”. On fera également, grâce à la fonction sum, une somme des différentes lignes (aboutissant donc à 100% dans chaque colonne). Enfin, pour les pourcentages, on va utiliser la fonction apply sur la seconde dimension (le 2) et en divisant les totaux divisés par 100. Si cette fonction vous semble compliquée, vous pouvez simplement vous contenter de comprendre que vous devez donner le nom de la table, ‘tab’, dans mon exemple et garder le reste sans changement. round( sweep( addmargins(tab, 1, list(list(All = sum, N = function(x) sum(x)^2/100))), 2,apply(tab, 2, sum)/100, &quot;/&quot;), 1) ## Survie ## classe 0 1 Sum ## 1 7.2 19.9 12.2 ## 2 8.8 12.6 10.4 ## 3 33.9 17.3 27.5 ## Sum 50.2 50.3 49.9 ## All 100.0 100.0 100.0 ## N 0.3 0.2 0.5 Avec cette même logique, on peut obtenir les pourcentages par ligne. round( sweep( addmargins( tab, 2, list(list(All = sum, N = function(x) sum(x)^2/100))), 1,apply(tab, 1, sum)/100, &quot;/&quot;), 1) ## Survie ## classe 0 1 Sum All N ## 1 18.2 31.4 50.4 100.0 0.1 ## 2 26.2 23.3 50.5 100.0 0.1 ## 3 37.8 12.0 50.2 100.0 0.3 ## Sum 30.8 19.2 50.0 100.0 0.5 Dans ce tableau, on constate beaucoup plus aisément qu’on avait plus de chance de survivre si on avait un ticket en \\(1^e\\) qu’en \\(3^e\\) classe. Lorsqu’on doit faire des tables plus complexes, impliquant plus de deux variables, on peut utiliser la fonction ftable. L’utilisation de la fonction est assez simple : il suffit de donner à R un ensemble de variables qu’il peut considérer comme des facteurs. Le plus simple est d’utiliser un data.frame et de préciser le nom des variables à inclure. Ainsi, si on veut regarder si le croisement entre le pont d’embarquement et le sexe a eu un impact sur la survie en fonction de la classe, on peut faire : ftable(titanic_train[,c(&quot;Survived&quot;, &quot;Pclass&quot;, &quot;Sex&quot;,&quot;Embarked&quot;)]) ## Embarked C Q S ## Survived Pclass Sex ## 0 1 female 0 1 0 2 ## male 0 25 1 51 ## 2 female 0 0 0 6 ## male 0 8 1 82 ## 3 female 0 8 9 55 ## male 0 33 36 231 ## 1 1 female 2 42 1 46 ## male 0 17 0 28 ## 2 female 0 7 2 61 ## male 0 2 0 15 ## 3 female 0 15 24 33 ## male 0 10 3 34 Notez qu’il est possible de modifier l’apparence de la table en choisissant les variables qui doivent être présentées sur les lignes et celles qu’il faut présenter sur les colonnes en précisant les arguments row.vars et col.vars. tab&lt;-ftable(titanic_train[,c(&quot;Survived&quot;, &quot;Pclass&quot;, &quot;Sex&quot;,&quot;Embarked&quot;)], row.vars =c(&quot;Pclass&quot;, &quot;Embarked&quot;) , col.vars =c(&quot;Survived&quot;, &quot;Sex&quot;)) tab ## Survived 0 1 ## Sex female male female male ## Pclass Embarked ## 1 0 0 2 0 ## C 1 25 42 17 ## Q 0 1 1 0 ## S 2 51 46 28 ## 2 0 0 0 0 ## C 0 8 7 2 ## Q 0 1 2 0 ## S 6 82 61 15 ## 3 0 0 0 0 ## C 8 33 15 10 ## Q 9 36 24 3 ## S 55 231 33 34 Exercice Cet exercice vous permettra de mettre en pratique les outils de statistiques descriptives sur des données qualitatives et d’analyse de tableaux croisés en R. Vous explorerez les relations entre les variables qualitatives du jeu de données Titanic en manipulant des tableaux de fréquences avec la fonction table et en ajoutant des totaux grâce à addmargins. Vous calculerez également des pourcentages à l’aide des fonctions sweep et apply, avant d’utiliser ftable pour analyser les interactions entre plusieurs variables, notamment le sexe, l’âge catégorisé, et la survie , vous êtes libres de chosir les variables à analyser. Enfin, vous serez amené à interpréter les résultats obtenus pour tirer des conclusions sur les tendances observées. Questions : Comptez le nombre de passagers en fonction du sexe. la survie est-elle la même entre les hommes et les femmes ? Ajoutez des totaux pour chaque catégorie dans le tableau croisé entre sexe et survie. Transformez les comptages en pourcentages pour visualiser les proportions de survie. Créez des groupes d’âge (enfants, jeunes adultes, adultes, seniors) et analysez la survie au sein de chaque groupe. Cliquez pour la solution Pour débuter, il est indispensable d’importer la base de données titanic_train nécessaire à la réalisation de l’exercice. Il est probable que le jeu de données soit déjà installé sur votre session mais si toutefois ça n’est pas le cas , vous pouvez utiliser la fonction data pour l’importer directement dans votre environnement de travail. data(&quot;titanic_train&quot;) Si le lecteur désire plus d’informations concernant le jeu de données titanic_train , il peut utiliser la commande ?titanic_train pour avoir le détail des informations concernant cette dernière. Commençons par examiner combien de passagers étaient des hommes et combien étaient des femmes dans le jeu de données. Pour ça on utilise la fonction table sur la variable Sex. table(titanic_train$Sex) ## ## female male ## 314 577 On voit que parmis les passagers, il y a 314 femmes et 577 hommes. Ensuite, analysons comment la survie varie selon le sexe. Nous croisons les variables Sex et Survived à l’aide d’un tableau à doubles entrées pour savoir si la survie est similaire entre les sexes. table(titanic_train$Sex, titanic_train$Survived) ## ## 0 1 ## female 81 233 ## male 468 109 Le tableau croisé montre le nombre de survivants (1) et de non-survivants (0) selon le sexe. Pour les femmes, 233 ont survécu et 81 n’ont pas survécu. Pour les hommes, 109 ont survécu et 468 n’ont pas survécu. Il semble donc, a priori, qu’il y ait plus de femmes à avoir survécu que d’hommes. Néanmoins, le tableau précédent n’est pas très lisible. Donnons des noms aux dimensions (colonnes et lignes) et ajoutons des marges pour avoir les totaux. Cela nous permettra d’interpréter plus facilement les résultats. tab &lt;- table(titanic_train$Sex, titanic_train$Survived, dnn = list(&quot;Sexe&quot;, &quot;Survie&quot;)) tab &lt;- addmargins(tab) tab ## Survie ## Sexe 0 1 Sum ## female 81 233 314 ## male 468 109 577 ## Sum 549 342 891 Pour mieux comprendre les proportions, transformons le tableau précédent en fréquences en divisant chaque cellule par le total des effectifs. tab &lt;- round(tab / sum(tab), 3) tab ## Survie ## Sexe 0 1 Sum ## female 0.023 0.065 0.088 ## male 0.131 0.031 0.162 ## Sum 0.154 0.096 0.250 Maintenant, voyons comment les pourcentages de survie varient par ligne (par sexe). Cette étape est utile pour analyser les chances de survie selon chaque groupe. round(sweep(addmargins(tab, 2, list(list(All = sum, N = function(x) sum(x)^2 / 100))), 1, apply(tab, 1, sum) / 100, &quot;/&quot;), 1) ## Survie ## Sexe 0 1 Sum All N ## female 13.1 36.9 50.0 100.0 0.2 ## male 40.4 9.6 50.0 100.0 0.3 ## Sum 30.8 19.2 50.0 100.0 0.5 Nous allons maintenant créer une nouvelle variable, AgeGroup, pour catégoriser l’âge en groupes : enfants, jeunes adultes, adultes et seniors. Une fois la variable créée, examinons si l’âge a eu un impact sur la survie des passagers. titanic_train$AgeGroup &lt;- cut(titanic_train$Age, breaks = c(0, 18, 35, 50, 100), labels = c(&quot;Enfant&quot;, &quot;Jeune adulte&quot;, &quot;Adulte&quot;, &quot;Senior&quot;)) table(titanic_train$AgeGroup, titanic_train$Survived) ## ## 0 1 ## Enfant 69 70 ## Jeune adulte 221 137 ## Adulte 92 61 ## Senior 42 22 Utiliser la fonction cut La fonction cut permet de diviser une variable continue en intervalles définis. Dans cet exemple, elle est utilisée pour créer une nouvelle variable AgeGroup en catégorisant les âges des passagers en quatre groupes : “Enfant” pour les passagers de 0 à 18 ans, “Jeune adulte” de 18 à 35 ans, “Adulte” de 35 à 50 ans, et “Senior” de 50 à 100 ans. Les intervalles sont définis avec breaks et les catégories sont nommées avec labels. Cette fonction transforme ainsi une variable numérique en une variable catégorielle et vous pouvez l’utiliser dans vos lignes de code. Enfin, on peut combiner les variables AgeGroup, Sex et Survived pour examiner leur interaction. Pour ça on peut utiliser la fonction ftable pour obtenir un tableau lisible et multidimensionnel. ftable(titanic_train[, c(&quot;Survived&quot;, &quot;AgeGroup&quot;, &quot;Sex&quot;)]) ## Sex female male ## Survived AgeGroup ## 0 Enfant 22 47 ## Jeune adulte 26 195 ## Adulte 15 77 ## Senior 1 41 ## 1 Enfant 46 24 ## Jeune adulte 94 43 ## Adulte 41 20 ## Senior 16 6 Le tableau montre que les femmes, surtout les enfants et les seniors, avaient des chances de survie nettement plus élevées que les hommes. Les enfants, indépendamment du sexe, avaient de meilleures chances, confirmant les priorités “femmes et enfants d’abord”. En revanche, les hommes jeunes et adultes ont eu les taux de survie les plus faibles, reflétant une vulnérabilité accrue dans ces groupes. 8.9 Statistiques descriptives avec easieR Pour réaliser des statistiques descriptives avec easieR (Stefaniak, 2018), il faut avoir au préalable avoir chargé le package easieR et lancé easieR avec la fonction easieR(). La seconde étape consiste à importer les données. Références Comtois, D. (2025). Summarytools: Tools to quickly and neatly summarize data. https://doi.org/10.32614/CRAN.package.summarytools Harrell Jr, F. E. (2025). Hmisc: Harrell miscellaneous. https://doi.org/10.32614/CRAN.package.Hmisc Hendricks, P. (2015). Titanic: Titanic passenger survival data set. https://doi.org/10.32614/CRAN.package.titanic Huber, P. J. (1981). Robust statistics. John Wiley &amp; Sons. Popper, K. R. (1959). The logic of scientific discovery. Hutchinson. Revelle, W. (2025). Psych: Procedures for psychological, psychometric, and personality research. Northwestern University. https://CRAN.R-project.org/package=psych Stefaniak, N. (2018). easieR: easieR: A GUI r metapackage. https://github.com/NicolasStefaniak/easieR Wickham, H. (2016a). ggplot2: Elegant graphics for data analysis. Springer-Verlag. https://ggplot2.tidyverse.org Wickham, H. (2016b). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org Wickham, H., &amp; Bryan, J. (2022). Readxl: Read excel files. https://CRAN.R-project.org/package=readxl Wilcox, R. R. (2012). Introduction to robust estimation and hypothesis testing (3rd ed.). Academic Press. cet indicateur requiert un niveau de maîtrise des statistiques plus avancé. Vous pouvez le passer si vous débutez.↩︎ "],["les-différentes-méthodes-dinférences-statistiques.html", "Chapter 9 Les différentes méthodes d’inférences statistiques 9.1 Introduction 9.2 L’approche de Neyman-Pearson 9.3 Lien entre la taille d’effet et la significativité 9.4 Quand c’est encore moins évident? 9.5 En synthèse 9.6 L’approche par maximum de vraisemblance. 9.7 L’approche bayesienne", " Chapter 9 Les différentes méthodes d’inférences statistiques “Le statisticien ne peut pas se soustraire à l’obligation d’être au clair quant aux principes de l’inférence scientifique, mais de même, aucune autre personne censée ne peut se soustraire à une telle obligation.” — Sir Ronald Fisher .col2 { columns: 2 200px; /* number of columns and width in pixels*/ -webkit-columns: 2 200px; /* chrome, safari */ -moz-columns: 2 200px; /* firefox */ } .col3 { columns: 3 100px; -webkit-columns: 3 100px; -moz-columns: 3 100px; } Résumé Dans ce chapitre nous aborderons les trois principales écoles de pensées pour l’inférence statistique : l’approche de Neyman-Pearson l’approche de vraisemblance l’approche bayésienne Nous aborderons les tenants et les aboutissants de ces différents méthodes, leurs avantages et leurs inconvénients. L’objectif est de permettre au lecteur d’avoir les notions indispensables pour lire, comprendre ou réaliser des analsyes statistiques selon n’importe laquelle des écoles d’inférence, et de pouvoir développer un regard critique sur chacune de ces écoles. Prérequis D’un point de vue théorique Savoir ce qu’est une distribution normale. Savoir ce qu’est une moyenne, un écart-type et une erreur-type Savoir ce qu’est le score z 9.1 Introduction Les statistiques permettent de tester des hypothèses, ces hypothèses peuvent être de nature extrêmement différentes : De combien de béton ai-je besoin pour éviter qu’un pont d’autoroute ne s’écroule avec un risque d’erreur de 1/100 000 ? Est-ce que ce traitement est efficace ? Est-ce que les oiseaux migratoires prennent toujours la même route ? Est-ce que des enfants apprennent mieux avec la méthode A ou avec la méthode B ? Quelle date de péremption faut-il mettre sur les emballages pour s’assurer que le produit ne sera pas périmé dans 99.9% des situations ? Quelle est la méthode la plus efficace pour faire comprendre les statistiques aux étudiants ? Toutes ces questions amènent des hypothèses. Par exemple, mes calculs permettent de prédire qu’il faut X tonnes de béton pour s’assurer que le pont ne fissure pas dans plus d’un 1 car sur 100 000. On peut raisonnablement se demander comment il est possible de répondre de manière fiable à ces questions. Il existe au minimum trois approches différentes pour apporter des éléments de réponses : l’approche de Neyman-Pearson (1933), l’approche bayésienne (Bayes &amp; Price, 1763; LaPlace, 1814) et l’approche par vraisemblance (Royall, 1997). Dans ce chapitre, nous allons considérer que l’approche de Neyman-Pearson (1933) est la plus indispensable pour débuter avec l’inférence statistique car c’est l’approche la plus communément rencontrée dans la littérature scientifique. Il est donc nécessaire de pouvoir apporter les bases théoriques nécessaires à la lecture des statistiques selon cette approche. Ainsi, dans ce chapitre, vous pourrez être dans une logique de découverte de l’inférence statistique et vous limitez à lire et comprendre l’approche de Neyman-Pearson (1933), être déjà au fait avec cette approche et découvrir une autre école d’inférence statistique ou enfin vouloir découvrir naïvement les différentes écoles de pensées et vous faire votre propre opinion sur celle qui vous correspond le mieux. Dans le premier et second cas de figure, vous pourrez concentrer votre lecture sur l’école de pensées qui vous intéresse et dans le dernier cas de figure, vous ne pourrez pas faire l’économie d’une lecture complète de ce chapitre. Si, pour toutes les approches, nous essaierons d’aborder les choses de manière assez intuitive dans un premier temps, ce chapitre est tellement important que ce serait une erreur d’aborder les choses que de manière superficielle. Ainsi, après cette première phase plus intuitive, nous aborderons progressivement les choses de manière plus technique. S’assurer d’une bonne compréhension de la méthode d’inférence que vous voudrez utiliser ou analyser est non seulement indispensable pour utiliser de manière adaptée les outils mais facilitera la compréhension des autres chapitres. 9.2 L’approche de Neyman-Pearson 9.2.1 Une approche intuitive Imaginons qu’un chercheur développe un médicament qui rend les gens intelligents. Pour montrer l’efficacité de son traitement, il faut passer un test d’intelligence à un groupe de personnes, il donne son traitement miraculeux à ce groupe et au bout de quelques semaines de traitement, il évalue à nouveau le niveau intellectuel de ces personnes (avec un autre test pour éviter les effets d’apprentissage mais qui est tout aussi bien standardisé). Il observe que la moyenne au temps 1 est de 100 et que la moyenne au temps 2 est 100,1. Après son expérimentation, ce chercheur présente les résultats de son étude en affirmant haut et fort que son traitement rend plus intelligent. Probablement, que face à cette affirmation, vous pourriez être amusé·e, mais probablement pas convaincu·e. La question qui se pose est de savoir à partir de quel moment vous seriez convaincu·e. la réponse à cette question est simple : à partir du moment où la différence observée fait sens. D’un point de vue statistique, c’est ce qu’on appelle la significativité10. Pour comprendre intuitivement la notion de significativité, demandez-vous si l’intelligence d’Albert Einstein est une intelligence exceptionnelle, ou une intelligence typique de la population humaine. Si vous aboutissez à la conclusion qu’Albert Einstein a une intelligence exceptionnelle, cela signifie que vous considérez que son intelligence est significativement différente de celle de la population générale. La raison pour laquelle vous arrivez à la conclusion qu’Einstein est quelqu’un de particulièrement intelligent est que, de manière générale, on voit peu de personnes ayant réalisé autant d’accomplissement scientifiques et que, pour atteindre un tel niveau d’accomplissement, il est nécessaire que cette personne ait une intelligence très développée. L’idée sous-tendant les tests statistiques est d’estimer la probabilité de croiser une personnes aussi intelligente ou plus intelligente qu’Albert Einstein en considérant que ce niveau intellectuel ne se distingue pas de celui de la population générale. Ainsi, en considérant dans un premier temps qu’Albert Einstein a une intelligence qu’on peut observer de manière générale dans la population, on considère qu’Albert Einstein ne se différencie pas de la population générale. D’un point de vue statistique, cette absence de différence est ce qu’on appelle une hypothèse nulle, souvent dénoté \\(H_0\\). Pour tester cette hypothèse, on peut utiliser un tester évaluant le quotient intellectuel (QI). La moyenne pour une échelle de QI est de 100 et l’écart-type est de 15. On considère que le niveau d’intelligence se distribue en suivant une distribution normale. Sachant qu’Einstein avait un QI estimé à 162, on peut calculer la probabilité d’obtenir un QI aussi élevé ou plus élevé que le sien. Nous pouvons obtenir cette probabilité à l’aide du score z, qui est obtenu par la formule (9.1) : \\[\\begin{equation} z=\\frac{x_i-\\bar{x}}{s} \\tag{9.1} \\end{equation}\\] Dans R, c’est la fonction pnorm qui permet de réaliser le score Z. Cette fonction a 4 arguments : q : le score \\(x_i\\) de l’individu mean : la moyenne du groupe de référence sd : l’écart-type du groupe de référence lower.tail : permet de choisir le côté de la courbe (lower.tail = TRUE signifie qu’on calcule la probabilité pour les scores inférieurs ou égal au \\(x_i\\) et lower.tail = FALSE pour les scrores supérieurs ou égal au \\(x_i\\)) pnorm(q=162, mean = 100, sd = 15, lower.tail = FALSE) ## [1] 1.787698e-05 Avec une probabilité de \\(1.78 \\times 10^{-5}\\), on trouve moins de deux personnes sur 100 000 ayant une intelligence aussi élevée ou plus élevée que celle d’Albert Einstein. Ainsi, la probabilité de croiser quelqu’un d’aussi intelligent qu’Albert Einstein est très faible, c’est pour cela que nous avons considéré intuitivement qu’il avait un QI qui se distinguait significativement de la plupart des gens. Il faut donc comprendre qu’on considère qu’il y a une différence significative lorsque la probabilité d’observer un phénomène au regard de la distribution sur laquelle on s’appuie est faible. Pour le formuler autrement, la probabilité associée à un test statistique dans le cadre des tests d’hypothèse nulle est la probabilité d’observer une telle différence (avoir un QI de 162 signifie qu’il y a une différence de 62 points par rapport à la moyenne) si cette différence est vraie (le QI d’Einstein aurait pu être un QI égal à 100). Ainsi, si la probabilité d’observer cette différence est faible, alors, on va considérer cette différence comme étant significative car il y a peu de chance que cette différence soit observé uniquement en raison du fruit du hasard. Habituellement, on considère que la probabilité est faible si la probabilité associée au test statistique est inférieure à 0.05. C’est ce qu’on appelle le seuil de significativité. Ainsi, dans le Figure 9.1, la barre verticale représente le seuil de significativité à 5% lorsqu’on considère que seul un côté de la courbe est digne d’intérêt. On parle dans ce cas d’hypothèse unilatérale. En d’autres termes, dans une hypothèse unilatérale, on considère que seul les 5% supérieurs de notre distribution sont intéressants pour tester notre hypothèse. Le seuil de significativité sera placé sur l’extrémité d’intérêt de la distribution. Pour reprendre l’exemple sur le QI. Au-delà d’Albert Einstein, on va considérer que quelqu’un a un niveau intellectuel qui se distingue significativement si son QI est supérieur à 124.6728, indiqué par la barre verticale l’indique dans la Figure 9.1. x&lt;-rnorm(10000, 100,15) r &lt;- c(40,160) hist(x,freq=FALSE,col=0,ylim=c(0,0.05),20, xlab=NULL, ylab=&quot;Densité&quot;, main=NULL) plot(function(x) dnorm(x,100,15),xlim=r,col=3,add=TRUE) cutoff&lt;-qnorm(0.95, 100, 15) abline(v=cutoff) text(y=0.005, x=135 ,labels =&quot;5%&quot;) Figure 9.1: Seuil de significativité pour une hypothèse unilatérale cutoff ## [1] 124.6728 Une autre manière de répartir les 5% est d’estimer que le QI d’une personne se distingue significativement dès lors qu’elle appartient au 5% d’une des deux extrémités de la courbe. Dans ce cas, il faut répartir les 5% sur les deux extrémités, ce qui signifie que le seuil de significativité est atteint lorsqu’une personne a un QI inférieur ou égal aux 2.5% à l’extrémité gauche de la courbe (i.e., les QI les plus faibles) ou aux 2.5% à l’extrémité droite de la courbe (i.e., les QI les plus élevés), voir Figure ??. cutoff1&lt;-qnorm(0.025, 100, 15) cutoff2&lt;-qnorm(0.975, 100, 15) cutoff1 ## [1] 70.60054 cutoff2 ## [1] 129.3995 x&lt;-rnorm(100000, 100,15) r &lt;- c(40,160) hist(x,freq=FALSE,col=0,ylim=c(0,0.05),20, xlab=NULL, ylab=&quot;Densité&quot;, main=NULL) plot(function(x) dnorm(x,100,15),xlim=r,col=3,add=TRUE) abline(v=cutoff1) abline(v=cutoff2) text(y=0.005, x=145 ,labels =&quot;2.5%&quot;) text(y=0.005, x=50 ,labels =&quot;2.5%&quot;) Figure 9.2: Seuil de significativité pour une hypothèse bilatérale Dans le premier cas de figure, on parle d’hypothèse unilatérale et dans le second cas de figure, on parle d’hypothèse bilatérale. Il est raisonnable d’utiliser le premier cas de figure uniquement lorsque les éléments théoriques sur lesquels on s’appuie pour tester l’hypothèse sont extrêmement robustes, et même dans ce cas, cela est, la plupart du temps, considéré comme insuffisant pour justifier une hypothèse unilatérale. Ainsi, si une personne a un QI de 135, on va rejeter l’hypothèse nulle et considérer qu’il y a une différence significative. Quand on conclut à la présence d’une différence significative, c’est que les données permettent d’étayer une autre hypothèse que l’hypothèse nulle : cette autre hypothèse est l’hypothèse d’une différence. On l’appelle l’hypothèse alternative. Tous les tests statistiques sont construits sur le même principe : ils opposent une hypothèse nulle à une hypothèse alternative. De manière systématique, l’hypothèse nulle doit être formalisée sous la forme d’une absence de différence et l’hypothèse alternative en termes de la présence d’une différence. Bien que cela semble simple lorsqu’on présente les choses ainsi, il n’est pas si simple de pouvoir générer ces deux hypothèses si on ne comprend pas à quoi sert le test, ce qui peut entraîner des erreurs d’interprétation. Cependant, si vous entraînez à formuler les hypothèses nulles et alternatives correctement, vous pourrez facilement transposer à de nouvelles situations que vous n’aurez jamais rencontrées en appliquant le même principe. Dans le cas d’Einstein, il n’était pas nécessaire de tester statistiquement l’hypothèse pour connaître la réponse car on savait qu’il s’agit d’une personne particulièrement intelligente. Le problème est que, dans la réalité, on ne connait pas la réponse à la question que l’on se pose avant d’avoir mis à l’épreuve cette question. Ainsi, si on estime le QI d’une personne, c’est par exemple, pour essayer de comprendre pourquoi cette personne est en échec scolaire. Ainsi, on pourrait faire l’hypothèse que son niveau intellectuel est faible, mais on pourrait également faire l’hypothèse que, en raison de son niveau intellectuel élevé, elle s’ennuie à l’école, elle se désintéresse de ses cours, ce qui la fait échouer. Dans la section suivante, nous allons décrire comment la significativité est déterminée et ce qu’elle signifie à partir dun exemple de travail plutôt simple. 9.2.2 Quid quand c’est moins évident? Une jeune femme fait l’hypothèse qu’il est possible de rencontrer l’homme de sa vie en sortant dans un bar. Elle n’est pas tout à fait naïve est sait très bien que la plupart des hommes ne seront pas son prince charmant. Pour accorder une chance au jeune homme qui viendra la trouver, elle va donc opérationnaliser son hypothèse en considérant que l’homme de sa vie sera en mesure de l’intéresser pendant au moins 90 minutes. Elle lui donnera donc son numéro qu’à cette condition. Pour formaliser les choses, les hommes géniaux doivent intéresser notre jeune femme pendant 5400 secondes. Néanmoins, dans la catégorie des hommes géniaux, il y en a qui sont moins bavards, il y en a qui sont plus timides. Donc, les hommes géniaux vont en réalité se distribuer autour de la moyenne de 5400 secondes avec un écart-type de 1000 seconde en suivant la distribution présentée en Figure 3. genial &lt;- rnorm(3000, 5400, 1000) hist(genial, main =&quot;Figure 3. Distribution des hommes géniaux, dignes d&#39;intérêt&quot;) Le problème est que les jeunes hommes qui ne méritent pas l’intérêt de notre jeune femme n’ont pas écrit sur leur front qu’ils ne sont pas dignes d’intérêt. Bien sûr, certains vont être éliminés très rapidement (“hey mademoiselle, t’as un 06 ?”), mais pour certains, les choses sont moins clairs. On peut raisonnablement penser qu’une personne qui n’est pas un prince charmant est tout de même en mesure d’intéresser notre jeune personne pendant une cinquantaine de minutes, c’est-à-dire 3000 secondes. La distribution des hommes qui sont moins dignes d’intérêt est représentée en Figure 4. banal&lt;- rnorm(3000, 3000, 1000) hist(banal, main=&quot;Figure 4. Distribution des hommes peu dignes d&#39;intérêt&quot;) Les choses se compliquent quand il y a une incertitude sur l’issue du test (quand on rencontre une personne, on n’a pas la certitude absolue qu’il s’agit de la personne de notre vie). La raison est due au fait que la zone de délimitation est floue et que la ditribution des hommes géniaux se superpose à la distribution des hommes ayant moins d’intérêt, comme vous pouvez le constater dans la Figure 5. ## Histogrammes non-imprimés : bp1 &lt;- hist(banal, plot=FALSE, nclass=20) # 20 classes... bp2 &lt;- hist(genial, plot=FALSE, nclass=20) # ## Calcul de minima/maxima : hlims &lt;- range(c(bp1$breaks, bp2$breaks)) vlims &lt;- range(c(bp1$counts, bp2$counts)) ## Couleurs de remplissage, dont une avec ajout de transparence : histcol &lt;- sapply(apply(rbind(col2rgb(c(&quot;orangered2&quot;, &quot;lightblue&quot;))/255, alpha = c(1, 0.5)), # Première couleur opaque, 50% de transparence pour la seconde. 2, as.list), do.call, what = rgb) ## Création du graphique : hist(banal, border=&quot;darkred&quot;, xlim=hlims, ylim=vlims, nclass=20, # 20 classes... col=histcol[1], main=&quot;Figure 5. Fréquence en fonction du type d&#39;hommes&quot;) hist(genial, border=&quot;darkblue&quot;, add=TRUE, breaks=seq(from=min(hlims), to=max(hlims), by=diff(bp1$breaks[1:2])), # largeur de classes égale à celle du précédent graphique col=histcol[2]) # Couleur transparente (en sur-impression). seuil&lt;-qnorm(0.975, 3000,1000) abline(v=seuil,lwd=3) text(x=3000, y=200, &quot;banal&quot;) text(x=6000, y=200, &quot;génial&quot;) Dans cet exemple, les hommes qui n’arriveront pas à intéresser notre jeune femme pendant au moins \\(4960\\) secondes seront considérés comme banal alors que ceux qui arriveront à intéresser la jeune femme plus longtemps que ce seuil seront considérés comme géniaux. Dans cet exemple, une partie des hommes qui appartiennent au groupe des hommes banals arrivent à passer le seuil à partir duquel on les conidère comme géniaux. Vous les connaissez, ce sont les beaux parleurs : vous croyez en leurs doux mots, vous leur proposez votre numéro … et ils ne vous rappellent pas. A l’inverse, une part importante des hommes qui appartiennent au groupe des princes charmants se voient affublés de l’étiquette “banal” car ils sont plus timides, moins diserts… En d’autres termes, dans avec un test statistique, il y a deux moyens d’avoir raison et deux moyens de se tromper. Lorsque vous considérez un homme banal comme banal, cela signifie que vous tolérez à raison l’hypothèse nulle selon laquelle il n’est pas significativement différent. Dans ce cas, on parle d’intervalle de confiance. Il est généralement à 95%. Cet intervalle de confiance signifie que, si on connait les paramètres d’une population, on peut estimer un intervalle à l’intérieur duquel 95% des estimations de ce paramètre se situeront. Le complément de l’intervalle de confiance est l’erreur de 1e espèce, appelée \\(\\alpha\\). Elle est habituellement de 5%, et dans notre exemple, ils sont représentés par les beaux parleurs. Lorsqu’on considère un homme comme appartenant à la catégorie des princes charmants et qu’il se trouve qu’il est effectivement un prince charmant, il s’agit de la seconde manière d’avoir raison. Cette condition consiste à rejeter à raison l’hypothèse nulle. On parle dans ce cas de puissance statistique. Le complèment de la puissance statistique est l’erreur de seconde espèce, appelée \\(\\beta\\). Ces deux manières d’avoir raison et ces deux manières d’avoir tort sont résumées dans le Tableau ??. 1 = IC + alpha 1 = bêta + puissance Dans notre exemple, les valeurs pour chacun des paramètres sont : ## ## Attachement du package : &#39;kableExtra&#39; ## Les objets suivants sont masqués depuis &#39;package:flextable&#39;: ## ## as_image, footnote ## L&#39;objet suivant est masqué depuis &#39;package:dplyr&#39;: ## ## group_rows Dans cet exemple, on se posait la question à un niveau individuel. Cependant, la plupart du temps, on se pose la questiion au niveau du groupe (i.e., est-ce que tel groupe se distingue de tel autre groupe ?). Pour comprendre la manière dont cela fonctionne au niveau du groupe, il faut comprendre deux notions essentielles en statistiques : la notion d’échantillonnage et le théorème central limit. La manière la plus simple de comprendre ces deux notions consiste à en fait la démonstration par un exemple. On va commencer par créer une population artificielle dont la distribution est plate. On obtient ce type de distribution en faisant par exemple une séquence de 1 à 10 000. pop&lt;-1:10000 On peut obtenir la distribution de cette population à l’aide la fonction hist. hist(pop, main = &quot;Figure 6. Représentation de la population créée&quot;) Deux constats doivent être dressés : 1) la distribution est plate ; 2) les valeurs vont de 1 à 10 000. La moyenne de cette distribution vaut \\(5000.5\\) et l’écart-type vaut \\(2886.8956799\\). Si on prend un échantillon de 50 personnes au hasard dans cette population grâce à la fonction sample, on va se rendre compte que la moyenne de cet échantillon ne sera pas exactement égal à la moyenne de la population. sample(pop,50, replace=F)-&gt;M_ech M_ech ## [1] 3594 334 7295 7434 7762 1804 4887 3000 8094 426 4731 5945 9980 9351 6801 ## [16] 3719 4698 9033 9953 8559 2007 4169 7474 2055 1514 9301 5394 7220 2793 6872 ## [31] 902 9422 5686 8958 8043 4125 1089 507 9244 7090 6206 5067 1040 3199 1304 ## [46] 9603 5950 2568 8578 3577 mean(M_ech) ## [1] 5367.14 Si on répète un grand nombre de fois cette opération, on va obtenir systématiquement une moyenne différente, quelque fois très proche de la moyenne de la population, d’autres fois plus éloignée. La manière dont les moyennes d’échantillon se distribuent autour de la vraie moyenne (i.e., la paramètre) est ce qu’on appelle la distribution d’échantillonnage L’appli sur le lien suivant permet de voir comment la distribution des moyennes d’échantillons : https://ihstevenson.shinyapps.io/sample_means/ Dans notre cas, nous allons faire une boucle grâce à la fonction for qui va permettre d’obtenir 100 000 moyennes d’échantillons que nous allons stocker dans un vecteur, appelé M_ech. for(i in 1:99999){ mean(sample(pop,50, replace=F))-&gt;M_ech2 c(M_ech, M_ech2)-&gt;M_ech } Nous pouvon faire une présentation graphique de la manière dont les moyennes d’échantillons se répartissent autour de la moyenne de la population grâce à la fonction hist. hist(M_ech, main=&quot;Figure 7.Représentation graphique de la distribution d&#39;échantillonnage&quot;, xlab=&quot;distribution des moyennes d&#39;échantillon&quot;, xlim=range(3500,6500)) mean(M_ech) ## [1] 5002.093 sd(M_ech) ## [1] 411.2281 A présent, 3 constats doivent être faits : La distribution n’est plus plate ; La moyenne des moyennes vaut \\(5002\\) ; L’écart-type de la distribution des moyennes d’échantillon vaut \\(411\\). Pour le premier point, ce phénomène s’explique par le théorème centrale limite, selon lequel, indépendamment de la distribution initiale des données, si l’échantillon est suffisamment grand (la notion de suffisamment grand étant relative), la distribution des moyennes d’échantillon suivra une distribution normale. Ce théorème explique un argument qu’on observe régulièrement concernant le non respect de la normalité de la distribution : ce n’est pas grave parce que, comme mon échantillon est suffisamment grand, la distribution d’échantillonnage suit une distribution normale grâce au théorème central limit. Cet argument n’est qu’à moitié correct : il ne tiendrait pas si une distribution fortement asymétrique avait été utilisée au lieu d’une distribution plate. Pour le second point, cela illustre que le fait de répéter un grand nombre de fois une expérimentation permet d’avoir une estimation relativement précise du paramètre de la population. Enfin, le 3e point amène à comprendre la notion d’erreur type. L’erreur-type, standard error (se) en anglais, est l’écart-type divisé par la racine carrée de la taille de l’échantillon. \\[se = \\frac{s}{\\sqrt{n}}\\] Et nous constatons que l’erreur-type simulé est assez proche de l’erreur-type estimée de manière théorique. sd(pop)/50^0.5 ## [1] 408.2687 Ce principe est donc à la base d’un test comme le t de Student comparaison à une norme. En effet, la formule du t de Student comparaison à une norme est la suivante : \\[t=\\frac{\\bar{x}-\\mu}{ \\frac{s}{\\sqrt{n}}}\\] où \\(\\bar{x}\\) correspond à la moyenne de l’échantillon, \\(\\mu\\) correspond à la moyenne de la population, s correspond à l’écart-type de l’échantillon et n correspond à la taille de l’échantillon. Si on reprend la formule du z, on identifie que c’est exactement la même formule avec un échantillon d’une personne : la moyenne d’une observation est cette observation, et faire la racine de 1 vaut 1. \\[z=\\frac{x_i-\\mu}{\\frac{s}{\\sqrt{1}}}=\\frac{x_i-\\mu}{s}\\] Ainsi, quand on réalise un test statistique, un probabilité va être associée à la valeur de la statistique. Si cette probabilité vaut 0.70, cela signifie que, sur la base d’un tirage aléatoire dans une population initiale, 70% des échantillons vont présenter une différence égale ou plus grande que la différence observée. Comme la probabilité est en l’occurrence élevée, il est tout à fait possible que la différence observée soit due à l’erreur (en termes d’estimation) d’échantillonnage. On ne peut donc pas rejeter l’hypothèse nulle avec un risque de se tromper suffisamment faible. Si vous avez suivi le raisonnement, vous devriez avoir compris que 5% des moyennes d’échantillons (i.e., 500) se répartiront de manière équitable au-dessous du percentile 2.5 et au-dessus du percentile 97.5 évalués estimé sur base d’une distribution normale. La fonction qnorm permet de tester cela. lower&lt;-mean(pop)+qnorm(0.025)*sd(pop)/50^0.5 # fixe la limite basse upper&lt;-mean(pop)+qnorm(0.975)*sd(pop)/50^0.5 # fixe la limite haute n.low&lt;-length(which(M_ech&lt;lower)) # nombre en dessous de la limite inférieure n.high&lt;-length(which(M_ech&gt;upper)) # nombre en dessous de la limite supérieure n.low ## [1] 2350 n.high ## [1] 2495 n.low+n.high ## [1] 4845 On constate qu’il y a effectivement environ 500 échantillons sur 10 000 qui ont dépassé le seuil de significativité, ce qui représente le pourcentage d’erreur de première espèce que nous admettrons. Donc, bien que ces 5% d’échantillons qui se trouvent aux deux extrêmes appartiennent à la population des que nous avons créée, on va considérer (à tort) qu’ils sont significativement différents de la population créée. 9.3 Lien entre la taille d’effet et la significativité Dans l’exemple que nous avons utilisé, les hommes géniaux étaient intéressants pendant 5400 secondes alors que les hommes banals étaient intéressants pendant 3000 secondes. Comme l’écart-type était de 1000, cela signifie que la moyenne des hommes banals et celle des hommes géniaux sont à 2.4 écart-types l’une de l’autre. En effet, on l’obtient de la manière suivante : \\[d_{Cohen}=\\frac{5400-3000}{1000}=2.4\\] Cette valeur de 2.4 est ce qu’on appelle le d de Cohen. Il correspond à la taille d’effet pour une comparaison de deux moyennes. Pour chaque test statistique (ou pratiquement), il existe une taille d’effet qui lui est propre. Pour Cohen (1988), une taille d’effet de 0.2 est une petit taille, une taille d’effet de 0.5 est une taille d’effet moyenne et une taille d’effet de 0.8 est une grande taille d’effet. Il faut tout de même avoir conscience que ces limites sont arbitraires et qu’elles dépendant grandement du domaine scientifique dans lequel on travaille : les tailles d’effet sont généralement plus faibles en psychologie qu’en biologie. Néanmoins, on comprendre qu’avec une taille d’effet de 2.4, la taille d’effet est colossale et qu’il est plutôt rare d’observer de telles tailles d’effets dans la réalité. Pour prendre un exemple plus réaliste, considérons à présent que les hommes banals sont intéressants pendant 5000 secondes alors que les hommes géniaux sont intéressants pendant 5400 secondes, la taille d’effet est à présent de 0.4 et la courbe des hommes géniaux et des hommes banals vont se recouvrir de la manière suivante : banal &lt;- rnorm(3000, 5000, 1000) genial &lt;- rnorm(3000, 5400, 1000) ## Histogrammes non-imprimés : bp1 &lt;- hist(banal, plot=FALSE, nclass=20) # 20 classes... bp2 &lt;- hist(genial, plot=FALSE, nclass=20) # ## Calcul de minima/maxima : hlims &lt;- range(c(bp1$breaks, bp2$breaks)) vlims &lt;- range(c(bp1$counts, bp2$counts)) ## Couleurs de remplissage, dont une avec ajout de transparence : histcol &lt;- sapply(apply(rbind(col2rgb(c(&quot;orangered2&quot;, &quot;lightblue&quot;))/255, alpha = c(1, 0.5)), # Première couleur opaque, 50% de transparence pour la seconde. 2, as.list), do.call, what = rgb) ## Création du graphique : hist(banal, border=&quot;darkred&quot;, xlim=hlims, ylim=vlims, nclass=20, # 20 classes... col=histcol[1], main=&quot;fréquence en fonction du type d&#39;hommes&quot;, xlab=&quot;banal et génial&quot;) hist(genial, border=&quot;darkblue&quot;, add=TRUE, breaks=seq(from=min(hlims), to=max(hlims), by=diff(bp1$breaks[1:2])), # ...largeur de classes égale à celle # du précédent graphique col=histcol[2]) # Couleur transparente (en sur-impression). On constate que la superposition entre les deux courbes est très important et qu’il est difficile de distinguer les hommes géniaux des hommes banals. De manière intéressante, si au lieu de s’intéresser à une personne particulière, on fait l’hypothèse que les princes charmants trainent ensemble et que les hommes banals trainent entre eux, mais ils ne se mélangent, vous pourriez rechercher des groupes d’une dizaine d’homme et, dans ce cas, la superposition des deux courbes serait moins importante. banal &lt;- rnorm(3000, 5000, 1000/10^0.5) genial &lt;- rnorm(3000, 5400, 1000/10^0.5) ## Histogrammes non-imprimés : bp1 &lt;- hist(banal, plot=FALSE, nclass=20) # 20 classes... bp2 &lt;- hist(genial, plot=FALSE, nclass=20) # ## Calcul de minima/maxima : hlims &lt;- range(c(bp1$breaks, bp2$breaks)) vlims &lt;- range(c(bp1$counts, bp2$counts)) ## Couleurs de remplissage, dont une avec ajout de transparence : histcol &lt;- sapply(apply(rbind(col2rgb(c(&quot;orangered2&quot;, &quot;lightblue&quot;))/255, alpha = c(1, 0.5)), # Première couleur opaque, 50% de transparence pour la seconde. 2, as.list), do.call, what = rgb) ## Création du graphique : hist(banal, border=&quot;darkred&quot;, xlim=hlims, ylim=vlims, nclass=20, # 20 classes... col=histcol[1], main=&quot;fréquence en fonction du type d&#39;hommes&quot;, xlab=&quot;banal et génial&quot;) hist(genial, border=&quot;darkblue&quot;, add=TRUE, breaks=seq(from=min(hlims), to=max(hlims), by=diff(bp1$breaks[1:2])), # ...largeur de classes égale à celle # du précédent graphique col=histcol[2]) # Couleur transparente (en sur-impression). Le même raisonnement vaudait pour un échantillon de 50 personnes, les deux courbes se superposeraient encore 9.4 Quand c’est encore moins évident? Superposition des courbes quand on prend un échantillon de 50 personnes banal &lt;- rnorm(3000, 5000, 1000/50^0.5) genial &lt;- rnorm(3000, 5400, 1000/50^0.5) ## Histogrammes non-imprimés : bp1 &lt;- hist(banal, plot=FALSE, nclass=20) # 20 classes... bp2 &lt;- hist(genial, plot=FALSE, nclass=20) # ## Calcul de minima/maxima : hlims &lt;- range(c(bp1$breaks, bp2$breaks)) vlims &lt;- range(c(bp1$counts, bp2$counts)) ## Couleurs de remplissage, dont une avec ajout de transparence : histcol &lt;- sapply(apply(rbind(col2rgb(c(&quot;orangered2&quot;, &quot;lightblue&quot;))/255, alpha = c(1, 0.5)), # Première couleur opaque, 50% de transparence pour la seconde. 2, as.list), do.call, what = rgb) ## Création du graphique : hist(banal, border=&quot;darkred&quot;, xlim=hlims, ylim=vlims, nclass=20, # 20 classes... col=histcol[1], main=&quot;fréquence en fonction du type d&#39;hommes&quot;, xlab=&quot;banal et génial&quot;) hist(genial, border=&quot;darkblue&quot;, add=TRUE, breaks=seq(from=min(hlims), to=max(hlims), by=diff(bp1$breaks[1:2])), # ...largeur de classes égale à celle # du précédent graphique col=histcol[2]) # Couleur transparente (en sur-impression). Ce qui est illustré dans ce qui précède est la notion de puissance statistique. Plus les courbes se superposent, et moins la puissance est élevée. A l’inverse, moins elles se superposent plus la puissance est élevée. Les facteurs qui ont un impact sur la puissance statistique sont : la taille de l’effet et la taille de l’échantillon. On pourrait rajouter certains paramètres expérimentaux comme être en intra participant, c’est-à-dire avoir les mêmes personnes/objets/animaux… pour l’ensemble des mesures, ou encore contrôler l’effet de certains effets parasites à l’aide de covariables. La puissance statistique doit être au minimum de 0.80 (Cohen, 1988), mais les recommendations actuelles tendent à l’augmenter à 0.95 (Lakens, 2013). Cependant, atteindre une puissance de 0.95 entraîne une inflation du nombre de participants (ou de sujet, au sens large - bien que ce terme n’est pas très élégant) qu’il faut recruter, ce qui n’est pas toujours possible dans les études. Pour notre exemple, on peut fixer la puissance à 0.80. Avec un échantillon pour lequel on s’attend à ce que la moyenne soit de 5000 et un autre pour lequel on s’attend à ce que la moyenne soit à 5400, et un écart-type de 1000, on peut obtenir la puissance à l’aide de la fonction pwr.t.test du package ‘pwr’. library(pwr) d&lt;-(5000-5400)/1000 d FALSE [1] -0.4 pwr.t.test(d=d,sig.level = 0.05, power=0.80, type=&quot;two.sample&quot;, alternative=&quot;two.sided&quot;) FALSE FALSE Two-sample t test power calculation FALSE FALSE n = 99.08032 FALSE d = 0.4 FALSE sig.level = 0.05 FALSE power = 0.8 FALSE alternative = two.sided FALSE FALSE NOTE: n is number in *each* group Il faut donc un échantillon de 99 personnes par groupe pour que la puissance statistique soit de 0.80. Cependant, il faut avoir conscience qu’il existe deux différences majeures entre mon exemple pédagogique et la réalité : Dans la réalité, on ne connait pas la valeur réelle des moyennes de chaque modalité qu’on veut comparer. Dans la réalité, il faut calculer la puissance statistique a priori, c’est-à-dire avant le début du recueil de données. Normalement, vous devriez avoir identifié un problème : dans la réalité, vous ne connaissez pas les moyennes des paramètres (ni l’écart-type) alors comment pourriez-vous calculer la puissance a priori ? Il existe plusieurs possibilités : vous vous appuyez sur la littérature scientifique, les champs de recherche proche de celui investigué pour déterminer la taille d’effet habituellement observée. vous réalisez une étude pilote pour estimer la taille de l’échantillon nécessaire, mais les données obtenues par l’étude pilote ne doivent pas faire partie du jeu de données final. vous estimez la taille d’effet minimum que vous désirez mettre en évidence dans l’étude et vous fixez la puissance en fonction de cette taille d’effet minimum. Par exemple, si vous voulez tester l’effet d’un médicament, et que vous savez que l’effet placebo explique 5% de la variance, alors vous pourriez décider que votre médicament pour être utile doit avoir un effet deux fois plus important que l’effet placebo, c’est-à-dire expliquer 10% de la variance. 9.5 En synthèse les tests statistiques testent une hypothèse d’absence de différence La probabilité fournie par un test est la probabilité d’observer la différence en raison des variations aléatoires dues à la distribution d’échantillonnage. Si la probabilité est faible, cela signifie qu’il est improbable que la différence soit dû au hasard et on l’attribuera alors à l’effet de la manipulation expérimental. Dans ce dernier cas, on dit qu’on rejette l’hypothèse nulle. On peut commettre deux types d’erreurs : l’erreur de 1e espèce et de 2e espèce. Habituellement, on fixe l’erreur de 1e espèce à 5% et l’erreur de 2e espèce au maximum à 20% Même si on a une population qui n’est pas distribuée normalement, la distribution d’échantillonnage suivra la loi normale si la distribution initiale ne présente pas une asymétrie trop importante. On diminue l’erreur de type II en augmentant la taille de l’échantillon Il faut connaître la taille de l’effet pour calculer la puissance a priori. Deux facteurs influencent la puissance : la taille de l’effet et la taille de l’échantillon 9.5.1 Une approche plus formelle 9.5.1.1 La crise de reproductibilité et le p hacking. Dans tous les domaines scientifiques, il est nécessaire de publier des articles, et dans les entreprises, d’avoir des résultats intéressants. Au niveau universitaire par exemple, publier permet d’avoir un poste, des promotions, des primes, des financements. Quand on a des financements, il faut pouvoir les justifier, expliquer ce qu’on a fait de ces financements. Généralement, on résume la nécessité de publier pour les chercheurs par un adage : “publish or perish!”. Au niveau privé, montrer qu’un médicament est efficace permet de le commercialiser, montrer qu’un engrais n’est pas dangereux permet de le commercialiser… Bref, tant au niveau public que dans le privé, les enjeux sont énormes, et quand les enjeux sont énormes, les dérives commencent à apparaître. Pour comprendre la cause de ces dérives, il est nécessaire de savoir ce qu’est un résultit dit “positif”. Il s’agit des études pour lesquels les données corroborent l’hypothèse par la présence de résultats significatifs. Ce sont ces résultats qui sont essentiellement publiés en science. En psychologie, par exemple, Fanelli (2010) souligne que 90% des études publiées sont des réultats positifs. Ce nombres d’études avec des résultats positifs suit une tendance constante depuis les année 1990. En effet, dans une étude de 2011, Fanelli montre que la publication de résultats positifs a augmenté de 22% depuis les années 1990. On comprend dès lors aisément pourquoi il faut des résultats positifs et que cela amène à des comprotements douteux. Un des comportements douteux est évidemment la fabrication de données, ce qui a été largement médiatisée par l’affaire Stapel, un psychologue social, qui avait fabriqué les résultats de plusieurs dizaines de ses articles. On pourrait penser que ce phénomène est plutôt rare, mais c’est loin d’être certain. En effet, Fanelli (2009), encore lui, a posé à des chercheurs deux questions : Avez-vous déjà trafiqué vos données ? Avez-vous déjà adopté une pratique douteuse (il n’est pas nécesaire de dévolopper ici les différentes pratiques douteuses) ? Ce qui est intéressant, c’est que, sur le panel de chercheurs, la plupart indique s’être toujours comporté de manière intègre, mais que les 3/4 connaissent un collègue qui a adopté une pratique douteuse, et qu’un tiers des chercheurs connaissent même quelqu’un qui a fabriqué ou falsifié ses données. Table 2. Résultats obtenus par Faneli (2009) Fabrication/modification Autres pratiques J’ai pratiqué 1,97% 14,12% Je connais quelqu’un 33,7% 72% Parmi les pratiques douteuses que la plupart des personnes n’identifient même pas comme problématique, il y a le p-hacking. 9.5.1.2 Le p hacking. Le p-hacking consiste à analyser les données qu’on a obtenu de différentes manières, en ajoutant parfois des observations, en supprimant des observations dites “outliers”, en ajoutant des covariables, ou simplement en prenant un grand nombre de mesures et en ne présentant les résultats que d’une minorité de ces mesures. Pour comprendre le p hacking, il faut comprendre un principe statistique essentiel : la multiplication de l’erreur de 1e espèce. Ce principe se résume à une phrase : plus vous faites d’analyses, plus vous augmentez le risque d’obtenir un effet significatif alors que vous ne devriez pas en avoir un. Pour l’illustrer, nous allons reprendre l’exemple que nous avions réalisé plus haut pour montrer la distribution d’échantillonnnage. La simulation montrait que 5% des échantillons étaient considérés comme dépassant le seuil de significativité. Pour reprendre le raisonnement complet, le t de Student comparaison à une norme est obtenu à l’aide la formule suivante : \\[t= \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt n}}\\] On peut obtenir le seuil précis de significativité grâve à la fonction qt. Dans cette fonction, il faut préciser le seuil unilatéral (i.e., 0.025) et les degrés de liberté (ddl). Pour le t comparaison à une norme, les ddl valent la taille de l’échantillon -1. Comme nous avons pris des échantillons de 50 personnes, les ddl valent 49. qt(0.025, 49) ## [1] -2.009575 Nous pouvons à présent calculer le t de Student pour l’ensemble des moyennes d’échantillons que nous avions obtenues pour illustrer la distribution d’échantillonnage. Le code ci-dessous fournit les t premiers t t&lt;-(M_ech-mean(pop))/(sd(pop)/50^0.5) Le code ci-dessous fournit les 10 premiers t obtenus pour les 10 premières moyennes d’échantillons. M_ech[1:10] ## [1] 3594 334 7295 7434 7762 1804 4887 3000 8094 426 round(t[1:10], 2) ## [1] -3.45 -11.43 5.62 5.96 6.76 -7.83 -0.28 -4.90 7.58 -11.20 On peut comptabiliser le nombre de t qui amènent à rejter à tort l’hypothèse nulle (erreur de première espèce). library(plyr) n&lt;-count(abs(t&gt;2.009575)) n ## x freq ## 1 0 97822 ## 2 1 2227 n/length(t) ## x freq ## 1 0.000000e+00 0.97774091 ## 2 9.995102e-06 0.02225909 En l’occurrence, l’erreur de 1e espèce est d’environ 5% comme attendu. Imaginons à présent que nous faisions deux analyses. On peut le simuler avec la fonctino sample qui va échantilloner deux t dans la liste des t disponibles. deux&lt;-sample(t, 2, replace=F) deux ## [1] -0.9578006 -0.2075594 Si nous répétons un grand nombre de fois (1000 par exemple) l’opération pour 2 échantillons, le nombre de situations où au moins un des t est significatif serait bien plus élevé que 5%. Le code ci-desous crée une boucle qui permet de faire cette simulation. for(i in 1:1000){sample(t, 2, replace=F)-&gt;deuxb rbind(deux, deuxb)-&gt;deux} (abs(deux)&gt;2.009575)-&gt;sig rowSums(sig)-&gt;n_sig2 count(n_sig2&gt;0)-&gt; n_sig2 n_sig2 ## x freq ## 1 FALSE 926 ## 2 TRUE 75 Dans ce cas, l’erreur de première espèce serait de \\(7.5\\) pourcents, donc bien supérieur au 5% intialement accepté. Ce phénomène est accenté lorsqu’on fait encore plus d’analyses, comme 5 ou 10 analyses. sample(t, 5, replace=F)-&gt;cinq for(i in 1:1000){sample(t, 5, replace=F)-&gt;cinqb rbind(cinq, cinqb)-&gt;cinq} (abs(cinq)&gt;2.009575)-&gt;sig rowSums(sig)-&gt;n_sig5 count(n_sig5&gt;0)-&gt; n_sig5 n_sig5 ## x freq ## 1 FALSE 793 ## 2 TRUE 208 En effet, le taux d’erreur de 1e espèce pour 5 analyses est de \\(20.8\\) pourcents. sample(t, 10, replace=F)-&gt;dix for(i in 1:1000){sample(t, 10, replace=F)-&gt;dixb rbind(dix, dixb)-&gt;dix} (abs(dix)&gt;2.009575)-&gt;sig rowSums(sig)-&gt;n_sig10 count(n_sig10&gt;0)-&gt; n_sig10 n_sig10 ## x freq ## 1 FALSE 607 ## 2 TRUE 394 En effet, le taux d’erreur de 1e espèce pour 10 analyses est de \\(39.4\\) pourcents. En d’autres termes, le problème est que, si on multiplie le nombre de comparaisons, on augmente également le risque de première espèce. Ainsi, il est nécessaire de distinguer l’erreur par comparaison (EC) et l’erreur par famille (EF). L’erreur par comparaison est le seuil auquel on travaille sur une comparaison (par exemple 5%). L’erreur par famille représente le seuil d’erreur pour l’ensemble des comparaisons. En réalité, il n’était pas nécessaire de faire ces simulations car il est très simple de prédire le taux d’erreur de première espèce qu’on va avoir en fonction du nombre d’analyses. Il suffit d’appliquer la formule suivante : \\[EF = 1 - (IC)^n \\] où EF représente l’erreur par famille, IC représente l’intervalle de confiance et n le nombre d’analyses. 1- (0.95)^2 ## [1] 0.0975 1-(0.95)^5 ## [1] 0.2262191 1-(0.95)^10 ## [1] 0.4012631 Comme attendu, ces valeurs sont proches des pourcentages d’erreurs qu’on a obtenu par simulation. Au-delà de l’erreur par comparaison et de l’erreur par famille. Il existe un troisième type d’erreur, appelée erreur par expérience. Cette terminologie un peu désuète à l’heure actuelle doit attirer votre attention sur le fait que, si on compare deux groupes, par exemple, sur un très grand nombre de variables dépendantes différentes, le test qui sera utilisé sera probablement un test de Student qui sera répété sur chacune des variables. Etant donné qu’il s’agit d’analyses différentes, on pourrait penser que le seuil de significativité est maintenu constant. En fait, il n’en est rien. Le taux d’erreurs par expérience inclut toutes les analyses conduites au sein d’une expérience. En pratique, aucune correction particulière n’est appliquée entre des analyses différentes, excepté quelquefois lorsqu’une même analyse est répétée un grand nombre de fois (des t de Student successifs par exemple). Néanmoins, il vous est grandement recommandé de réaliser le moins de comparaisons possibles. Lorsque vous ne pouvez pas éviter d’avoir un grand nombre de variables dépendantes, et donc de comparaisons, il est souhaitable de se tourner vers les analyses multivariées : MANOVA, extraction de facteurs par l’analyse factorielle exploratoire, ou analyse en composante principale pour réaliser une réduction de données. Une autre manière pour éviter ce problème consiste à corriger la probabilité. Il existe une multitude de corrections des probabilité, dont la plus connue est sans aucun doute la correction de Bonferroni. Cette correction consiste à multiplier les probabilités obtenues pour chaque analyse par le nombre d’analyses réalisées. Les probabilités qui restent inférieures au seuil de significativité sont considérées comme significatives alors que celles qui sont supérieures au seuil de significativité après la correction doivent être considérées comme non significatives car pouvant être dues à une erreur de 1e espèce. Imaginons le vecteur de probabilités organisées par ordre croissant ps : ps&lt;-c(0.001,0.002,0.009,0.01,0.03,0.08) Sur ces 6 probabilités, les 5 premières doivent être considées comme révélant la présence d’un effet significatif. Cependant, 3 de ces probabilités deviennent non significatives après la correction de Bonferroni. ps*6 ## [1] 0.006 0.012 0.054 0.060 0.180 0.480 Cela signifie que, pour 3 analyses, il était tout à fait possible que l’effet était dû à une erreur de première espèce. Evidemment, il n’est pas nécessaire de réaliser cette correction à la main mais peut-être obtenu à l’aide d’une fonction R, qui est p.adjust p.adjust(ps, &quot;bonferroni&quot;) ## [1] 0.006 0.012 0.054 0.060 0.180 0.480 Si la correction de Bonferroni permet de maintenir l’erreur de 1e espèce constante, cette correction augmente le risque de seconde espèce. Pour éviter ce problème, il est préférable d’utiliser la correction de Holm. Le principe de cette correction est le même que celui utilisé pour la correction de Bonferroni, mais en prenant en compte le nombre d’analyse qu’il reste encore à corriger. Pour le formuler autrement, on multiplie chaque probabilité par son rang en ordre décroissant. rangs&lt;-rank(-ps) rangs ## [1] 6 5 4 3 2 1 ps*rangs ## [1] 0.006 0.010 0.036 0.030 0.060 0.080 Notez que, contrairement à Bonferroni, seule une probabilité est considérée comme non significative après correction alors qu’elle était significative avant la correction. A nouveau, il est possible d’utiliser la fonction p.adjuste pour obtenir cette correction automatiquement : p.adjust(ps, &quot;holm&quot;) ## [1] 0.006 0.010 0.036 0.036 0.060 0.080 Peut-être avez-vous remarquer que, dans la correction de la probabilité à la main, la 4e probabilité corrigée manuellement vaut 0.03, alors qu’elle vaut 0.036 quand elle est corrigée avec p.adjust. Ce phénomène s’explique aisément par le fait que la 4e probabilité non corrigée vaut 0.01 alors que la 3e vaut 0.009. Comme il n’est pas normal qu’une probabilité corrigée ait une valeur supérieure à une autre probabilité qui lui était supérieure quand elles n’étaient pas corrigées, la règle est qu’une probabilité corrigée ne peut pas être inférieure à une autre probabilité si cette probabilité lui était inférieure avant la correction, raison pour laquelle la 3e et la 4e probabilité ont la même valeur, à savoir 0.036. Pour ceux qui veulent développer leurs compétences de p-hacker et voir comment vous pouvez obtenir un effet significatif alors que vous n’auriez pas dû en avoir un, vous pouvez utiliser l’application shiny présente sur le lien suivant : http://shinyapps.org/apps/p-hacker/ Au finale, les situations minimales où il faudra utiliser ces corrections sont dans les matrices de corrélations et les comparaisons a posteriori dans les anova où les hypothèses. En réalité, toutes les analyses d’une même recherche devraient avoir leur probabilité corrigée, mais cette pratique est peu courante. Une dernière remarque concernant l’erreur par comparaison et l’erreur par famille. Il existe un troisième type d’erreur, appelée erreur par expérience. Cette terminologie un peu désuète à l’heure actuelle doit attirer votre attention sur le fait que, si on compare deux groupes par exemple, sur un très grand nombre de variables dépendantes différentes, le test qui sera utilisé sera probablement un test de Student qui sera répété sur chacune des variables. Etant donné qu’il s’agit d’analyses différentes, on pourrait penser que le seuil de significativité est maintenu constant. En fait, il n’en est rien. Le taux d’erreurs par famille inclut toutes les analyses conduites au sein d’une expérience. En pratique, aucune correction particulière n’est appliquée entre des analyses différentes, excepté quelquefois lorsqu’une même analyse est répétée un grand nombre de fois (des t de Student successifs par exemple). Néanmoins, il vous est grandement recommandé de réaliser le moins de comparaisons possibles. Lorsque vous ne pouvez pas éviter d’avoir un grand nombre de variables dépendantes, et donc de comparaisons, il est souhaitable de se tourner vers les analyses multivariées : MANOVA, extraction de facteurs par l’analyse factorielle exploratoire, ou analyse en composante principale pour réaliser une réduction de données. 9.5.2 D’autres manières de penser les statistiques 9.5.2.1 La taille d’effet Que ce soit en utilisant la valeur de la probabilité ou l’intervalle de confiance, on peut adresser comme critique que, aussi infime que soit la différence, avec un échantillon suffisamment grand, un effet significatif sera observé. Pour illustrer ce phénomène, regarder les deux photos ci-dessous et décider celle que vous préférez. Celle de gauche représente les lacs de Plivitce en Croatie et à droite, le coucher du soleil sur le Grand Canyon. On peut imaginer que la préférence pour le payasage dépend de la filière à laquelle vous appartenez. Ainsi, on pourrait imaginer comparer des étudiants en psychologie avec des étudiants en statistiques. Les données sont présentées dans la table 4. library(lsr) data.frame(c(&quot;europe&quot;, &quot;europe&quot;, &quot;usa&quot;, &quot;usa&quot;), c(&quot;psycho&quot;,&quot;stat&quot;, &quot;psycho&quot;, &quot;stat&quot;),c(16,14,14,16))-&gt;data1 names(data1)&lt;-c(&quot;lieu&quot;, &quot;etudiants&quot;, &quot;effectifs&quot;) tab&lt;-tapply(data1$effectifs,list(data1$lieu,data1$etudiants),sum,na.rm=TRUE) kable(tab) psycho stat europe 16 14 usa 14 16 Table 4. Table des effectifs concernant la préférence pour les images en fonction de la filière Pour tester s’il existe une dépendance entre la préférence et le type d’études, on va réaliser un \\(\\chi^2\\) d’indépendance. chisq.test(tab, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: tab ## X-squared = 0.26667, df = 1, p-value = 0.6056 Cette analyse révèle qu’il n’y a pas de lien entre la préférence et le type d’étude, \\(\\chi^2\\)(1) = 0.267, p= .6056. Néanmoins, l’échantillon sur lequel l’analyse a été faite est relativement faible. Si cet échantillon était 100 fois plus grand, voici ce que nous obtiendrions. tab*100-&gt;tab2 tab2 ## psycho stat ## europe 1600 1400 ## usa 1400 1600 chisq.test(tab2, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: tab2 ## X-squared = 26.667, df = 1, p-value = 2.418e-07 Donc, contraitement à la première analyse, cette analyse indique qu’il existe un lien entre la formation et la préférence pour la photo. Cela indique que, pour des jeux de données de taille différente, la valeur de la statistique n’est pas informative. Pour rendre les choses comparable, il faut calculer une taille d’effet. Il existe deux familles de tailles d’effet : la famille de d et la famille des \\(R^2\\). La famille des d consiste à présenter la taille d’effet comme un “distance” alors que dans la famille des \\(R^2\\), on présente la taille d’effet comme un pourcentage de variance expliquée. Pour le \\(\\chi^2\\) d’indépendance, une des mesures de la taille d’effet est le V de Cramer (qu’on pourrait élever au carré pour avoir un pseudo-pourcentage de variance expliquée). On obtient ce V de Cramer grâce à la fonction cramersV du package ‘lsr’. cramersV(tab, correct = FALSE) ## [1] 0.06666667 cramersV(tab2, correct = FALSE) ## [1] 0.06666667 On constate que, peu importe la taille d’échantillon, la valeur est identique : le V de Cramer vaut 0.067. Ainsi, la taille d’effet rend les études comparables entre elles. Pour comprendre en quoi la taille d’effet peut être considéré comme un pourcentage de variance expliqué. Intéressons-nous à la corrélation. Une étude (réelle) a montré que le nombre d’arrestations pour consommation de marijuana aux USA était significativement corrélé au nombre de colonies d’abeilles productrices de miel (les données peuvent être trouvées ici : http://tylervigen.com/view_correlation?id=1582) La corrélation entre les deux variable vaut -0.911: require(psych) corr.test(miel$Miel, miel$Marijuana) ## Call:corr.test(x = miel$Miel, y = miel$Marijuana) ## Correlation matrix ## [1] -0.91 ## Sample Size ## [1] 200 ## These are the unadjusted probability values. ## The probability values adjusted for multiple tests are in the p.adj object. ## [1] 0 ## ## To see confidence intervals of the correlations, print with the short=FALSE option Cela signifie que le pourcentage de variance expliquée est la valeur de cette corrélation au carré. round(corr.test(miel$Miel, miel$Marijuana)$r^2,4) ## [1] 0.8295 Donc, 83% de la variance est expliquée par le lien entre le nombre de colonies d’abeilles et le nombre d’arrestation pour possession de marijuana (vous savez à présent ce qu’il vous reste à faire si vous consommez de la marijuana). Pour comprendre qu’il s’agit d’un pourcentage de variance, il faut réaliser le modèle linéaire correspondant à cette corrélation (en l’occurrence, le sens a peu d’importance, l’objectif n’est pas de déterminer la variable qui prédit l’autre). modele&lt;-lm(miel$Miel~miel$Marijuana) A partir de ce modèle, on peut voir que la variance de la variable ‘Miel’ est la somme entre la variance des valeurs prédites par le modèle (obtenues par la fonction &lt;code&lt;fitted) et la variance des résidus (obtenus par la fonction residuals). var(miel$Miel) ## [1] 75406.48 var(fitted(modele))+var(resid(modele)) ## [1] 75406.48 On peut donc déterminer quel est le pourcentage de la variance de la variable ‘Miel’ qui est prédit par la variance expliquée par le modèle : var(fitted(modele))/var(miel$Miel) ## [1] 0.8294704 Cette valeur correspond à la valeur de la corrélation au carré. Le fait que la taille d’effet soit comparable entre les études fait que plusieurs revues et sociétés savantes (telle que l’American Psychological Association ) incitent à fournir les tailles d’effet. Certains auteurs recommandent même d’abandonner le report de la valeur de la probabilité (qui a finalement peu d’intérêt et dépend de la taille de l’échantillon) pour focaliser sur une analyse plus minitieuse de la taille d’effet. Une illustration intéressante de l’importance de la compréhension de la taille d’effet provient de l’analyse de l’effet de la mémantine, un traitement contre les démences de type Alzheimer. Cette question est fortement politisée à l’heure actuelle car le gouvement a décidé de ne plus rembourser les frais liés à ces médicaments. Des associations, comme l’association France Alzheimer, s’insurge en arguant que ces médicaments ont un effet, bien qu’il soit modéré. Sans vouloir entrer dans un cours complet sur la mémantine, une méta-analyse plutôt récente a été réalisée pour tester l’efficacité de la mémantine sur la sphère comportementale (Kishi &amp; Iwata, 2017). Les auteurs concluent que le médicament est efficace sur la plupart des aspects de la sphère comportementale. Cependant, ces auteurs ne semblent pas comprendre leur taille d’effet. En effet, la taille d’effet est aux alentours de 0.1 (il s’agit d’un d de Cohen). Présenté ainsi, il est difficile de se faire une opinion sur l’efficacité du médicament. Bien sûr l’effet est significatif puisque, agrégées, les études de la méta-analyse porte sur plusieurs milliers de patients. La question qu’il faut se poser est quel est le vrai impact de la mémantine sur les aspects comportementaux des patients. En moyenne, une personne atteinte de la maladie d’Alzheimer va vivre avec sa maladie 10 ans. La mémantine coûte environ 36 euros pour 28 pillule à raison d’une pillule par jour. Le coût pour le patient est donc de 4741 euros au bout de 10 ans. Si vous décidiez de soudoyer un enseignant et qu’il vous demandait 4741 euros pour augmenter votre moyenne de 0.25 points (données réelles sur les notes des étudiants de psychologie), est-ce que vous estimeriez que c’est un bon investissement ? Pourtant, voilà à quoi correspond l’effet de la mémantine et voilà pourquoi il est indispensable de comprendre la notion de taille d’effet. 9.5.2.2 L’intervalle de confiance Normalement, il est acquis à présent que, lorsque vous faites une analyse statistique, le test que vous réalisez teste l’absence de différence et que, si la probabilité renvoyée est inférieure au seuil de significativité alors vous considérez que la différence est significative. On peut appliquer le principe de manière similaire avec l’intervalle de confiance. Généralement, un intervalle de confiance est à 95% (le complément de l’erreur de \\(1^e\\) espèce). Il s’agit des limites entre lesquelles le paramètre que vous estimez va se situer dans 95% des situations. La Figure 8 illustre l’intervalle de confiance pour l’estimation d’une moyenne. Cinquante échantillons ont été tirés aléatoirement dans une population et on estime la moyenne de chacun de ces échantillons ainsi que l’intervalle de confiance autour de la moyenne estimée pour chacun d’eux. On observe que 95% (96% pour être précis) des estimations l’intervalle de confiance autour de la moyenne estimée coupe l’axe verticale de la moyenne de la population. Ainsi, quand l’intervalle de confiance recouvre le paramètre, cela indique que la moyenne estimée ne se différencie pas significativement de la moyenne de la population. Figure 8. Illustration de l’intervalle de confiance. Vous pouvez reproduire ce type de graphique à l’aide du package ‘TeachingDemos’ library(&quot;TeachingDemos&quot;) ci.examp() Il est possible de calculer un intervalle de confiance sur pratiquement tout. Lorsque l’intervalle de confiance porte sur une statistique, on considère qu’il n’y a pas de différence significative lorsque l’intervalle de confiance comprend la valeur de l’hypothèse nulle. Par exemple, pour un t de Student, l’hypothèse nulle est qu’il n’y a pas de différence entre deux moyennes, et si c’est le cas, la valeur du t vaudra 0. Pour le comprendre, nous allons l’illustrer à l’aide d’une étude que nous avons réalisée et qui visait à déterminer pourquoi les super-héros gagnent toujours à la fin. Nous avons émis l’hypothèse que c’était tout simplement parce qu’ils étaient plus forts. A l’aide d’un matériel adapté complètement expérimental et top secret, nous avons donc mesuré la force de super-héros et de super-vilains. Les données sont présentées dans le Tableau 2 Pour comparer la force de ces deux groupes, l’analyse la plus naturelle est de comparer la force moyenne des super-héros à celle des super-vilains. Le test adapté pour faire cette analyse est le t de Student pour échantillons indépendants ## ## Welch Two Sample t-test ## ## data: Force by Groupe ## t = -0.44532, df = 29.959, p-value = 0.6593 ## alternative hypothesis: true difference in means between group super_héros and group super_vilains is not equal to 0 ## 95 percent confidence interval: ## -8.728694 5.603694 ## sample estimates: ## mean in group super_héros mean in group super_vilains ## 83.3750 84.9375 Cette analyse fournit un intervalle de confiance indiquant que la “vraie” valeur du t de Student est située entre -8.72 et 5.60. Comme cet intervalle recouvre le zéro, on ne peut pas exclure que les deux groupes ne se différencient pas sur leur force. Cette manière d’envisager les statistiques permet d’identifier rapidement une différence significative grâce aux représentations graphiques représentation graphique. Dans ce graphique, la ligne horizontale du milieu représente la moyenne, tandis que les limites supérieures et inférieures de la boîte représente la moyenne plus une erreur-type et la moyenne moins une erreur-type. Enfin, les extrémités des barres verticales représente l’intervalle de confiance à 95%. Il existe deux manière de lire ce graphique : la première manière consiste à regarder la barre verticale. Si elle dépasse la barre horizontale représentant la moyenne de l’autre groupe, cela signifie que la différence n’est pas significative. Si elle ne l’atteint pas, c’est que la différence est significative. Comme l’intervalle de confiance à 95% est calculé en multipliant l’erreur-type par 1.96, on peut arriver à la même conclusion avec l’erreur-type comme information uniquement. En l’occurrence, comme les super-vilains ont une moyenne supérieure en force, on peut savoir si cette différence est significative en regardant la limite inférieure de la boîte (la ligne horizontale inférieure). Si cette ligne inférieure est au-dessus de la ligne supérieure de la boîte des super-héros, cela signifie qu’il y a au moins 2 erreurs-types qui séparent les deux groupes, et donc que la différence est significative. Dans notre exemple, la limite inférieure des super-vilains est à un niveau inférieur à la limite supérieure des super-héros. La différence est donc non significative. Il est de plus en plus souvent demandé de fournir les intervalles de confiance dans les articles. Si certains chercheurs ne sont pas convaincus de leur utilité, pour Thompson (2002), l’intervalle de confiance est un moyen pour les tailles d’effets. Néanmoins, à partir de la probabilité, on peut retrouver sans difficulté l’intervalle de confiance (Altman, 2011). On peut raisonnablement penser que ces intervalles de confiance sont utiles dans les représentations graphiques, et que son utilité de fournir des informations sur l’intervalle à l’intérieur duquel se trouve le vrai paramètre dans 95% des situations (Jiroutek, &amp; Turner, 2016). note : il s’agit ici d’un abus de langage. En réalité, on devrait dire que, étant donné une moyenne d’une population, l’estimation de la moyenne d’un échantillon a 95% de chance d’être inclus dans l’intervalle de confiance. Ainsi, comme on ne peut avoir accès directement aux paramètres, on peut créé un intervalle autour de l’estimation du paramètre qui englobera le paramètre dans 95% des situations. 9.5.2.3 Les bootstraps Dans de nombreuses situations, l’hypothèse faite sur la distribution des données est que la distribution suit une distribution normale. Cependant, cette condition est loin d’être systématiquement respectée. Une manière de contourner ce problème pour estimer un paramètre de la manière la plus optimale possible est de réaliser un bootstrap. Cette technique généraliste consiste à échantillonner dans un échantillon un grand nombre de fois. L’échantillon créé par bootstrap est de taille identique à la taille initiale de l’échantillon. Les échantillons sont néanmoins différents à chaque itération car l’échantillonnage est réalisé avec remise. Ainsi, l’échantillon est composé de Pierre, Paul, Jacques. Comme il y a remise, Pierre peut donc être tiré plusieurs fois et votre premier échantillon de bootstrap pourrait être Pierre, Pierre, Paul. Evidemment, pour que le bootstrap ait du sens, il faut que la taille de l’échantillon soit suffisamment importante pour que le même échantillon ne revienne pas dans la répétition de l’opération. L’intérêt de réaliser un boostrap est qu’on se dégage de la distribution des données puisqu’on va créer notre intervalle de confiance par simulation. Donc, peu importe la distribution initiale des données, le bootstrap s’y adapatera puisque la simulation de la distribution se basera sur la forme spécifique de la distribution des données de notre échantillon. Dans une de nos études, nous n’avons pas très bien compris l’expérience de Milgram sur la soumission à l’autorité. Dans l’étude originale de Milgram, les participants doivent soumettre des chocs électriques à un autre participant chaque fois qu’il commet une erreur à une question. Ces chocs deviennent de plus en plus immportant au fur et à mesure que le nombre d’erreurs augmente. A l’insu des participants, la personne qui reçoit les chocs est un comparse de l’expérimentateur qui joue la comédie car elle ne reçoit pas vraiment des chocs. Dans notre étude, nous n’avions pas vraiment compris que c’était un comparse, alors nous avons administré à nos participants des chocs électriques pour qu’ils apprennent à jouer aux fléchettes (pour ceux/celles qui auraient le moindre doute sur la question, et qui voudraient me signaler aux comités d’éthique ou aux CPP, nous tenons à leur préciser que cette étude est un fake … notre laboratoire ne dispose pas de dispositif pour administrer des chocs électriques). Nous avons voulu savoir si les personnes soumises à un choc électrique envoyaient leurs fléchettes plus près du centre de la cible. On sait que la distance habituelle du centre de la cible est de 34 mm dans notre cas. Nous avons donc réalisé cette étude auprès de 50 personnes. Voici les données : ## [1] 40 53 37 47 35 32 47 41 53 35 56 47 43 55 40 38 43 23 36 58 46 34 37 39 29 ## [26] 45 46 29 54 42 38 36 50 50 32 37 40 44 41 59 33 55 47 51 46 31 43 51 11 39 Pour déterminer si le groupe soumis à un choc électrique est plus proche du centre de la cible, il faut réaliser un t de Student comparaison à une norme. mean(fleche) ## [1] 41.88 (mean(fleche)-34)/(sd(fleche)/50^0.5)-&gt;t.value # le 34 mm est la norme connue dans la population t.value ## [1] 5.911544 Dans le bootstrap, on va tirer au hasard avec remise un nombre d’observations de notre échantillon qui correspond à la taille de l’échantillon. Pour faire un tirage au sort, on utilise la fonction sample. On indique dans cette fonction la taille d’échantillon qu’on veut échantillonner et l’argument ‘replace’ permet de préciser si c’est un tirage avec remise ou non. L’argument ‘T’ (pour TRUE) indique que c’est avec remise. Comme notre échantillon fait 50 personnes, il faut que chaque échantillon du boostrap soit composé de 50 personnes. sample(fleche,50,replace = T) ## [1] 43 43 36 32 38 32 33 47 43 38 23 35 29 58 55 50 47 37 53 37 35 34 43 29 47 ## [26] 47 50 39 41 40 56 34 40 46 35 53 37 38 35 32 47 32 32 53 32 53 45 59 42 58 Sur ce nouvel échantillon, on peut donc calculer un nouveau t de Student comparaison à une norme qui va avoir une valeur différente de l’échantillon initial. f2&lt;-sample(fleche,50,replace = T) t2&lt;-(mean(f2)-34)/(sd(f2)/50^0.5) t2 ## [1] 8.344773 Le principe du bootstrap est de répéter cette opération un grand nombre de fois (par exemple 1000, et au minimum 500) pour créer l’intervalle de confiance sur un paramètre ou une statistique. La boucle for permet de réaliser cette opération. On va créer un vecteur avec 1000 t de Student comparaison à une norme basé sur des échantillonnage avec remise. Notez qu’on peut également faire le bootstrap sur la moyenne pour connaître l’intervalle à l’intérieur duquel la vraie moyenne à 95% de chance d’être. for(i in 1:999){ f2&lt;-sample(fleche,50,replace = T) t3&lt;-(mean(f2)-34)/(sd(f2)/50^0.5) t2&lt;-c(t2, t3) } A présent, l’objet t2 dans la mémoire de R est composé de 1000 valeurs, qui correspondent chacune à un t de Student comparaison à une norme qu’on a obtenues par échantillonnage avec remise. Nous pouvons obtenir l’intervalle de confiance sur ces t de Student. Il existe plusieurs méthodes pour obtenir cet intervalle de confiance. L’une d’entre elle consiste à utiliser les percentiles. Comme nous voulons les 95% pourcents des t au centre de la courbe, on répartit les 5 pourcents de manière équitable sur les deux extrémités, c’est-à-dire 2.5% de chaque côté. On obtient ces centiles grâces à la fonction quantile en précisant les valeurs des centiles désirées, donc 0.025 et 0.975 dans notre cas. quantile(t2, c(0.025,0.975)) ## 2.5% 97.5% ## 3.565388 9.138221 En l’occurrence, l’intervalle de confiance ne recouvre pas le 0, ce qui signifie qu’il est improbable que la valeur du t soit égale à 0. On peut donc interpréter cet intervalle comme révélant la présence d’un effet significatif. Dans notre exemple, nous avons réaliser le boostrap sur la valeur de la statistique, mais on aurait pu le faire sur la moyenne. Dans ce cas, si l’intervalle ne recouvrait pas 34 (la norme), c’est-à-dire que 34 n’était pas compris entre la limite inférieure et la limite supérieure de l’intervalle de confiance, alors c’est qu’il y avait une différence significative. Dans le cas contraire, la différence était non significative. Comme d’habitude, il n’est pas nécessaire de programmer soi-même le bootrap. Il existe un ensemble de packages dans R qui le font à votre place et, pour certains packages ont développé des fonctions spécifiques de bootstrap, qui sont prêt à l’emploi. En l’occurrence, la fonction boot du package ‘boot’ permet de très bien faire cela. Nous allons en l’occurrence faire un boostrap sur une moyenne. library(boot) # on commence par créer une fonction où il faut des données et les itérations. meanfun &lt;- function(data, i){ d &lt;- data[i] # on stocke une itération donnée dans l&#39;objet d return(mean(d)) # on fait moyenne } boot.out&lt;-boot(fleche, statistic = meanfun, R=1000) boot.ci(boot.out) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = boot.out) ## ## Intervals : ## Level Normal Basic ## 95% (39.26, 44.55 ) (39.18, 44.64 ) ## ## Level Percentile BCa ## 95% (39.12, 44.58 ) (39.09, 44.56 ) ## Calculations and Intervals on Original Scale Notez que, comme les échantillons ne sont pas les mêmes d’une fois à l’autre, l’intervalle de confiance sera sensiblement modifié entre deux répétitions. Pour terminer sur le bootstrap, comme précisé plus haut, il existe différentes manières de calculer l’intervalle de confiance. La méthode la plus robuste est le bootrap corrigé pour des biais d’asymétrie (BCa) (Efron &amp; Gong, 1983). 9.6 L’approche par maximum de vraisemblance. Le maximum de vraisemblance est un concept essentiel au niveau statistique car il amène la personne qui veut réaliser des statistiques à penser en termes de modèle. Il s’agit habituellement d’un estimateur qui est utilisé dans les techniques statistiques plus avancées telles que la régression logistique, les analyses factorielles ou les modèles linéaires mixtes, et plus généralement dans tous les modèles non linéaires. Néanmoins, on peut l’utiliser pour estimer n’importe quel paramètre. En général, le maximum de vraisemblance est un méthode pour obtenir les estimations de paramètres inconnus en optimisant la fonction de vraisemblance. Le principe la vraisemblance est d’estimer la probabilité d’observer les données de manière itérative. Ainsi, on part d’une valeur “aléatoire”, qui va être la solution initiale, et on va chercher à améliorer cette solution initiale de sorte à augmenter, c’est-à-dire optimiser, la probabilité d’observer cette distribution de données par simulation. Le maximum de vraisemblance est la valeur de la probabilité pour laquelle les paramètre seront optimisés pour avoir la probabilité maximale. Quand cette probabilité a atteint un maximum (moyennant une certaine tolérance), on dit que le modèle a convergé. 9.6.1 Explication mathématique AVERTISSEMENT : cette section peut être ignorée, en particulier pour ceux à qui les formules mathématiques donnent la nausée. Dans ce cas, vous pouvez vous reporter à la section suivante sur le ratio de vraisemblance. Pour les autres, que Dieu ait votre âme. D’un point de vue mathématique, la vraisemblance est la densité de probabilité associée aux données observées : \\[L_x(\\theta)= f(x)\\] Pour illustrer le concept, nous allons reprendre l’exemple des chocs électriques reçus par les participants quand ils s’éloignent du centre de la cible. On peut calculer la vraisemblance de la moyenne Pour une variable numérique, la vraisemblance est maximale lorsque la moyenne et l’écart-type sont identiques à la moyenne et l’écart-type calculés. mean(fleche) ## [1] 41.88 sd(fleche) ## [1] 9.425627 Dans cet exemple assez simpliste, nous voulons donc estimer la moyenne et l’écart-type. Fondamentale, on peut les calculer l’un après l’autre assez aisément. Cependant, dans des situations plus complexes, ce ne sera pas le cas. Nous allons donc essayer de deviner la moyenne et l’écart-type sur la base des données : fleche ## [1] 40 53 37 47 35 32 47 41 53 35 56 47 43 55 40 38 43 23 36 58 46 34 37 39 29 ## [26] 45 46 29 54 42 38 36 50 50 32 37 40 44 41 59 33 55 47 51 46 31 43 51 11 39 Les données sont comprises grosso modo entre 20 et 68. Il est raisonnable de considérer que la vraie moyenne se situe quelque part entre ces deux valeurs et que l’écart-type vaut environ \\(1/4\\) de cette distance puisque 95% des observations sont comprises entre -1.96 et +1.96 écart-types. Ainsi, je devine que la moyenne est à 44 et l’écart-type vaut 12. Je peux calculer la vraisemblance de cette estimation en calculant la densité de probabilité associée aux données (en considérant que les données suivent une distribution normale) Ainsi, pour chaque observation d’une variable numérique, on peut estimer la densité de probabilité de cette observation sur la base d’une distribution normale (pour simplifier un peu, la hauteur de la courbe normale). dnorm(fleche, mean= 44 , sd= 12, log=F)[1:10] # on obtient la densité de probabilité pour chaque observation ## [1] 0.03144860 0.02509479 0.02804390 0.03222234 0.02509479 0.02016423 ## [7] 0.03222234 0.03222234 0.02509479 0.02509479 La fonction de vraisemblance est obtenue en appliquant le logarithme à chacune des observations. dnorm(fleche, mean= 44 , sd= 12, log=T)[1:10] # on obtient la fonction de vraisemblance pour chaque observation ## [1] -3.459401 -3.685095 -3.573984 -3.435095 -3.685095 -3.903845 -3.435095 ## [8] -3.435095 -3.685095 -3.685095 On obtient la vraisemblance en additionnant le logarithme des probabilités d’observer cette distribution. sum(dnorm(fleche, mean= 44 , sd= 12, log=T)) ## [1] -186.0881 La valeur de la vraisemblance est ici particulièrement faible. Nous allons tenter de l’améliorer. Pour cela, je vais modifier les paramètres et déterminer si on peut augmenter la vraisemblance. Par exemple, je peux tester si la moyenne n’est pas à 43 plutôt qu’à 44. sum(dnorm(fleche, mean= 43 , sd= 12, log=T)) ## [1] -185.5256 La vraisemblance à augmenter. Je vais donc continuer et tenter de voir si la moyenne n’est pas à 42 au lieu de 43. sum(dnorm(fleche, mean= 42 , sd= 12, log=T)) ## [1] -185.3103 La vraisemblance continue à augmenter. On va donc continuer à estimer nos deux paramètres de manière itérative jusqu’à atteindre un maximum. Ce maximum sera atteint quand la moyenne testé sur la moyenne de nos données et l’écart-type sera l’écart-type des données : sum(dnorm(fleche, mean= 39.72 , sd= 10.876, log=T)) ## [1] -184.6622 On peut évidemment obtenir directement ces valeurs. Le package ‘EstimationTools’ permet de manipuler les différents paramères avec la fonction maxlog. library(&quot;EstimationTools&quot;) ## Le chargement a nécessité le package : survival ## ## Attachement du package : &#39;survival&#39; ## L&#39;objet suivant est masqué depuis &#39;package:boot&#39;: ## ## aml ## ## &gt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; EstimationTools Version 4.3.1 &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&lt; ## Feel free to report bugs in https://github.com/Jaimemosg/EstimationTools/issues fit1 &lt;- maxlogL(x = fleche, dist = &#39;dnorm&#39;, start=c(2, 3),lower=c(20, 5), upper=c(65, 20)) fit1$fit$par ## mean sd ## 41.880000 9.330895 fit1$fit$objective ## [1] -182.6135 Remarquez qu’il y a une toute petite différence. Cela est dû au fait que l’estimation de l’écart-type est légèrement différente de celle calculée. Dans ce modèle, la vraisemblance est égale à -188.7698. Cette valeur n’est pas directement utilisable. Il est nécessaire de calculer la déviance en la multipliant par -2. -2*-188.9068 # calcul de la déviance ## [1] 377.8136 9.6.2 Le ratio de vraisemblance (LRT) Le ratio de vraisemblance (LRT) est la différence entre deux déviances. Cette différence se distribue approximativement comme un \\(\\chi^2\\) ayant comme degrés de liberté le nombre de paramètres estimés. \\[LRT = -2 [l(\\theta-l(\\hat\\theta))]\\] LRT&lt;-(-2*(sum(dnorm(fleche, mean= 44 , sd= 12, log=T)) - sum(dnorm(fleche, mean= 39.72 , sd= 10.876, log=T)) )) LRT ## [1] 2.851758 Ainsi, il faut se demander un \\(\\chi^2\\) de 2.851758 ayant 2 degrés de liberté est significatif. pchisq(LRT, df=2, lower=F) # calcule la probabilité de la valeur LRT avec 2 degrés de liberté, en donnant la probabilité d&#39;avoir une valeur supérieure à cette valeur (lower = F) ## [1] 0.2402972 9.6.3 Les critères d’information. Les critères d’information fournissent une manière d’évaluer l’ajustement d’un modèle sur la base de la valeur optimale du log de vraisemblance tout en pénalisant les modèles qui ne sont pas parcimonieux. L’intérêt des critères d’information est qu’ils permettent de comparer n’importe quels modèles ajustés sur le même jeu de données. Il n’est donc pas nécessaire que les modèles soient emboîtés. Il existe deux critères d’information qui sont particulièrement utilisés : le AIC (Critère d’Informations d’Akaike) et le BIC (Critère d’Information de Bayes). Dans les deux cas, la logique est la même : on va s’appuyer sur le calcul de la vraisemblance, mais en pénalisant le modèle à chaque paramètre estimé. Pour le AIC, la déviance va être pénalisée en additionnant 2 fois le nombre de prédicteur. \\[ AIC=-2×l(β ̂,θ ̂ )+2p \\] Dans notre modèle, le log de vraisemblance vaut 2.851758, la déviance vaut donc -2$$2.851758, ce qui donne -5.703516. Les degrés de liberté résiduels sont de 2. Le AIC vaut donc : -2*fit1$fit$objective + 2 * 2 # ou le second 2 représente le nombre de paramètres estimés ## [1] 369.2269 qu’on obtient directement avec la fonction AIC : AIC(lm(fleche~1)) # on réalise un modèle linéaire où on ne calcule que la constante, ce qui permet d&#39;estimer la moyenne et la variance (ou l&#39;écart-type) ## [1] 369.2269 Pour le critère d’information de Bayes, le principe est assez similaire, la différence porte sur la manière de calculer la pénalité pour le manque de parcimonie. Ainsi, le nombre de paramètres estimés sera multiplié par le logarithme de la taille de l’échantillon. \\[BIC=-2×l(β ̂,θ ̂ )+p×ln(n) \\] -2*fit1$fit$objective + 2 * log(50, base=exp(1)) ## [1] 373.051 qu’on obtient directement avec la fonction BIC : BIC(lm(fleche~1))# on réalise un modèle linéaire où on ne calcule que la constante, ce qui permet d&#39;estimer la moyenne et la variance (ou l&#39;écart-type) ## [1] 373.051 Puisque ces deux critères d’informations s’appuient sur la déviance pour le calcul, à laquelle on va ajouter une pénalité aux modèles pour lesquels de nombreux paramètres sont estimés, cela signifie que, comme pour la déviance où, plus la valeur est faible, plus les données sont ajustées, plus la valeur pour ces deux critères d’informations est petite, meilleur est l’ajustement. 9.7 L’approche bayesienne Comme nous l’avons évoqué, pour publier, il faut des résultats, résultats dits positifs. La raison de ce phénomène est que lorsqu’on tolère l’hypothèse nulle, nous n’avons pas d’information sur le fait qu’on n’a pas assez d’information pour avoir un résultat significatif ou s’il n’y a vraiment pas d’effet. Pour le formuler autrement l’absence de preuve n’est pas la preuve de l’absence. Cette difficulté est une des grosses limites des tests d’hypothèse nulle. Une autre critique fréquemment adressée aux tests d’hypothèse nulle est que le seuil à 0.05 est arbitraire, c’est pourquoi un certain nombre de chercheurs encouragent l’utilisation des facteurs bayesiens. 9.7.1 Les facteurs bayesiens : un rapport entre deux vraisemblances Supposons un amphi pour lequel on se demande si la parité est respectée. Dans cet amphi, il y a 200 personnes, dont 160 femmes. La probabilité que la parité soit respectée s’obtient par une binomiale \\[ \\begin{pmatrix} 200 \\\\160 \\end{pmatrix} 0,5 ^{160} (1-0,5)^{40}\\ \\] et vaut p.h0&lt;-dbinom(160, 200, 0.5,FALSE) p.h0 ## [1] 1.275816e-18 L’hypothèse alternative à cette hypothèse est que la parité n’est pas respectée. La probabilité que la parité ne soit pas respectée (le ratio h/f est n’importe quelle autre valeur que 0,5) s’obtient par : \\[\\int_{0}^{1} \\begin{pmatrix} 200 \\\\160 \\end{pmatrix} p ^{160} (1-p)^{40}\\, \\mathrm{d}p \\] ** note : pour être parfaitement précis, il faudrait retirer de l’intégrale la probabilité d’avoir 0.5) et vaut : fdbinom&lt;-function(p=NULL){ r&lt;-dbinom(160, 200, p,FALSE) return(r) } p.h1&lt;-integrate(fdbinom, 0,1) p.h1$value ## [1] 0.004975124 Selon les tests d’hypothèse nulle, on est amené à rejeter à la fois l’hypothèse de la parité et l’hypothèse selon laquelle la parité n’est pas respectée. Le facteur bayesien peut résoudre ce problème. En effet, il s’agit du rapport entre ces deux probabilités. ## 1.275816e-18 / 0.004975124 = 3.899561e+15 Ainsi, on sait quel est l’hypothèse la plus probable. Un facteur bayesien s’interprète de la manière indiquée dans le Tableau 4. Notez que ce tableau teste l’hypothèse nulle. Les critères doivent être inversés si on teste l’hypothèse alternative. Tableau 4. Seuil pour interpréter les facteurs bayesiens (Wagenmakers et al., 2011) Cette approche permet donc de résoudre le problème des tests d’hypothèse nulle consistant à pouvoir tester une absence de différence. Pour le comprendre, prenons deux groupes de 1000 personnes ayant exactement la même moyenne (i.e., 0) et le même écart-type (i.e.,1). groupe1&lt;-rnorm(1000, 0, 1) groupe2&lt;-rnorm(1000, 0, 1) Pour comparer ces deux groupes, on va classiquement réaliser un t de Student pour échantillons indépendants. Nous allons à présent pouvoir comparer l’évolution de la décision lorsque la taille d’échantillon augmente par rapport tant lorsqu’on adopte un test d’hypothèse que lorsqu’on utilise les facteurs bayesiens. library(BayesFactor) ## Le chargement a nécessité le package : coda ## Le chargement a nécessité le package : Matrix ## ************ ## Welcome to BayesFactor 0.9.12-4.7. If you have questions, please contact Richard Morey (richarddmorey@gmail.com). ## ## Type BFManual() to open the manual. ## ************ library(ggplot2) list.t&lt;-c() list.bf&lt;-c() for(i in 5:1000){ ttest&lt;-t.test(groupe1[1:i], groupe2[1:i]) list.t&lt;-c(list.t, ttest$p.value) bf&lt;-ttestBF(x = groupe1[1:i], y = groupe2[1:i], rscale = &quot;medium&quot;) list.bf&lt;-c(list.bf, extractBF(bf)$bf) } simul&lt;-data.frame(index=5:1000, &quot;Bayes.Factor&quot;=list.bf,&quot;test.t.p&quot;=list.t) cutoff.bf &lt;- data.frame( x = c(-Inf, Inf), y =0.33, cutoff = factor(50) ) cutoff.bf2&lt;-data.frame( x = c(-Inf, Inf), y =3, cutoff = factor(50) ) p&lt;-ggplot(simul, aes(x=index, y=Bayes.Factor))+geom_point()+ ylim(0,3.5)+geom_line(aes( x, y, linetype = cutoff ), cutoff.bf)+ geom_line(aes( x, y, linetype = cutoff ), cutoff.bf2)+ xlab(&quot;Taille de l&#39;échantillon&quot;) + ylab(&quot;Valeur du facteur bayesien&quot;)+ theme(legend.position=&quot;none&quot;)+ labs(title=&quot;Figure 11. Evolution de la valeur du Facteur Bayesien pour un t de Student \\n en fonction de la taille de l&#39;échantillon.&quot;)+ theme(axis.line.x = element_line(color=&quot;black&quot;, size = 0.6), axis.line.y = element_line(color=&quot;black&quot;, size = 0.6))+ theme(text = element_text(size=10)) cutoff &lt;- data.frame( x = c(-Inf, Inf), y = 0.05, cutoff = factor(50) ) p2&lt;-ggplot(simul, aes(x=index, y=test.t.p))+geom_point()+ geom_line(aes( x, y, linetype = cutoff ), cutoff)+ xlab(&quot;Taille de l&#39;échantillon&quot;) + ylab(&quot;Valeur de la probabilité du test t&quot;)+ theme(legend.position=&quot;none&quot;)+ labs(title=&quot;Figure 10. Evolution de la probabilité pour un t de Student \\n en fonction de la taille de l&#39;échantillon.&quot;)+ theme(axis.line.x = element_line(color=&quot;black&quot;, size = 0.6), axis.line.y = element_line(color=&quot;black&quot;, size = 0.6))+ theme(text = element_text(size=10)) Commençons pour regarder l’évolution de la probabilité, qui est présentée en Figure 10. On se rend compte que peu importe la taille de l’échantillon, la probabilité associée au test de Student peut se situer entre n’importe quelle valeur sur une échelle allant de 0 à 1, avec un certain nombre de valeurs inférieures au seuil de significativité à 0.05, identifié par la droite horizontale. Ceci amène donc à la conclusion que peu importe la probabilité, nous ne sommes pas en mesure de tirer une conclusion. Si on s’intéresse à présent à l’évolution du Facteur Bayes (Figure 11). On constate que le FB ne dépasse jamais le seuil de 3 (ligne horizontale du haut) qui indique la présence d’une différence significative entre les deux groupes. En revanche, à partir d’un échantillon suffisamment élevé, le FB ne dépasse plus le seuil de 0.33 (la ligne horizontale du bas) qui indique des éléments en faveur de l’absence de différence. Il s’agit donc d’une grosse plus-value des facteurs bayesiens. Pour utiliser les facteurs bayesiens, on applique le même raisonnement que pour les tests d’hypothèse nulle : donc si vous devez un t de Student, vous ferez un t de Student avec une approche bayesienne aussi. Le package R qui permet d’adopter cette approche est le package ‘BayesFactor’ (il en existe d’autres, mais ils sont un peu plus complexes d’utilisation). Nous pouvons reprendre notre exemple sur la force des super-héros pour illustrer la manière dont la fonction doit être utilisée. bf&lt;-ttestBF(formula=Force~Groupe, data=super, rscale = &quot;medium&quot;, paired=F) bf ## Bayes factor analysis ## -------------- ## [1] Alt., r=0.707 : 0.3628706 ±0% ## ## Against denominator: ## Null, mu1-mu2 = 0 ## --- ## Bayes factor type: BFindepSample, JZS Vous avez peut-être identifié que, par rapport à la fonction du test de Student classique, il y a un argument supplémentaire, qui est le rscale. Le r-scale correspond à la taille d’effet en valeur absolue que le chercheur estime pouvoir observer dans plus de 50% des situations. La valeur par défaut (“medium”) pour le test t est de 0.707. En d’autres termes, dans les facteurs bayesiens, il faut explicitement préciser la taille d’effet. C’est ce qu’on appelle le prior. Pour les utilisateurs des tests d’hypothèse nulle, il est raisonnable d’utiliser comme prior la valeur du d de Cohen qu’on s’attend à observer (Lakens, 2018). Une autre différence par rapport aux tests d’hypothèse nulle classique est que la distribution qui est utilisée est une distribution Cauchy plutôt qu’une distribution normale. Cependant, comme le montre la Figure 12, la différence entre les deux distribution est suffisamment subtile pour ne pas devoir s’en soucier. En effet, il n’est pas moins ou plus raisonnable de faire l’hypothèse d’une distribution normale et d’une distribution Cauchy pour a distribution des données. Références Bayes, T., &amp; Price, R. (1763). An essay towards solving a problem in the doctrine of chances. Philosophical Transactions of the Royal Society of London, 53, 370–418. https://doi.org/10.1098/rstl.1763.0053 LaPlace, P. S. (1814). Essai philosophique sur les probabilités. Courcier. http://eudml.org/doc/203193 Neyman, J., &amp; Pearson, E. S. (1933). Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 231(694–706), 289–337. https://doi.org/10.1098/rsta.1933.0009 Royall, R. M. (1997). Statistical evidence: A likelihood paradigm. Chapman &amp; Hall. nous verrons plus loin les limites de la significativité telle que nous allons la décrire dans cette partie↩︎ "],["quel-outil-choisir.html", "Chapter 10 Quel outil choisir 10.1 L’approche par comparaison de modèles ou l’approche par test ? 10.2 Comment utiliser l’outil d’aide à la décision ?", " Chapter 10 Quel outil choisir “There are as many statistical tests as there are statisticians.” — Auteur inconnu 10.1 L’approche par comparaison de modèles ou l’approche par test ? Quand il s’agit d’enseigner les statistiques, et en particulier, apprendre aux personnes à choisir l’outil adapté à leurs besoins, deux grandes approches peuvent être utilisées. La première consiste à penser les choses en termes de modèles. La seconde consiste à penser les choses en termes d’outils en fonction des variables qui doivent être analysées. Dans l’approche par modèle, l’idée est de pouvoir résoudre l’équation (10.1) \\[\\begin{equation} Données=Modèle + Erreur \\tag{10.1} \\end{equation}\\] Cette approche a pour avantage qu’elle est élégante et synthétique pour expliquer les statistiques. Elle est également l’approche que vous devriez privilégier si vous préférez une approche bayésienne ou un approche de vraisemblance pour mener vos inférences. Cependant, ^pour les utilisateurs débutants, elle pourrait présenter deux écueils : l’objectif quand on se forme aux statistiques est d’être capable de réaliser ses statistiques soi-même (ce que cette méthode permet), mais également de pouvoir comprendre ce que les autres ont fait (ce qui n’est pas possible si on n’a pas une culture sur les outils statistiques qui existent) ; si, pour les personnes qui ont déjà une bonne maîtrise des outils statistiques, cette approche est limpide, formaliser le modèle requiert des capacités d’abstraction concernant les données qui peuvent représenter un vrai challenge, en particulier pour les modèles plus complexes. Nous laisserons le lecteur choisir l’approche qui lui convient le mieux en indiquant dans les chapitre correspondant comment formaliser le modèle pour les différents cas de figures, et dans tous les cas, nous encourageons les lecteurs à pouvoir avoir suffisamment de flexibilité pour penser les statistiques des deux manières. Dans ce chapitre, nous proposerons un outil d’aide à la décision dont la finalité est de guider les utilisateurs des statitiques à utiliser l’outil qu’il sera le plus naturel d’utiliser en fonction de ses besoins. Il est important ici d’avoir identifié que le terme utilisé est naturel et non adapté. En effet, certains outils peuvent représenter de meilleures alternatives mais sont plus complexes à la fois en terme de conceptualisation théorique et en termes de mise en oeuvre pratique. Par exemple, les modèles linéaires mixtes représentent une meilleure alternative aux analyses de variances en mesures répétées, mais comprendre les modèles linéaires mixtes et le mettre en oeuvre de manière effective est plus complexe. Ainsi, dans le cadre de cet outil d’aide à la décision, des étudiant·es de licence ou de master pourront se satisfaire de l’analyse à privilégier, tandis que des doctorant·es ou des chercheur·ses devraient privilégier les outils proposés dans le point **remarques importantes* quand elles sont présentes. Avant d’aborder la manière d’utiliser cet outil, il faut avoir conscience qu’il est quasiment impossible de faire la liste complète des outils existants. Cet outil d’aide à la décision représente donc une option acceptable, robustesse et correspondant aux pratiques de la recherche en psychologie. 10.2 Comment utiliser l’outil d’aide à la décision ? La clé centrale pour identifier correctement la bonne analyse à réaliser est d’être capable d’identifier correctement les variables. Plusieurs étapes devront être menées à bien pour pouvoir être guidé vers la bonne analyse : Identifier le nombre de variables dépendantes Identifier la nature de la variable dépendante Identifier le nombre de variables indépendantes et de contrôle Identifier la nature des variables indépendantes et de contrôle Si les variables indépendantes sont qualitatives, il vous faudra aussi identifier le nombre de modalités de la variable indépendante ainsi que la nature inter11 ou intra-indivudelle12 de ces variables. Cet outil vous proposera alors l’analyse que vous devriez réaliser si vous êtes en licence ou master, fera un rappel sur les conditions d’application et proposera des alternatives à l’analyse de prédilectionà envisager dans l’hypothèse où les conditions d’application de l’analyse à privilégier ne sont pas respectées. Pour finir, des informations supplémentaires peuvent être fournies : des points d’attention ou des alternatives à envisager pour les personnes qui ont un niveau de maîrise des outils statistiques plus avancé. 10.2.1 Une seule variable dépendante Étape 1 : identifier la nature de la variable dépendante Variable dépendante quantitative Étape 2 : Identifier le nombre de variables indépendantes Il n’y a pas de variable indépendante Étape 3 : Identifier le nombre d’observations Une seule observation Analyse à privilégier : score z Condition d’application : ✓ Normalité de la distribution Que faire en cas de non respect des conditions d’application ? : ✓ Utilisation des centiles Points d’attention : ✓ Il est nécessaire d’avoir à disposition un échantillon de référence auquel on peut comparer l’observation Remarque importante : Si l’usage, en particulier des cliniciens, du score z s’est imposé, c’est surtout en raison de sa facilité de calcul. Cependant, cette manière de faire est en réalité un usage impropre du score z car, une méthode qui autorise des inférences doit prendre en compte la taille de l’échantillon de référence sur lequel on s’appuie, ce qui n’est pas le cas dans le cadre du score z (Crawford &amp; Howell, 1998). C’est pourquoi l’utilisation du t modifé que Crawford et Howell (1998) proposent est une meilleure alternative. On peut également utiliser des méthodes basées sur les régressions (Shirk et al., 2011; Timmerman et al., 2021) Un groupe Analyse à privilégier : t de Student comparaison à une norme Condition d’application : ✓ Normalité de la distribution ✓ Indépendance des observations Que faire en cas de non respect des conditions d’application ? : ✓ Test de Wilcoxon Points d’attention : ✓ Il est nécessaire de disposer d’une norme. Cette norme peut être fournie et peut représenter le niveau hasard quand, par exemple, on peut répondre correctement à des questions en raison du hasard comme dans les cas de questions à choix multiples. Une variable indépendante Étape 3 : Identifier la nature de la variable indépendante Une variable indépendante quantitative Analyse à privilégier : ✓ Corrélation de Bravais-Pearson Condition d’application : ✓ Normalité du résidu ✓ Homogénéité des variances ✓ Indépendance des observations Que faire en cas de non respect des conditions d’application ? : ✓ Rho de Spearman ✓ Tau de Kendall ✓ Présenter l’intervalle de confiance obtenu par boostrap Une variable indépendante ordinale Analyse à privilégier : ✓ Rho de Spearman ✓ Tau de Kendall Condition d’application : ✓ Indépendance des observations Points d’attention : Le rho de Spearman est également adapté lorsque la relation est croissante mais non linéaire. ✓ Présenter l’intervalle de confiance obtenu par boostrap Une variable indépendante qualitative Étape 4 : identifier le nombre de modalités sur la variable indépendante Deux modalités Étape 5 : déterminer si la variable est intra ou inter-participant13 Variable intra-participant Analyse à privilégier : ✓ t de Student pour échantillons appariés Condition d’application : ✓ Normalité du résidu ✓ indépendance des observations Que faire en cas de non respect des conditions d’application ? : ✓ Test de Wilcoxon ✓ Test t sur les moyennes tronquées Variable inter-participant Analyse à privilégier : ✓ t de Student pour échantillons indépendants Condition d’application : ✓ Normalité du résidu ✓ Homogénéité des variances ✓ Indépendance des observations Que faire en cas de non respect des conditions d’application ? ✓ Si l’homogénéité des variances n’est pas respectée, le choix de prédiction est d’appliquer la correction de Satterthwaite14 ✓ Test de Mann-Whitney ✓ Test t sur les moyennes tronquées Plus que deux modalités Étape 5 : déterminer si la variable est intra ou inter-participant Variable intra-participant Analyse à privilégier : ✓ Anova à mesure répétée Condition d’application : ✓ Normalité du résidu ✓ Sphéricité de la matrice de covariance Alternative en cas non respect des condition d’application : ✓ Si seule la sphéricité de la matrice de covariance n’est pas respectée, application de la correction de Greenhouse-Geisser (Greenhouse &amp; Geisser, 1959) ou de Huynh-Feldt (Huynh &amp; Feldt, 1976). ✓ Anova de Friedman ✓ Anova de Friedman ✓ Anova à mesure répétée sur les moyennes tronquées Points d’attention : La correction de Greehouse-Geisser est souvent appliquée de manière automatique. Elle se manifeste par une correction des degrés de liberté Les modèles linéaires mixtes sont, dans bien des situations, une meilleure alternative à l’anova à mesure répétée. Variable à groupes indépendants Analyse à privilégier : Anova simple Condition d’application : ✓ Normalité du résidu ✓ Homogénéité des variances ✓ Indépendance des observations Alternative en cas non respect des condition d’application : ✓ Correction de Box ✓ Test de Kruskal-Wallis ✓ Anova sur les moyennes tronquées Une variable indépendante et une ou des variables de contrôle quantitatives Étape 3 : Identifier la nature de la variable indépendante Une variable indépendante quantitative Analyse à privilégier : ✓ Corrélation partielle de Bravais-Pearson Condition d’application : ✓ Normalité du résidu ✓ Homogénéité des variances ✓ Taille d’échantillon suffisante Alternative en cas non respect des condition d’application : ✓ Rho partiel de Spearman Point d’attention : ✓ si une ou plusieurs variables à contrôler sont ordinales, il est préférable de privilégier le rho partiel de Spearman ✓ Une alternative à envisager est la régression multiple, en particulier si on veut voir l’impact de plusieurs variables indépendantes sur la variable dépendante. Dans ce cas, les analyses de régression hiérarchiques devraient être privilégiées. Une variable indépendante ordinale Analyse à privilégier : ✓ Rho partiel de Spearman (ou tau partiel) Condition d’application : ✓ Indépendance des observations Une variable indépendante qualitative Analyse à privilégier : ✓ Analyse de covariance Condition d’application : ✓ Normalité du résidu ✓ Homogénéité des variances ✓ Sphéricité de la matrice de covariance si la variable indépendante est intra ET a plus de deux modalités ✓ Indépendance des observations Alternative en cas non respect des condition d’application : ✓ Analyse de covariance robuste Point d’attention : ✓ Il est en réalité statistiquement imprécis de considérer que l’analyse de covariance permet de contrôler l’effet de variables parasites. Seule une analyse de régression hiérarchique est en mesure de le faire. ✓ Les modèles linéaires mixtes représentent dans la très grande majorité des cas une meilleure alternative à l’analyse de covariance, en particulier s’il y a une variable indépendante qualitative intra, et ne représente jamais une moins bonne option. Plusieurs variables indépendantes Plusieurs variables indépendantes qualitatives Étape 3 : Identifier si les variables sont inter ou intra-participants Plusieurs variables indépendantes qualitatives inter Analyse à privilégier : ✓ Anova factorielle inter-participants Condition d’application : ✓ Normalité du résidu ✓ Homogénéité des variances ✓ Indépendance des observations Alternative en cas non respect des condition d’application : ✓ Anova à 2 ou 3 facteurs sur les médianes Point d’attention : ✓ Les anovas factorielles permettent de tester les interactions entre les variables indépendantes Plusieurs variables indépendantes qualitatives intra Analyse à privilégier : ✓ Anova factorielle intra-participant Condition d’application : ✓ Normalité du résidu ✓ Sphéricité de la matrice de covariance si au moins un des variables indépendantes a plus de deux modalités Alternative en cas non respect des condition d’application : ✓ Utilisation de bootstrap ✓ Modèles linéaires mixtes généralisés Point d’attention : ✓ Les modèles linéaires mixtes représentent dans la très grande majorité des cas une meilleure alternative à l’anova à mesure répétée (factorielle) et ne représente jamais une moins bonne option. Mélange entre des variables inter et intra Analyse à privilégier : ✓ anova mixte Condition d’application : ✓ Normalité du résidu ✓ Sphéricité de la matrice de covariance si au moins une des variables indépendantes intra a plus de deux modalités Alternative en cas non respect des condition d’application : ✓ anova sur les moyennes tronquées ✓ Modèles linéaires mixtes généralisés Point d’attention : ✓ Les modèles linéaires mixtes représentent dans la très grande majorité des cas une meilleure alternative à l’anova à mesure répétée et ne représente jamais une moins bonne option. Plusieurs variables indépendantes quantitatives Analyse à privilégier : ✓ régressions (appelées aussi modèles linéaires) Condition d’application : ✓ Normalité du résidu ✓ homogénéité des variances ✓ Absence de multicolinéarité ✓ Absence d’autocorrélation Alternative en cas non respect des condition d’application : ✓ régressions sur les M-estimators ✓ Utilisation de bootstrap Points d’attention ✓ Dans les régressions linéaires multiples, on fait l’hypothèse que la relation qui explique le mieux le lien entre la variable dépendante et les variables indépendantes sont des relations linéaires. Il est possible néanmoins de modéliser des relations non linéaires, comme des relations quadratiques ou cubiques. On parle dans cas de régressions non linéaires. Mélange entre plusieurs types de variables indépendantes Analyse à privilégier : ✓ régressions (appelées aussi modèles linéaires) Condition d’application : ✓ Normalité du résidu ✓ homogénéité des variances ✓ Absence de multicolinéarité ✓ Absence d’autocorrélation Alternative en cas non respect des condition d’application : ✓ régressions sur les M-estimators ✓ Utilisation de bootstrap Point d’attention ✓ si l’accent est plutôt mis sur l’impact de la variable indépendante qualitative et qu’on veut contrôler les variables quantitatives, on choisira plutôt l’analyse de covariance, même si en réalité, les deux approches reviennent au même. ✓ Dans les régressions linéaires multiples, on fait l’hypothèse que la relation qui explique le mieux le lien entre la variable dépendante et les variables indépendantes sont des relations linéaires. Il est possible néanmoins de modéliser des relations non linéaires, comme des relations quadratiques ou cubiques. On parle dans cas de régressions non linéaires. Variable dépendante ordinale Étape 2 : Identifier le nombre de variables indépendantes Il n’y a pas de variable indépendante Étape 3 : Identifier le nombre d’observations Une seule observation Analyse à privilégier : ✓ Utilisation des centiles Points d’attention : ✓ Il est nécessaire d’avoir à disposition un échantillon de référence auquel on peut comparer l’observation Remarque importante : ✓ Si l’usage, en particulier des cliniciens, des centiles s’est, comme pour le score z, imposé, c’est surtout en raison de sa facilité de calcul. Cependant, cette manière de faire est en réalité un usage impropre centiles car, une méthode qui autorise des inférences doit prendre en compte la taille de l’échantillon de référence sur lequel on s’appuie, ce qui n’est pas le cas dans le cadre centiles. C’est pourquoi l’utilisation basées sur des modèles linéaires généralisés devraient être envisagés (Henry et al., 2023) Un groupe Analyse à privilégier : ✓ Test de Wilcoxon Condition d’application : ✓ Indépendance des observations Points d’attention : ✓ Il est nécessaire de disposer d’une norme. Cette norme peut être fournie et peut représenter le niveau hasard quand, par exemple, on peut répondre correctement à des questions en raison du hasard comme dans les cas de questions à choix multiples. Une variable indépendante Étape 3 : Identifier la nature de la variable indépendante Une variable indépendante quantitative Analyse à privilégier : ✓ Rho de Spearman ✓ Tau de Kendall Condition d’application : ✓ Indépendance des observations Points d’attention : ✓ Le rho de Spearman est également adapté lorsque la relation est croissante mais non linéaire. Une variable indépendante ordinale Analyse à privilégier : ✓ Rho de Spearman ✓ Tau de Kendall Condition d’application : ✓ Indépendance des observations Points d’attention : ✓ Le rho de Spearman est également adapté lorsque la relation est croissante mais non linéaire. Une variable indépendante qualitative Étape 4 : identifier le nombre de modalités sur la variable indépendante Deux modalités Étape 5 : déterminer si la variable est intra ou inter-participant Variable intra-participant Analyse à privilégier : ✓ Test de Wilcoxon Condition d’application : ✓ indépendance des observations Variable inter-participant Analyse à privilégier : ✓ Test de Mann-Whitney Condition d’application : ✓ Indépendance des observations Plus que deux modalités Étape 5 : déterminer si la variable est intra ou inter-participant Variable intra-participant Analyse à privilégier : ✓ Anova de Friedman Condition d’application : ✓ Aucune Points d’attention : ✓ On ne peut faire une anova de Friedman que pour les situations où aucune valeur n’est manquante. Variable à groupes indépendants Analyse à privilégier : ✓ Test de Kruskal-Wallis Condition d’application : ✓ Indépendance des observations Une variable indépendante et une ou des variables de contrôle quantitatives ou ordinales Étape 3 : Identifier la nature de la variable indépendante Une variable indépendante quantitative Analyse à privilégier : ✓ Rho partiel de Spearman ✓ tau partiel dfe Kendall Condition d’application : ✓ Indépendance des observations Point d’attention : ✓ Une alternative à envisager est le modèle linéaire généralisé Une variable indépendante ordinale Analyse à privilégier : ✓ Rho partiel de Spearman ✓ tau partiel de Kendall Condition d’application : ✓ Indépendance des observations Point d’attention : ✓ Une alternative à envisager est le modèle linéaire généralisé Une variable indépendante qualitative Analyse à privilégier : ✓ Modèles linéaires généralisés Condition d’application : ✓ Indépendance des observations Point d’attention : ✓ L’utilisation des modèles linéaires généralisés requiert d’être en mesure de choisir la bonne distribution. ✓ Les modèles linéaires mixtes généralisés représentent une alternative si la variable indépendante qualitative est intra-participant. Plusieurs variables indépendantes Analyse à privilégier : ✓ Modèles linéaires généralisés s’il n’y a aucune variable intra-participant ✓ Modèles linéaires mixtes généralisés s’il y a au moins une variable intra-participant Condition d’application : ✓ Indépendance des observations Point d’attention : ✓ L’utilisation des modèles linéaires généralisés requiert d’être en mesure de choisir la bonne distribution. ✓ Les modèles linéaires mixtes généralisés représentent une alternative si la variable indépendante qualitative est intra-participant. Variable dépendante qualitative Étape 2 : Identifier le nombre de variables indépendantes Il n’y a pas de variable indépendante Un groupe Analyse à privilégier : ✓ khi² d’ajustement Condition d’application : ✓ Indépendance des observations ✓ Pour le khi² d’ajustement, les effectifs théoriques doivent être supérieurs à 5 dans au moins 80% des cases du tableau Points d’attention : ✓ Il est nécessaire de disposer d’un modèle théorique. Ce modèle théorique peut être une répartition équiprobable entre les différentes modalités ✓ Si les conditions d’applications ne sont pas respectées, on peut envisager de regrouper des modalités si cela fait sens. Une variable indépendante Étape 3 : Identifier la nature de la variable indépendante Une variable indépendante quantitative Analyse à privilégier : ✓ Modèle linéaire généralisé ✓ régression logistique (il s’agit d’un cas particulier du modèle linéaire généralisé où la variable dépendance à 2 modalités) Condition d’application : ✓ Indépendance des observations Une variable indépendante ordinale Analyse à privilégier : ✓ khi² d’indépendance ✓ Modèle linéaire généralisé Condition d’application : ✓ Pour le khi² d’indépendance, les effectifs théoriques doivent être supérieurs à 5 dans au moins 80% des cases du tableau ✓ Indépendance des observations Que faire en cas de non respect des conditions d’application ? : ✓ Fisher exact test Une variable indépendante qualitative Étape 4 : identifier le nombre de modalités sur la variable indépendante Deux modalités Étape 5 : déterminer si la variable est intra ou inter-participant Variable intra-participant Analyse à privilégier : ✓ Test de McNemar Condition d’application : ✓ La somme des deux cases pour lesquelles il y a eu un changement doit être supérieure à 10 ✓ indépendance des observations Variable inter-participant Analyse à privilégier : ✓ khi² d’indépendance Condition d’application : ✓ Indépendance des observations ✓ Pour le khi² d’indépendance, les effectifs théoriques doivent être supérieurs à 5 dans au moins 80% des cases du tableau Que faire en cas de non respect des conditions d’application ? : ✓ Fisher exact test Plus que deux modalités Étape 5 : déterminer si la variable est intra ou inter-participant Variable intra-participant Analyse à privilégier : ✓ Modèles linéaires mixtes Condition d’application : ✓ Aucune Variable à groupes indépendants Analyse à privilégier : ✓ khi² d’indépendance Condition d’application : ✓ Indépendance des observations ✓ Pour le khi² d’indépendance, les effectifs théoriques doivent être supérieurs à 5 dans au moins 80% des cases du tableau Que faire en cas de non respect des conditions d’application ? : ✓ Fisher exact test Une variable indépendante et une ou des variables de contrôle quantitative(s) Étape 3 : Identifier la nature de la variable indépendante Une variable indépendante quantitative Analyse à privilégier : ✓ Modèles linéaires généralisés Condition d’application : ✓ Indépendance des observations Point d’attention : ✓ L’utilisation des modèles linéaires généralisés requiert d’être en mesure de choisir la bonne distribution. Une variable indépendante ordinale Analyse à privilégier : ✓ Modèle linéaire généralisé Condition d’application : ✓ Indépendance des observations Point d’attention : ✓ L’utilisation des modèles linéaires généralisés requiert d’être en mesure de choisir la bonne distribution. Une variable indépendante qualitative Analyse à privilégier : ✓ Modèles linéaires généralisés Condition d’application : ✓ Indépendance des observations Point d’attention : ✓ L’utilisation des modèles linéaires généralisés requiert d’être en mesure de choisir la bonne distribution. Plusieurs variables indépendantes Analyse à privilégier : ✓ Modèles linéaires généralisés Condition d’application : ✓ Indépendance des observations Point d’attention : ✓ L’utilisation des modèles linéaires généralisés requiert d’être en mesure de choisir la bonne distribution. 10.2.2 Plusieurs variables dépendantes Les situations qui ont été décrites pour les analyses de variances, les modèles linéaires, les corrélations, les modèles linéaires généralisés et les modèles linéaires mixtes ont des équivalents multivariés où l’objectif est de déterminer comment les variables dépendantes impactent un ensemble de variables dépendantes simultanément. Ainsi, l’outil d’aide à la décision décrit plus haut peut être retranscrit dans le Tableau 10.1 : .cl-3b98f3fe{}.cl-3b8def4a{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-3b927100{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3b928d0c{width:2in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b928d16{width:2in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b928d20{width:2in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 10.1: Correspondance multivariée des analyses univariées Analyse univariéeÉquivalent multivariéanova, ancovamanovacorrélationsanalyse canonique de corrélationsrégressionsrégressions multivéesmodèles linéaires (mixtes) généralisésmodèles linéraires (mixtes) généralisés multivariés 10.2.3 Les analyses statistiques sur de petits échantillons Il est très fréquents que les psychologues aient à gérer des situations où l’échantillon est de taille restreinte. Dans ce genre de situation, plutôt que de vouloir appliquer des analyses inférentielles décrites ci-dessus, il est utile d’envisager de traiter les données sous l’angle des analyses de cas unique. Il est important néanmoins de faire les choses dans l’ordre et d’être au clair sur les contraintes qu’impliquent ce type d’analyse avant de commencer le recueil pour éviter d’être dans une situation où ces outils seront inexploitables. 10.2.4 Les outils statistiques relevant de la méta-science Il semble utile de préciser que certains outils statistiques permettent d’aborder les questions scientifiques d’une manière plus globale. Ainsi, quand on veut pouvoir faire une synthèse de l’ensemble des résultats d’une question particulière, la méta-analyse est l’outil de prédilection. Cependant,les résultats des méta-analyses peuvent être trompeur s’il existe un biais de publication. Les courbe p (p-curve en anglais) représente un outil complémentaire qui permettent d’évaluer ce biais. L’autre panoplie d’outils qui méritent également toute notre attention sont les outils qui permettent de développer des modèles et de les tester. S’il existe différents noms en fonction de la manière de formaliser le modèle, on peut raisonnablement considérer que l’ensemble des analyses appartiennent à la famille des modèles d’équation structurale. 10.2.5 Les outils statistiques relevant de la psychométrie Le développement d’outils psychométriques requiert d’évaluer toute une série de propriété de ces outils, que ce soit en termes de validité, de fiabilité mais également en termes de leur valeur prédictive. Dans bien des situations, on va s’appuyer sur des corrélations pour examiner ces propriétés mais certains outils sont néanmoins plus élaborés. Il ne semble pas opportun de développer dans ce chapitre les différentes moyens qui peuvent être envisagés pour examiner ces propriétés. Il semble plus pertinent de s’orienter vers le chapitre correspondant de cet ouvrage. 10.2.6 Les autres outils qui s’appuient sur les statistiques Que ce soit l’intelligence artificielle ou le machine learning, ces outils s’appuient peu ou prou sur des outils statistiques que nous avons déjà décrits. Si ces outils sont de plus en plus utilisés, y compris par des chercheurs en psychologie, il ne semble pas opportun de les décrire ici en détails dans la mesure où il existe une multitude d’algorithme et d’outils qui peuvent être utilisés et en faire une liste exhaustive aurait un effet plus délétère que formateur. Nous limiterons cette section à quelques banalités qui pourront éventuellement éveiller la curiosité du lecteur si ce type d’outils correspond à ses besoins. Parmi ces banalités, la première chose à comprendre est que l’objectif de ces outils de machine learning (parfois aussi appelé apprentissage statistique) est de prédire les nouvelles observations sur la base des informations relatives à ces observations. Par exemple, si on s’intéresse à la maladie d’Alzheimer, on pourrait se demander, sachant qu’une personne à 75 ans, est fumeur, a consommé l’équivalent de 4 verres de vins tous les jours de sa vie, et a obtenu le bac, quelle est la probabilité que cette personne développe une maladie d’Alzheimer. Dans ce domaine, il existe deux grandes catégories d’outils : l’apprentissage supervisé pour lequel on fournit des données permettant au modèle d’apprendre à prédire le score ou la catégoriser d’une personne (telle personne a telles caractéristiques et appartient à la catégorie A, telle autre personne présente telles autres caractéristiques et appartient à la catégorie B…) et l’apprentissage non supervisé où on ne fournit pas au modèle d’informations sur ce qu’il doit trouver, parfois parce que le chercheur ne le sait pas lui-même. Les outils basés sur les régressions ou l’analyse discriminante font partie de la première catégorie, tandis que les analyses en cluster font partie de la seconde. Références Crawford, J. R., &amp; Howell, D. C. (1998). Comparing an individual’s test score against norms derived from small samples. The Clinical Neuropsychologist, 12(4), 482–486. https://doi.org/10.1076/clin.12.4.482.7241 Greenhouse, S. W., &amp; Geisser, S. (1959). On methods in the analysis of profile data. Psychometrika, 24(2), 95–112. https://doi.org/10.1007/BF02289823 Henry, A., Stefaniak, N., Schmid, F., Kwiatkowski, A., Hautecoeur, P., &amp; Lenne, B. (2023). Assessing cognitive changes in multiple sclerosis: Criteria for a reliable decision. Journal of Clinical and Experimental Neuropsychology, 45(4), 321–344. https://doi.org/10.1080/13803395.2023.2232122 Huynh, H., &amp; Feldt, L. S. (1976). Estimation of the box correction for degrees of freedom from sample data in randomized block and split-plot designs. Journal of Educational Statistics, 1(1), 69. https://doi.org/10.2307/1164736 Shirk, S. D., Mitchell, M., Shaughnessy, L., Sherman, J. C., &amp; Locascio, J. J. (2011). A web-based normative calculator for the uniform data set neuropsychological test battery. Journal of Alzheimer’s Disease, 26(3), 297–305. https://doi.org/10.3233/JAD-2011-110812 Timmerman, M. E., Voncken, L., &amp; Albers, C. J. (2021). A tutorial on regression-based norming of psychological tests with GAMLSS. Psychological Methods, 26(6), 735–750. https://doi.org/10.1037/met0000348 ce sont des personnes différentes qui passent par les différentes modalités de la variable indépendante↩︎ ce sont les mêmes personnes qui passent par les différentes modalités de la variable indépendante↩︎ La même personne passe par les différentes modalités↩︎ il s’agit d’une correction des degrés de liberté, ce qui, de facto entraîne une correction de la probabilité de dépassement↩︎ "],["références.html", "Chapter 11 Références", " Chapter 11 Références Bayes, T., &amp; Price, R. (1763). An essay towards solving a problem in the doctrine of chances. Philosophical Transactions of the Royal Society of London, 53, 370–418. https://doi.org/10.1098/rstl.1763.0053 Choueiry. (2021). Statistical Software Popularity in 40,582 Research Papers &amp;#x2013; QUANTIFYING HEALTH — quantifyinghealth.com. https://quantifyinghealth.com/statistical-software-popularity-in-research/. Comtois, D. (2025). Summarytools: Tools to quickly and neatly summarize data. https://doi.org/10.32614/CRAN.package.summarytools Crawford, J. R., &amp; Howell, D. C. (1998). Comparing an individual’s test score against norms derived from small samples. The Clinical Neuropsychologist, 12(4), 482–486. https://doi.org/10.1076/clin.12.4.482.7241 Dweck, C. S. (2006). Mindset. Random House. Greenhouse, S. W., &amp; Geisser, S. (1959). On methods in the analysis of profile data. Psychometrika, 24(2), 95–112. https://doi.org/10.1007/BF02289823 Harrell Jr, F. E. (2025). Hmisc: Harrell miscellaneous. https://doi.org/10.32614/CRAN.package.Hmisc Hendricks, P. (2015). Titanic: Titanic passenger survival data set. https://doi.org/10.32614/CRAN.package.titanic Henry, A., Stefaniak, N., Schmid, F., Kwiatkowski, A., Hautecoeur, P., &amp; Lenne, B. (2023). Assessing cognitive changes in multiple sclerosis: Criteria for a reliable decision. Journal of Clinical and Experimental Neuropsychology, 45(4), 321–344. https://doi.org/10.1080/13803395.2023.2232122 Hofer, H. von. (2000). Crime statistics as constructs: The case of swedish rape statistics. European Journal on Criminal Policy and Research, 8(1), 77–89. https://doi.org/10.1023/A:1008713631586 https://www.facebook.com/48576411181. The 2016 Top Programming Languages — spectrum.ieee.org. https://spectrum.ieee.org/the-2016-top-programming-languages. Huber, P. J. (1981). Robust statistics. John Wiley &amp; Sons. Huynh, H., &amp; Feldt, L. S. (1976). Estimation of the box correction for degrees of freedom from sample data in randomized block and split-plot designs. Journal of Educational Statistics, 1(1), 69. https://doi.org/10.2307/1164736 JASP Team. (2025). Jeffrey’s Amazing Statistics Program, JASP (Version 0.19.3)[Computer software]. https://jasp-stats.org/ Koss, M. P., Gidycz, C. A., &amp; Wisniewski, N. (1987). The scope of rape: Incidence and prevalence of sexual aggression and victimization in a national sample of higher education students. Journal of Consulting and Clinical Psychology, 55(2), 162–170. https://doi.org/10.1037/0022-006X.55.2.162 Lagier, J.-C., Million, M., Gautret, P., Colson, P., Cortaredona, S., Giraud-Gatineau, A., Honoré, S., Gaubert, J.-Y., Fournier, P.-E., Tissot-Dupont, H., Chabrière, E., Stein, A., Deharo, J.-C., Fenollar, F., Rolain, J.-M., Obadia, Y., Jacquier, A., La Scola, B., Brouqui, P., … Zandotti, C. (2020). Outcomes of 3, 737 COVID-19 patients treated with hydroxychloroquine/azithromycin and other regimens in marseille, france: A retrospective analysis. Travel Medicine and Infectious Disease, 36, 101791. https://doi.org/10.1016/j.tmaid.2020.101791 LaPlace, P. S. (1814). Essai philosophique sur les probabilités. Courcier. http://eudml.org/doc/203193 Le Breton, M. (2016). Pour 27% des français, l’auteur d’un viol est moins responsable si la victime portait une tenue sexy. Huffington Post France. http://www.huffingtonpost.fr/2016/02/29/francais-victime-viol-etude_n_9346702.html Lilienfeld, S. O., Wood, J. M., &amp; Garb, H. N. (2000). The scientific status of projective techniques. Psychological Science in the Public Interest, 1(2), 27–66. https://doi.org/10.1111/1529-1006.002 McKiernan, E. C., Bourne, P. E., Brown, C. T., Buck, S., Kenall, A., Lin, J., McDougall, D., Nosek, B. A., Ram, K., Soderberg, C. K., Spies, J. R., Thaney, K., Updegrove, A., Woo, K. H., &amp; Yarkoni, T. (2016). How open science helps researchers succeed. Elife, 5. Muenchen, B. (2015). R now contains 150 times as many commands as SAS. R-bloggers. https://www.r-bloggers.com/2015/05/r-now-contains-150-times-as-many-commands-as-sas/ Neyman, J., &amp; Pearson, E. S. (1933). Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 231(694–706), 289–337. https://doi.org/10.1098/rsta.1933.0009 Nuijten, M. B., Hartgerink, C. H. J., Assen, M. A. L. M. van, Epskamp, S., &amp; Wicherts, J. M. (2016). The prevalence of statistical reporting errors in psychology (1985–2013). Behavioral Research Methods, 48(4), 1205–1226. Popper, K. R. (1959). The logic of scientific discovery. Hutchinson. R Core Team. (2022). Foreign: Read data stored by ’minitab’, ’s’, ’SAS’, ’SPSS’, ’stata’, ’systat’, ’weka’, ’dBase’, ... https://CRAN.R-project.org/package=foreign R Core Team. (2025). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/ Revelle, W. (2025). Psych: Procedures for psychological, psychometric, and personality research. Northwestern University. https://CRAN.R-project.org/package=psych Royall, R. M. (1997). Statistical evidence: A likelihood paradigm. Chapman &amp; Hall. Shirk, S. D., Mitchell, M., Shaughnessy, L., Sherman, J. C., &amp; Locascio, J. J. (2011). A web-based normative calculator for the uniform data set neuropsychological test battery. Journal of Alzheimer’s Disease, 26(3), 297–305. https://doi.org/10.3233/JAD-2011-110812 Snow, G. (2024). TeachingDemos: Demonstrations for teaching and learning. https://doi.org/10.32614/CRAN.package.TeachingDemos Steegen, S., Tuerlinckx, F., Gelman, A., &amp; Vanpaemel, W. (2016). Increasing transparency through a multiverse analysis. Perspectives on Psychological Science, 11(5), 702–712. https://doi.org/10.1177/1745691616658637 Stefaniak, N. (2018). easieR: easieR: A GUI r metapackage. https://github.com/NicolasStefaniak/easieR Stefaniak, N., Meulemans, T., &amp; Willems, S. (2010). Semantic hyperpriming in normal aging: A consequence of instructions? [Article]. Aging, Neuropsychology, and Cognition, 17(5), 615–632. https://doi.org/10.1080/13825585.2010.492205 Strauss, E. M. S., Sherman, E. M. S., &amp; Spreen, O. (2006). Psychometrics in neuropsychological assessment. In E. Strauss, E. M. S. Sherman, &amp; O. Spreen (Eds.), A compendium of neuropsychological tests: Administration, norms, and commentary (3rd ed., pp. 3–43). Oxford University Press. The jamovi project. (2025). Jamovi (version 2.4). https://www.jamovi.org. Timmerman, M. E., Voncken, L., &amp; Albers, C. J. (2021). A tutorial on regression-based norming of psychological tests with GAMLSS. Psychological Methods, 26(6), 735–750. https://doi.org/10.1037/met0000348 Wickham, H. (2016a). ggplot2: Elegant graphics for data analysis. Springer-Verlag. https://ggplot2.tidyverse.org Wickham, H. (2016b). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://doi.org/10.21105/joss.01686 Wickham, H., &amp; Bryan, J. (2022). Readxl: Read excel files. https://CRAN.R-project.org/package=readxl Wickham, H., François, R., Henry, L., &amp; Müller, K. (2022). Dplyr: A grammar of data manipulation. https://CRAN.R-project.org/package=dplyr Wickham, H., Hester, J., Chang, W., &amp; Bryan, J. (2025). Devtools: Tools to make developing r packages easier. https://doi.org/10.32614/CRAN.package.devtools Wilcox, R. R. (2012). Introduction to robust estimation and hypothesis testing (3rd ed.). Academic Press. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
